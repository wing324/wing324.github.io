<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[Python Private Methods And Variables]]></title>
      <url>%2F2020%2F03%2F23%2FPython-Private-Methods-And-Variables%2F</url>
      <content type="text"><![CDATA[Python doesn’t have private method and variable like the OOD programming language. But, we do have other way to do it. In Python, we can use __ (double underscore) to make a method or variable private. Private VariableCode as below: class Person: def __init__(self, name, age): self.name = name # age is a private variable self.__age = age Bob = Person(&quot;Bob&quot;, 17)print(&quot;name:&quot;, Bob.name)# this will cause an error: no `__age` variableprint(&quot;Age:&quot;, Bob.__age) Private MethodCode as below: class Person: def __init__(self, name, age): self.name = name # age is a private variable self.age = age def __agePlus(self): self.age = self.age+1Bob = Person(&quot;Bob&quot;, 17)# this method will cause an error: no &apos;__agePlus&apos; methodBob.__agePlus() Access Private StuffCode as below: class Person: def __init__(self, name, age): self.name = name # age is a private variable self.__age = age Bob = Person("Bob", 17)print("name:", Bob.name)# this will cause an error: no `__age` variableprint("Age:", Bob.__age) # we can access __age like this:print(Bob._Person__age)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Instance method, Class method, and Static method in Python Class]]></title>
      <url>%2F2020%2F03%2F23%2FInstance-method-Class-method-and-Static-method-in-Python%2F</url>
      <content type="text"><![CDATA[This article is going to talk about the difference between instance method, class method, and static method in Python class. Instance MethodInstance method is the most used method in python class. The instance method can be accessed by each instance. Each instance has its own instance variables. So, instance methods are used to access and modify instance variables. Instance Method must have self argument as the first argument of the instance method. self just a name, it can be a different name. self refers to the instance. class Employee: # constructor def __init__(self, ID=None, name=None, department=None): self.ID = ID self.name = name self.department = department # instance method # a,b,c,d is instance variables def demo(self, a, b, c=4, d=None): print(a) print(b) print(c) print(d) Class MethodClass method works with class variables and access by class name. It also can access by instance, but we would like to access it by class name. All the class instances share the class variables and class method. So, class methods are used to access and modify class variables. Class method must have @classmethod before the class method. And class method must have cls as the first argument. cls just a name, it can be a different name, cls refers to the class. class Employee: # class variable # Access/Update by all instances team = "IT" # class method @classmethod def getTeamName(cls): return cls.teamSteve = Employee()# use class methodprint(Steve.getTeamName())# Prefer to use this way to use class methodprint(Employee.getTeamName()) Static MethodStatic method is not related to class variables or instance variables. It contains some fixed information and can not be modified. Static method must have @staticmethod before the static method. And static method don’t have to use self or cls as first argument. It doesn’t have any mandatory argument. class Employee: # static method # static method don't need to use self/cls @staticmethod def sMethod(): print("I am a static method")# use static methodSteve.sMethod()# Prefer to use this way to use static methodEmployee.sMethod()]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[How to deploy an app on Heroku]]></title>
      <url>%2F2020%2F01%2F29%2FHow-to-deploy-an-app-on-Heroku%2F</url>
      <content type="text"><![CDATA[This is a blog for deploying an app on Heroku First of all, you should have a Heroku account, if you don’t have it, just sign up for free. Then, you should have an app, if you don’t have, you can just use the sample. Assume our source code in src folder We have to put our code at root folder, because Heroku will find code at root folder, the root folder in the post is src folder // src/index.js// import express by using require, since node.js doesn't support import expressconst express = require('express');const app = express();app.get('/', (req, res) =&gt; &#123; res.send(&#123;'hi': 'there'&#125;);&#125;);// process.env.PORT in order to user Heroku port for our appconst PORT = process.env.PORT || 5000app.listen(PORT);// src/package.json// we have to add "engines" field to tell HeroKu what version of node+npm you used// we have to add "scripts" field to tell HeroKu how to run our app&#123; "name": "src", "version": "1.0.0", "description": "", "main": "index.js", "engines": &#123; "node": "10.16.0", "npm": "6.9.0" &#125;, "scripts": &#123; "start": "node index.js" &#125;, "author": "Wing Yu", "license": "ISC", "dependencies": &#123; "express": "^4.17.1" &#125;&#125; Install Heroku CLI $ npm install -g heroku Check Heroku installation $ heroku --versionheroku/7.37.0 win32-x64 node-v10.16.0 Login your Heroku by Heroku CLI $ heroku loginheroku: Press any key to open up the browser to login or q to exit: Go to your app root folder(the post is src folder), and create Heroku app cd src/heroku create# After you finish run the command, you will get 2 link# link1 | link2# link1 is your app website# link2 is your app git repo site on Heroku Push your code into GitHub, you should have a git repo for your app. Add your git repo into your Heroku git repo git remote add heroku link2 Push your GitHub code to your Heroku git repo git push heroku master Open your app on Heroku heroku open# or open link1 If you have any issue, you can also look the Heroku log heroku logs]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[React Uses SVG by svgr tool]]></title>
      <url>%2F2019%2F09%2F20%2FReact-Uses-SVG-by-svgr-tool%2F</url>
      <content type="text"><![CDATA[svgr tool helps us automatically change svg file to React component, which is easy for us use. Create assests and Icons folder under src folder. Put SVG source code into assests folder. In package.json file, we add the following script: &quot;scripts&quot;: &#123; &quot;svgr&quot;: &quot;svgr -d src/Icons/ src/assets/&quot; &#125;, Then, let’s download svgr tool by Yarn. yarn add @svgr/cli The last step we will run command yarn run svgr. When the command finished, we can check Icons folder, we will find svgr tool helps us convert all svg files to react components(js files).]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Setup React app on AWS EC2 instance]]></title>
      <url>%2F2019%2F09%2F20%2FSetup-React-app-on-AWS-EC2-instance%2F</url>
      <content type="text"><![CDATA[How to setup React app on AWS EC2 instance. 1. yarn run buildFirst of all, we should build our project by running yarn run build, and we will get a director under our project root directory call dist or build, then we will upload the dist folder to our AWS EC2 instance.With this article, I will put dist director to /data director on AWS. 2. yum install Nginx3. Change Nginx configNormally, we won’t use root user to do it, but at here I will use root user for convenience. vim /etc/nginx/nginx.conf## updated config user root; ## updatedworker_processes auto;error_log /var/log/nginx/error.log;pid /run/nginx.pid;# Load dynamic modules. See /usr/share/doc/nginx/README.dynamic.include /usr/share/nginx/modules/*.conf;events &#123; worker_connections 1024;&#125;http &#123; log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; access_log /var/log/nginx/access.log main; sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; types_hash_max_size 2048; include /etc/nginx/mime.types; default_type application/octet-stream; include /etc/nginx/vhost/*.conf; ### updated server &#123; listen 80 default_server; listen [::]:80 default_server; server_name _; root /usr/share/nginx/html;# # Load configuration files for the default server block. include /etc/nginx/default.d/*.conf; location / &#123; &#125; error_page 404 /404.html; location = /40x.html &#123; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; &#125; &#125; 4. Make own Nginx config Create /etc/nginx/vhost directory to save our own Nginx config mkdir /etc/nginx/vhost Make own Nginx config vim /etc/nginx/vhost/react.conf### updatedserver &#123; listen 3000; server_name localhost; root /data/dist; location / &#123; try_files $uri $uri/ /index.html; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125;&#125; 5. Change user and mode for build folderchown -R root:root /data/distchmod 755 /data/dist 6. Run your Nginxsystemctl start nginx 7. Disable SELinux# Temporarysetenforce 0# permanentvim /etc/selinux/config# change SELINUX=enforcing to SELINUX=disabled Now you can use IP and port to get the React App.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[How to set up a React Project without create-react-app]]></title>
      <url>%2F2019%2F07%2F27%2FHow-to-set-up-a-React-Project-without-create-react-app%2F</url>
      <content type="text"><![CDATA[This is a blog for creating a new React project without create-react-app tools. 1. Set up project- Setting up Node.js or Yarn You can download Node.js or Yarn from their official website:Node.js: https://nodejs.org/en/download/Yarn: https://yarnpkg.com/lang/en/docs/install/#windows-stable Initializing the project Create a new and empty directory for the project, and get into the new directory. &gt; mkdir newProject&gt; cd newProject&gt; &gt; Initialize the project &gt; npm init&gt; // or&gt; yarn init&gt; &gt; Create a src directory to store the project source code inside newProject directory &gt; mkdir src&gt; &gt; Utill now, the project directory looks like: &gt; newProject&gt; |--src&gt; - Setting up Webpack Install Webpack related package &gt; npm install --save-dev webpack webpack-dev-server webpack-cli&gt; //or&gt; yarn add webpack webpack-dev-server webpack-cli&gt; &gt; Create a new file webpack.config.js under newProject folder, and put the below code into the webpack.config.js file. &gt; const path = require(&apos;path&apos;);&gt; module.exports = &#123;&gt; // define entry file and output&gt; entry: &apos;./src/index.js&apos;,&gt; output: &#123;&gt; path: path.resolve(&apos;dist&apos;),&gt; filename: &apos;main.js&apos;&gt; &#125;,&gt; // define babel loader&gt; module: &#123;&gt; rules: [&gt; &#123; test: /\.jsx?$/, loader: &apos;babel-loader&apos;, exclude: /node_modules/ &#125;&gt; ]&gt; &#125;&gt; &#125;;&gt; - Setting up Babel Install Babel related package &gt; npm install --save-dev babel-core babel-loader babel-preset-es2015 babel-preset-react @babel/core @babel/preset-env @babel/preset-react&gt; npm install --save-dev babel-plugin-transform-object-rest-spread &gt; // or&gt; yarn add babel-core babel-loader babel-preset-es2015&gt; yarn add babel-plugin-transform-object-rest-spread&gt; &gt; Create a new file .babelrc under newProject folder, and put the below code into the .babelrc file. &gt; &#123;&gt; &quot;presets&quot;:[&gt; &quot;@babel/preset-env&quot;,&gt; &quot;@babel/preset-react&quot;],&gt; &quot;plugins&quot;: [ &quot;transform-object-rest-spread&quot; ]&gt; &#125;&gt; - Setting up the entry files Create newProject/src/index.js file, and put the below code into the file. &gt; console.log(&apos;hello world!&apos;)&gt; &gt; Create newProject/index.html file, and put the below code into the file. &gt; &lt;!DOCTYPE html&gt;&gt; &lt;html&gt;&gt; &lt;head&gt;&gt; &lt;meta charset=&quot;utf-8&quot;&gt;&gt; &lt;title&gt;React&lt;/title&gt;&gt; &lt;/head&gt;&gt; &lt;body&gt;&gt; &lt;div id=&quot;root&quot;&gt;&lt;/div&gt;&gt; &lt;script src=&quot;main.js&quot;&gt;&lt;/script&gt;&gt; &lt;/body&gt;&gt; &lt;/html&gt;&gt; - Running Project Put below code into package.json file. &gt; &quot;scripts&quot;: &#123;&gt; &quot;start&quot;: &quot;webpack-dev-server&quot;,&gt; &quot;build&quot;: &quot;webpack&quot;&gt; &#125;,&gt; &gt; Run the project. &gt; npm start&gt; // or&gt; yarn start&gt; 2. Set up ReactInstall React Use the below command at the newProject folder. &gt; npm install --save react react-dom&gt; // or&gt; yarn add react react-dom&gt; 3. Set up reduxInstall Redux Use the below command at the newProject folder. &gt; npm install --save redux react-redux&gt; // or&gt; yarn add redux react-redux&gt; 4. Set up JSXInstall JSX Use the below command at the newProject folder. &gt; npm install --save-dev babel-preset-react&gt; // or &gt; yarn add babel-preset-react&gt; Config .babelrc file]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[How to use create-react-app]]></title>
      <url>%2F2019%2F07%2F27%2FHow-to-use-create-react-app%2F</url>
      <content type="text"><![CDATA[This is a blog for creating a new React project by using create-react-app tools. Install Node.js https://nodejs.org/en/ Navigate to the folder where you want to create your new project, and create a project by using create-react-app, for example the new project name is “helloworld” yarn create react-app my-app When you finished the above steps, you maybe have a error as below: A template was not provided. This is likely because you&apos;re using an outdated version of create-react-app.Please note that global installs of create-react-app are no longer supported. The reason is: If you&apos;ve previously installed create-react-app globally via npm install -g create-react-app, we recommend you uninstall the package using npm uninstall -g create-react-app to ensure that npx always uses the latest version.https://create-react-app.dev/docs/getting-started(According to create-react-app official site) So, make sure you use command below to create your react app by create-react-app, : # either of themnpx create-react-app my-appnpm init react-app my-appyarn create react-app my-app Preview your project in your browser cd helloworldnpm start / yarn start Create a production build and you will get a “build” folder npm run build When this has completed, you can follow the onscreen prompts to deploy it to your server or just test it locally using the popular serve node package]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Five underscores in Python]]></title>
      <url>%2F2019%2F06%2F27%2FFive-underscores-in-Python%2F</url>
      <content type="text"><![CDATA[Do you know what does means about _var,var_,__var,__var__ and _, if you don’t know, let\’s get in details. 1. _var(single underscore)This is a hint to programmer, it means the variable for internal use. It is just a hint, not enforce by Python. &gt;&gt;&gt; class Test:... def __init__(self):... self.foo = 11... self._bar = 23...&gt;&gt;&gt; t = Test()&gt;&gt;&gt; t.foo11&gt;&gt;&gt; t._bar23 From the example, we can see that although _bar for internal use, we can also get it from outside. Because it is just a hint, not enforce by Python. 2. var_(single underscore)Sometimes, we already use a suitable name for others or some keyword we can not use as a variable name, so we can user var_ instead of it. &gt;&gt;&gt; def test(class_):... print("%s" %class_)...&gt;&gt;&gt; test("hello class_")hello class_ For the instance, we can not use class as a variable name, but we can use class_ as a variable name. 3. __var(double underscore)A double underscore prefix cause Python rewrite the attribute name. This is also called name mangling, the Python interpreter change variable name in order to make it harder create collisions when the class is extended later, it also looks like private attribute. &gt;&gt;&gt; class Test:... def __init__(self):... self.foo=11... self._bar = 23... self.__baz = 23...&gt;&gt;&gt; t1 = Test()&gt;&gt;&gt; dir(t1)['_Test__baz', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_bar', 'foo']&gt;&gt;&gt; class ExtendedTest(Test):... def __init__(self):... super().__init__()... self.foo = "overrideden"... self._bar = "overrideden"... self.__baz = "overrideden"...&gt;&gt;&gt; t2 = ExtendedTest()&gt;&gt;&gt; dir(t2)['_ExtendedTest__baz', '_Test__baz', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_bar', 'foo']&gt;&gt;&gt; t2.foo'overrideden'&gt;&gt;&gt; t2._bar'overrideden'&gt;&gt;&gt; t2.__bazTraceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;AttributeError: 'ExtendedTest' object has no attribute '__baz'&gt;&gt;&gt; t2._ExtendedTest__baz'overrideden' From the example, we can see that variables name with double underscore prefix changed to another name inside, and we can not get the variable with the name we assigned to it, we should use the name which the Python interpreter assign to it. But, the amazing thing is our code can use the name we assigned inside of the class. Let\’s get another example: &gt;&gt;&gt; class ManglingTest:... def __init__(self):... self.__mangled = "hello"... def get_mangled(self):... return self.__mangled...&gt;&gt;&gt; mtest = ManglingTest()# we can not get __mangled outside of ManglingTest class&gt;&gt;&gt; mtest.__mangledTraceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;AttributeError: 'ManglingTest' object has no attribute '__mangled'# ManglingTest method can get __mangled inside of ManglingTest class&gt;&gt;&gt; mtest.get_mangled()'hello' 4. __var__(double underscore)Name Mangling doesn\’t apply for variable name starts and ends with double underscore. Variables name start and end with double underscore means the variable is constructor(like __init__) or can be call by outside(like public attribute). Normally, we don\’t use start and end double underscore to name our variables. class PrefixPostfixTest: def __init__(self): self.__bam__ = 42&gt;&gt;&gt; PrefixPostfixTest().__bam__42 5. _ (just a single underscore)_ is just temporary variables, and we don’t actually use it. it looks like placeholder variables. # temporary variables example &gt;&gt;&gt; for _ in range(32):... print('Hello, World.')# placeholder variables example&gt;&gt;&gt; car = ('red', 'auto', 12, 3812.4)&gt;&gt;&gt; color, _, _, mileage = car&gt;&gt;&gt; color'red'&gt;&gt;&gt; mileage3812.4&gt;&gt;&gt; _12 Reference: The Meaning of Underscores in Python]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Time Complexity - Big O]]></title>
      <url>%2F2019%2F06%2F18%2FTime-Complexity%2F</url>
      <content type="text"><![CDATA[Data structure and sorting algorithm time complexity. 1. Array Access: O(1) Insert: O(N) Delete: O(N) 2. Linked List Access: O(N) Insert: O(1) Delete: O(1) 3. Stack(First In Last Out) Access: O(N) Insert: O(1) Delete: O(1) 4. Queue(First In First Out) Access: O(N) Insert: O(1) Delete: O(1) Time Complexity Compare Data Structure Time Complexity Array Sorting Algorithm Time Complexity]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python3 Brief Note]]></title>
      <url>%2F2019%2F06%2F16%2FPython3-Brief-Note%2F</url>
      <content type="text"><![CDATA[Some knowledge hard to understand in Python. 1. List Comprehension# if item in list is even, return item*item[ i*i for i in range(1,11) if(i%2==0)]# result[4, 16, 36, 64, 100] 2. Dict Comprehension# code without dict comprehensionz_name = &#123;"A", "B","C"&#125;z_num = &#123;&#125;for i in z_name: z_num[i]=0 # code with dict comprehensionz_name = &#123;"A","B","C"&#125;z_num = [ i:0 for i in z_name] 3. Exception# concepttry: &lt;monitor code&gt;except Exception: &lt;exception code&gt;finally: &lt;excute code whether or not there is an exception&gt; # example 1try: year = int(input("PLS enter a year"))except ValueError: print("Please enter an INT for the input box.") # example 2try: fo = open("name.txt")except Exception as e: print(e)finally: fo.close() 4. Variable Parameters for Function# exampledef howlong(first, *others): return len(first) + len(others)print(howlong("a", "def", "ghijklmn", "opq"))# result4 5. Variable Scope# example without globalx = 123def func(): x = 456 print("Inside X is: %d" %x)func()print("Outside X is: %d" %x)# resultInside X is: 456Outside X is: 123 # example with globalx = 123def func(): global x x = 456 print("Inside X is: %d" %x)func()print("Outside X is: %d" %x)# resultInside X is: 456Outside X is: 456 6. lambda expression# example&gt;&gt;&gt; g = lambda x:x+1&gt;&gt;&gt; g(3)4&gt;&gt;&gt; g(5)6# filter examplea = [1,2 ,3,4,5,6,7]print(list(filter(lambda x:x&gt;4, a)))# result[5, 6, 7]# map example 1a = [1,2 ,3,4,5,6,7]print(list(map(lambda x:x*x, a)))# result[1, 4, 9, 16, 25, 36, 49]# map example 2a = [1,2 ,3,4,5,6,7]b = [11,22,33,44,55,66,77]print(list(map(lambda x,y:x*y, a, b)))# result[11, 44, 99, 176, 275, 396, 539]# reduce example&gt;&gt;&gt; from functools import reduce&gt;&gt;&gt; reduce(lambda x,y:x+y, [7,8,9],1)25&gt;&gt;&gt; 1+7+8+925&gt;&gt;&gt; reduce(lambda x,y:x+y, [7,8,9],2)26&gt;&gt;&gt; 2+7+8+926# zip example 7. Closure# exampledef func(): a = 1 b = 2 return a+bdef sum(a): def add(b): return a+b return addnum1 = func()num2 = sum(1)print(type(num1))print(type(num2))print(num1)print(num2(2))# result&lt;class 'int'&gt;&lt;class 'function'&gt; # this is "add" function33 8. Decorator# exampleimport timedef timer(func): def wrapper(): start_time = time.time() func() stop_time = time.time() print("Total time is: %s second." %(stop_time - start_time)) return wrapper@timerdef i_can_sleep(): time.sleep(3)i_can_sleep()# resultTotal time is: 3.0007171630859375 second. 9. Class# exampleclass Player(): def __init__(self, name, life): self.__name = name self.life = life def print_role(self): print("name is: %s, life is %d" %(self.__name, self.life))p1 = Player("Boss",100)p1.print_role()# resultname is: Boss, life is 100 10. Inheritance# exampleclass Animal(): def __init__(self, name): self.name = name def run(self): print("%s is running." %(self.name))class Dog(Animal): def __init__(self, name): # self.name = name super().__init__(name)class Cat(Animal): passt1 = Animal("animal")print(t1.name)print(t1.run())t2 = Dog("dog")print(t2.name)print(t2.run())t3 = Cat("cat")print(t3.name)print(t3.run())# resultanimalanimal is running.dogdog is running.catcat is running. 11. With expression# example without exceptionclass Testwith(): def __enter__(self): print("run") def __exit__(self, exc_type, exc_val, exc_tb): print("exit")with Testwith(): print("Test is running")# resultrunTest is runningexit# example with exceptionclass Testwith(): def __enter__(self): print("run") def __exit__(self, exc_type, exc_val, exc_tb): if exc_tb is None: print("Normal exit") else: print("exit with error: %s" %exc_tb)with Testwith(): print("Test is running") raise NameError("Test NameError")# resultrunTest is runningexit with error: &lt;traceback object at 0x030830D0&gt;Traceback (most recent call last): File "hellopython.py", line 108, in &lt;module&gt; raise NameError("Test NameError")NameError: Test NameError 12. Multi Threads# example while MainThread end firstimport threadingimport timedef myThread(arg1, arg2): print(threading.current_thread().getName(), "start") time.sleep(1) print("%s %s" %(arg1, arg1)) print(threading.current_thread().getName(), "stop")for i in range(1,6,1): t1 = threading.Thread(target=myThread, args=(i, i+1)) t1.start()print(threading.current_thread().getName(), "end")# resultThread-1 startThread-2 startThread-3 startThread-4 startThread-5 startMainThread end3 3Thread-3 stop2 2Thread-2 stop1 1Thread-1 stop4 4Thread-4 stop5 5Thread-5 stop# example while SubThread end firstimport threadingclass MyThread(threading.Thread): def run(self): print(threading.current_thread().getName(),"Start") print("Run") print(threading.current_thread().getName(), "Stop")t1 = MyThread()t1.start()t1.join()print(threading.current_thread().getName(), "End")# resultThread-1 StartRunThread-1 StopMainThread End 13. Queue&gt;&gt;&gt; import queue&gt;&gt;&gt; q = queue.Queue()&gt;&gt;&gt; q.put(1)&gt;&gt;&gt; q.put(2)&gt;&gt;&gt; q.put(3)&gt;&gt;&gt; q.put(4)&gt;&gt;&gt; q.get()1&gt;&gt;&gt; q.get()2&gt;&gt;&gt; q.get()3&gt;&gt;&gt; q.get()4 14. Producer &amp; Consumer problem# basic codefrom threading import Thread, current_threadimport timeimport randomfrom queue import Queuequeue = Queue(5)class ProducerThread(Thread): def run(self): name = current_thread().getName() nums = range(100) global queue while True: num = random.choice(nums) queue.put(num) print("Producer %s produces %s data" %(name, num)) t = random.randint(1,3) time.sleep(t) print("Producer %s sleeps %s seconds" %(name, t))class ConsumerThread(Thread): def run(self): name = current_thread().getName() global queue while True: num = queue.get() queue.task_done() print("Consumer %s consumers %s data" %(name, num)) t = random.randint(1,5) time.sleep(t) print("Consumer %s sleeps %s seconds" %(name, t))# Consumer more than Producerp1 = ProducerThread(name="p1")p1.start()c1 = ConsumerThread(name="c1"prop)c1.start()c2 = ConsumerThread(name="c2")c2.start()# resultProducer p1 produces 73 dataConsumer c1 consumers 73 dataProducer p1 sleeps 1 secondsProducer p1 produces 51 dataConsumer c2 consumers 51 dataConsumer c1 sleeps 2 secondsProducer p1 sleeps 3 secondsProducer p1 produces 7 dataConsumer c1 consumers 7 dataConsumer c1 sleeps 1 secondsProducer p1 sleeps 1 secondsProducer p1 produces 30 dataConsumer c1 consumers 30 dataConsumer c2 sleeps 5 secondsProducer p1 sleeps 1 seconds# Producer more than Consumerp1 = ProducerThread(name="p1")p1.start()p2 = ProducerThread(name="p2")p2.start()p3 = ProducerThread(name="p3")p3.start()c1 = ConsumerThread(name="c1")c1.start()c2 = ConsumerThread(name="c2")c2.start()#resultProducer p1 produces 32 dataProducer p2 produces 4 dataProducer p3 produces 17 dataConsumer c1 consumers 32 dataConsumer c2 consumers 4 dataProducer p1 sleeps 2 secondsProducer p1 produces 79 dataProducer p2 sleeps 2 secondsProducer p2 produces 33 dataConsumer c1 sleeps 3 secondsConsumer c1 consumers 17 dataProducer p1 sleeps 1 secondsProducer p3 sleeps 3 secondsProducer p3 produces 55 dataProducer p1 produces 1 dataConsumer c2 sleeps 4 secondsConsumer c2 consumers 79 dataProducer p2 sleeps 3 secondsProducer p2 produces 52 dataProducer p3 sleeps 2 secondsProducer p3 produces 34 data 15. Collect Data from Website# example 1from urllib import requesturl = "https://www.google.com/"res = request.urlopen(url, timeout=1)print(res.read())# example 2from urllib import requestfrom urllib import parse# getres1 = request.urlopen("http://httpbin.org/get", timeout=1)print(res1.read())# postdata = bytes(parse.urlencode(&#123;"world":"hello"&#125;), encoding="utf8")print(data)res2 = request.urlopen("http://httpbin.org/post", data=data)print(res2.read().decode("utf-8"))]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Yum install python3]]></title>
      <url>%2F2019%2F02%2F21%2FYum-install-python3%2F</url>
      <content type="text"><![CDATA[This is just a note for me to reuse in the future, this post just for CentOS 7.Original resource from Chinese: click here. Install EPEL and IUS resource: yum install epel-release -yyum install https://centos7.iuscommunity.org/ius-release.rpm -y Install Python3.6 yum install python36u -y Create a soft link to python ln -s /bin/python3.6 /bin/python3 Be careful, you can’t link /bin/python3 to /bin/python, yum should use python2, it can’t use python3. Of cause, we always use pip3 for python3, so we also need to install pip3.6(the same version with Python3.6) yum install python36u-pip -y Create a soft link from pip3.6 to pip ln -s /bin/pip3.6 /bin/pip3]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CSS组合器]]></title>
      <url>%2F2018%2F11%2F08%2FCSS%E7%BB%84%E5%90%88%E5%99%A8%2F</url>
      <content type="text"><![CDATA[CSS的几种组合选择器总是让我非常的迷糊，所以自己把别人总结的保存一下。 文章来源：https://developer.mozilla.org/zh-CN/docs/Learn/CSS/Introduction_to_CSS/Combinators_and_multiple_selectors 名称 组合器 选择 A,B 匹配满足A（和/或）B的任意元素（参见下方 同一规则集上的多个选择器）. 后代选择器 A B 匹配B元素，满足条件：B是A的后代结点（B是A的子节点，或者A的子节点的子节点） 子选择器 A &gt; B 匹配B元素，满足条件：B是A的直接子节点 相邻兄弟选择器 A + B 匹配B元素，满足条件：B是A的下一个兄弟节点（AB有相同的父结点，并且B紧跟在A的后面） 通用兄弟选择器 A ~ B 匹配B元素，满足条件：B是A之后的兄弟节点中的任意一个（AB有相同的父节点，B在A之后，但不一定是紧挨着A）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CSS常用选择器]]></title>
      <url>%2F2018%2F10%2F30%2FCSS%E5%B8%B8%E7%94%A8%E9%80%89%E6%8B%A9%E5%99%A8%2F</url>
      <content type="text"><![CDATA[CSS的几种选择器总是让我非常的迷糊，所以我把别人总结的拿来保存一下。 文章来源：https://developer.mozilla.org/zh-CN/docs/Learn/Getting_started_with_the_web/CSS_basics 选择器名称 选择的内容 示例 元素选择器（有时也称作标签或类型选择器） 所有指定类型的 HTML 元素 p选择 &lt;p&gt; 标识（ID）选择器 页面中指定标识的元素（在一个 HTML 页面中，每个标识只被允许指定到一个元素） #my-id选择 &lt;p id=&quot;my-id&quot;&gt; 或 &lt;a id=&quot;my-id&quot;&gt; 类别选择器 页面中指定类别的元素（一个页面中可以出现多个类别实例） .my-class选择 &lt;p class=&quot;my-class&quot;&gt; 和 &lt;a class=&quot;my-class&quot;&gt; 属性选择器 页面中拥有指定属性的元素 img[src]选择 &lt;img src=&quot;myimage.png&quot;&gt; 而不是 &lt;img&gt; 伪类选择器 指定的元素，但是需要在特殊的状态，比如悬停 a:hover选择 &lt;a&gt;, 但是只在鼠标悬停在链接上时]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[SHOW PROCESSLIST和SHOW PROFILE的Status整理（英文官方文档版）]]></title>
      <url>%2F2018%2F04%2F02%2FSHOW-PROCESSLIST%E5%92%8CSHOW-PROFILE%E7%9A%84Status%E6%95%B4%E7%90%86%EF%BC%88%E8%8B%B1%E6%96%87%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E7%89%88%EF%BC%89%2F</url>
      <content type="text"><![CDATA[MySQL官方文档中的status存在于好几个小节中，每个status的英文解释其实一般是可以看的懂的，主要是寻找相应的status有点困难，所以我将所有的status整理在一个页面，直接Ctrl+F就可以查找到相应的status啦。 Thread CommandA thread can have any of the following Command values: Binlog DumpThis is a thread on a master server for sending binary log contents to a slave server. Change userThe thread is executing a change-user operation. Close stmtThe thread is closing a prepared statement. ConnectA replication slave is connected to its master. Connect OutA replication slave is connecting to its master. Create DBThe thread is executing a create-database operation. DaemonThis thread is internal to the server, not a thread that services a client connection. DebugThe thread is generating debugging information. Delayed insertThe thread is a delayed-insert handler. Drop DBThe thread is executing a drop-database operation. ErrorExecuteThe thread is executing a prepared statement. FetchThe thread is fetching the results from executing a prepared statement. Field ListThe thread is retrieving information for table columns. Init DBThe thread is selecting a default database. KillThe thread is killing another thread. Long DataThe thread is retrieving long data in the result of executing a prepared statement. PingThe thread is handling a server-ping request. PrepareThe thread is preparing a prepared statement. ProcesslistThe thread is producing information about server threads. QueryThe thread is executing a statement. QuitThe thread is terminating. RefreshThe thread is flushing table, logs, or caches, or resetting status variable or replication server information. Register SlaveThe thread is registering a slave server. Reset stmtThe thread is resetting a prepared statement. Set optionThe thread is setting or resetting a client statement-execution option. ShutdownThe thread is shutting down the server. SleepThe thread is waiting for the client to send a new statement to it. StatisticsThe thread is producing server-status information. Table DumpThe thread is sending table contents to a slave server. TimeUnused. General ThreadThe following list describes thread State values that are associated with general query processing and not more specialized activities such as replication. Many of these are useful only for finding bugs in the server. After createThis occurs when the thread creates a table (including internal temporary tables), at the end of the function that creates the table. This state is used even if the table could not be created due to some error. altering tableThe server is in the process of executing an in-place ALTER TABLE. AnalyzingThe thread is calculating a MyISAM table key distributions (for example, for ANALYZE TABLE). checking permissionsThe thread is checking whether the server has the required privileges to execute the statement. Checking tableThe thread is performing a table check operation. cleaning upThe thread has processed one command and is preparing to free memory and reset certain state variables. closing tablesThe thread is flushing the changed table data to disk and closing the used tables. This should be a fast operation. If not, verify that you do not have a full disk and that the disk is not in very heavy use. committing alter table to storage engineThe server has finished an in-place ALTER TABLE and is committing the result. converting HEAP to MyISAMThe thread is converting an internal temporary table from a MEMORY table to an on-disk MyISAM table. copy to tmp tableThe thread is processing an ALTER TABLE statement. This state occurs after the table with the new structure has been created but before rows are copied into it. Copying to group tableIf a statement has different ORDER BY and GROUP BY criteria, the rows are sorted by group and copied to a temporary table. Copying to tmp tableThe server is copying to a temporary table in memory. Copying to tmp table on diskThe server is copying to a temporary table on disk. The temporary result set has become too large (see Section 8.4.4, “Internal Temporary Table Use in MySQL”). Consequently, the thread is changing the temporary table from in-memory to disk-based format to save memory. Creating indexThe thread is processing ALTER TABLE … ENABLE KEYS for a MyISAM table. Creating sort indexThe thread is processing a SELECT that is resolved using an internal temporary table. creating tableThe thread is creating a table. This includes creation of temporary tables. Creating tmp tableThe thread is creating a temporary table in memory or on disk. If the table is created in memory but later is converted to an on-disk table, the state during that operation will be Copying to tmp table on disk. deleting from main tableThe server is executing the first part of a multiple-table delete. It is deleting only from the first table, and saving columns and offsets to be used for deleting from the other (reference) tables. deleting from reference tablesThe server is executing the second part of a multiple-table delete and deleting the matched rows from the other tables. discard_or_import_tablespaceThe thread is processing an ALTER TABLE … DISCARD TABLESPACE or ALTER TABLE … IMPORT TABLESPACE statement. endThis occurs at the end but before the cleanup of ALTER TABLE, CREATE VIEW, DELETE, INSERT, SELECT, or UPDATE statements. executingThe thread has begun executing a statement. Execution of init_commandThe thread is executing statements in the value of the init_command system variable. freeing itemsThe thread has executed a command. Some freeing of items done during this state involves the query cache. This state is usually followed by cleaning up. Flushing tablesThe thread is executing FLUSH TABLES and is waiting for all threads to close their tables. FULLTEXT initializationThe server is preparing to perform a natural-language full-text search. initThis occurs before the initialization of ALTER TABLE, DELETE, INSERT, SELECT, or UPDATE statements. Actions taken by the server in this state include flushing the binary log, the InnoDB log, and some query cache cleanup operations.For the end state, the following operations could be happening: Removing query cache entries after data in a table is changed Writing an event to the binary log Freeing memory buffers, including for blobs KilledSomeone has sent a KILL statement to the thread and it should abort next time it checks the kill flag. The flag is checked in each major loop in MySQL, but in some cases it might still take a short time for the thread to die. If the thread is locked by some other thread, the kill takes effect as soon as the other thread releases its lock. logging slow queryThe thread is writing a statement to the slow-query log. NULLThis state is used for the SHOW PROCESSLIST state. loginThe initial state for a connection thread until the client has been authenticated successfully. manage keysThe server is enabling or disabling a table index. Opening tables, Opening tableThe thread is trying to open a table. This is should be very fast procedure, unless something prevents opening. For example, an ALTER TABLE or a LOCK TABLE statement can prevent opening a table until the statement is finished. It is also worth checking that your table_open_cache value is large enough. optimizingThe server is performing initial optimizations for a query. preparingThis state occurs during query optimization. preparing for alter tableThe server is preparing to execute an in-place ALTER TABLE. Purging old relay logsThe thread is removing unneeded relay log files. query endThis state occurs after processing a query but before the freeing items state. Reading from netThe server is reading a packet from the network. Removing duplicatesThe query was using SELECT DISTINCT in such a way that MySQL could not optimize away the distinct operation at an early stage. Because of this, MySQL requires an extra stage to remove all duplicated rows before sending the result to the client. removing tmp tableThe thread is removing an internal temporary table after processing a SELECT statement. This state is not used if no temporary table was created. renameThe thread is renaming a table. rename result tableThe thread is processing an ALTER TABLE statement, has created the new table, and is renaming it to replace the original table. Reopen tablesThe thread got a lock for the table, but noticed after getting the lock that the underlying table structure changed. It has freed the lock, closed the table, and is trying to reopen it. Repair by sortingThe repair code is using a sort to create indexes. Repair doneThe thread has completed a multi-threaded repair for a MyISAM table. Repair with keycacheThe repair code is using creating keys one by one through the key cache. This is much slower than Repair by sorting. Rolling backThe thread is rolling back a transaction. Saving stateFor MyISAM table operations such as repair or analysis, the thread is saving the new table state to the .MYI file header. State includes information such as number of rows, the AUTO_INCREMENT counter, and key distributions. Searching rows for updateThe thread is doing a first phase to find all matching rows before updating them. This has to be done if the UPDATE is changing the index that is used to find the involved rows. Sending dataThe thread is reading and processing rows for a SELECT statement, and sending data to the client. Because operations occurring during this state tend to perform large amounts of disk access (reads), it is often the longest-running state over the lifetime of a given query. setupThe thread is beginning an ALTER TABLE operation. Sorting for groupThe thread is doing a sort to satisfy a GROUP BY. Sorting for orderThe thread is doing a sort to satisfy a ORDER BY. Sorting indexThe thread is sorting index pages for more efficient access during a MyISAM table optimization operation. Sorting resultFor a SELECT statement, this is similar to Creating sort index, but for nontemporary tables. statisticsThe server is calculating statistics to develop a query execution plan. If a thread is in this state for a long time, the server is probably disk-bound performing other work. System lockThe thread is going to request or is waiting for an internal or external system lock for the table. If this state is being caused by requests for external locks and you are not using multiple mysqld servers that are accessing the same MyISAM tables, you can disable external system locks with the –skip-external-locking option. However, external locking is disabled by default, so it is likely that this option will have no effect. For SHOW PROFILE, this state means the thread is requesting the lock (not waiting for it). updateThe thread is getting ready to start updating the table. UpdatingThe thread is searching for rows to update and is updating them. updating main tableThe server is executing the first part of a multiple-table update. It is updating only the first table, and saving columns and offsets to be used for updating the other (reference) tables. updating reference tablesThe server is executing the second part of a multiple-table update and updating the matched rows from the other tables. User lockThe thread is going to request or is waiting for an advisory lock requested with a GET_LOCK() call. For SHOW PROFILE, this state means the thread is requesting the lock (not waiting for it). User sleepThe thread has invoked a SLEEP() call. Waiting for commit lockFLUSH TABLES WITH READ LOCK is waiting for a commit lock. Waiting for global read lockFLUSH TABLES WITH READ LOCK is waiting for a global read lock or the global read_only system variable is being set. Waiting for tables, Waiting for table flushThe thread got a notification that the underlying structure for a table has changed and it needs to reopen the table to get the new structure. However, to reopen the table, it must wait until all other threads have closed the table in question.This notification takes place if another thread has used FLUSH TABLES or one of the following statements on the table in question: FLUSH TABLES tbl_name, ALTER TABLE, RENAME TABLE, REPAIR TABLE, ANALYZE TABLE, or OPTIMIZE TABLE. Waiting for lock_type lockThe server is waiting to acquire a lock, where lock_type indicates the type of lock: Waiting for event metadata lock Waiting for global read lock Waiting for schema metadata lock Waiting for stored function metadata lock Waiting for stored procedure metadata lock Waiting for table level lock Waiting for table metadata lock Waiting for trigger metadata lock Waiting on condA generic state in which the thread is waiting for a condition to become true. No specific state information is available. Writing to netThe server is writing a packet to the network. Delayed-Insert ThreadThese thread states are associated with processing for DELAYED inserts (see Section 13.2.5.2, “INSERT DELAYED Syntax”). Some states are associated with connection threads that process INSERT DELAYED statements from clients. Other states are associated with delayed-insert handler threads that insert the rows. There is a delayed-insert handler thread for each table for which INSERT DELAYED statements are issued.States associated with a connection thread that processes an INSERT DELAYED statement from the client: allocating local tableThe thread is preparing to feed rows to the delayed-insert handler thread. Creating delayed handlerThe thread is creating a handler for DELAYED inserts. got handler lockThis occurs before the allocating local table state and after the waiting for handler lock state, when the connection thread gets access to the delayed-insert handler thread. got old tableThis occurs after the waiting for handler open state. The delayed-insert handler thread has signaled that it has ended its initialization phase, which includes opening the table for delayed inserts. storing row into queueThe thread is adding a new row to the list of rows that the delayed-insert handler thread must insert. waiting for delay_listThis occurs during the initialization phase when the thread is trying to find the delayed-insert handler thread for the table, and before attempting to gain access to the list of delayed-insert threads. waiting for handler insertAn INSERT DELAYED handler has processed all pending inserts and is waiting for new ones. waiting for handler lockThis occurs before the allocating local table state when the connection thread waits for access to the delayed-insert handler thread. waiting for handler openThis occurs after the Creating delayed handler state and before the got old table state. The delayed-insert handler thread has just been started, and the connection thread is waiting for it to initialize.States associated with a delayed-insert handler thread that inserts the rows: insertThe state that occurs just before inserting rows into the table. rescheduleAfter inserting a number of rows, the delayed-insert thread sleeps to let other threads do work. upgrading lockA delayed-insert handler is trying to get a lock for the table to insert rows. Waiting for INSERTA delayed-insert handler is waiting for a connection thread to add rows to the queue (see storing row into queue). Query Cache ThreadThese thread states are associated with the query cache (see Section 8.10.3, “The MySQL Query Cache”). checking privileges on cached queryThe server is checking whether the user has privileges to access a cached query result. checking query cache for queryThe server is checking whether the current query is present in the query cache. invalidating query cache entriesQuery cache entries are being marked invalid because the underlying tables have changed. sending cached result to clientThe server is taking the result of a query from the query cache and sending it to the client. storing result in query cacheThe server is storing the result of a query in the query cache. Waiting for query cache lockThis state occurs while a session is waiting to take the query cache lock. This can happen for any statement that needs to perform some query cache operation, such as an INSERT or DELETE that invalidates the query cache, a SELECT that looks for a cached entry, RESET QUERY CACHE, and so forth. Replication Master ThreadThe following list shows the most common states you may see in the State column for the master’s Binlog Dump thread. If you see no Binlog Dump threads on a master server, this means that replication is not running—that is, that no slaves are currently connected. Sending binlog event to slaveBinary logs consist of events, where an event is usually an update plus some other information. The thread has read an event from the binary log and is now sending it to the slave. Finished reading one binlog; switching to next binlogThe thread has finished reading a binary log file and is opening the next one to send to the slave. Master has sent all binlog to slave; waiting for binlog to be updatedThe thread has read all outstanding updates from the binary logs and sent them to the slave. The thread is now idle, waiting for new events to appear in the binary log resulting from new updates occurring on the master. Waiting to finalize terminationA very brief state that occurs as the thread is stopping. Replication Slave I/O ThreadThe following list shows the most common states you see in the State column for a slave server I/O thread. This state also appears in the Slave_IO_State column displayed by SHOW SLAVE STATUS, so you can get a good view of what is happening by using that statement. Waiting for master updateThe initial state before Connecting to master. Connecting to masterThe thread is attempting to connect to the master. Checking master versionA state that occurs very briefly, after the connection to the master is established. Registering slave on masterA state that occurs very briefly after the connection to the master is established. Requesting binlog dumpA state that occurs very briefly, after the connection to the master is established. The thread sends to the master a request for the contents of its binary logs, starting from the requested binary log file name and position. Waiting to reconnect after a failed binlog dump requestIf the binary log dump request failed (due to disconnection), the thread goes into this state while it sleeps, then tries to reconnect periodically. The interval between retries can be specified using the CHANGE MASTER TO statement. Reconnecting after a failed binlog dump requestThe thread is trying to reconnect to the master. Waiting for master to send eventThe thread has connected to the master and is waiting for binary log events to arrive. This can last for a long time if the master is idle. If the wait lasts for slave_net_timeout seconds, a timeout occurs. At that point, the thread considers the connection to be broken and makes an attempt to reconnect. Queueing master event to the relay logThe thread has read an event and is copying it to the relay log so that the SQL thread can process it. Waiting to reconnect after a failed master event readAn error occurred while reading (due to disconnection). The thread is sleeping for the number of seconds set by the CHANGE MASTER TO statement (default 60) before attempting to reconnect. Reconnecting after a failed master event readThe thread is trying to reconnect to the master. When connection is established again, the state becomes Waiting for master to send event. Waiting for the slave SQL thread to free enough relay log spaceYou are using a nonzero relay_log_space_limit value, and the relay logs have grown large enough that their combined size exceeds this value. The I/O thread is waiting until the SQL thread frees enough space by processing relay log contents so that it can delete some relay log files. Waiting for slave mutex on exitA state that occurs briefly as the thread is stopping. Replication Slave SQL ThreadThe following list shows the most common states you may see in the State column for a slave server SQL thread: Waiting for the next event in relay logThe initial state before Reading event from the relay log. Reading event from the relay logThe thread has read an event from the relay log so that the event can be processed. Making temporary file (append) before replaying LOAD DATA INFILEThe thread is executing a LOAD DATA INFILE statement and is appending the data to a temporary file containing the data from which the slave will read rows. Making temporary file (create) before replaying LOAD DATA INFILEThe thread is executing a LOAD DATA INFILE statement and is creating a temporary file containing the data from which the slave will read rows. This state can only be encountered if the original LOAD DATA INFILE statement was logged by a master running a version of MySQL earlier than version 5.0.3. Slave has read all relay log; waiting for more updatesThe thread has processed all events in the relay log files, and is now waiting for the I/O thread to write new events to the relay log. Waiting for slave mutex on exitA very brief state that occurs as the thread is stopping. Waiting until MASTER_DELAY seconds after master executed eventThe SQL thread has read an event but is waiting for the slave delay to lapse. This delay is set with the MASTER_DELAY option of CHANGE MASTER TO. Killing slaveThe thread is processing a STOP SLAVE statement. Waiting for an event from CoordinatorUsing the multi-threaded slave (slave_parallel_workers is greater than 1), one of the slave worker threads is waiting for an event from the coordinator thread. The Info column for the SQL thread may also show the text of a statement. This indicates that the thread has read an event from the relay log, extracted the statement from it, and may be executing it. Replication Slave Connection ThreadThese thread states occur on a replication slave but are associated with connection threads, not with the I/O or SQL threads. Changing masterThe thread is processing a CHANGE MASTER TO statement. Killing slaveThe thread is processing a STOP SLAVE statement. Opening master dump tableThis state occurs after Creating table from master dump. Reading master dump table dataThis state occurs after Opening master dump table. Rebuilding the index on master dump tableThis state occurs after Reading master dump table data. MySQL Cluster ThreadCommitting events to binlogOpening mysql.ndb_apply_statusProcessing eventsThe thread is processing events for binary logging. Processing events from schema tableThe thread is doing the work of schema replication. Shutting downSyncing ndb table schema operation and binlogThis is used to have a correct binary log of schema operations for NDB. Waiting for event from ndbclusterThe server is acting as an SQL node in a MySQL Cluster, and is connected to a cluster management node. Waiting for first event from ndbclusterWaiting for ndbcluster binlog update to reach current positionWaiting for ndbcluster to startWaiting for schema epochThe thread is waiting for a schema epoch (that is, a global checkpoint). Waiting for allowed to take ndbcluster global schema lockThe thread is waiting for permission to take a global schema lock. Waiting for ndbcluster global schema lockThe thread is waiting for a global schema lock held by another thread to be released. Event Scheduler ThreadThese states occur for the Event Scheduler thread, threads that are created to execute scheduled events, or threads that terminate the scheduler. ClearingThe scheduler thread or a thread that was executing an event is terminating and is about to end. InitializedThe scheduler thread or a thread that will execute an event has been initialized. Waiting for next activationThe scheduler has a nonempty event queue but the next activation is in the future. Waiting for scheduler to stopThe thread issued SET GLOBAL event_scheduler=OFF and is waiting for the scheduler to stop. Waiting on empty queueThe scheduler’s event queue is empty and it is sleeping.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux的账号管理]]></title>
      <url>%2F2018%2F04%2F02%2FLinux%E7%9A%84%E8%B4%A6%E5%8F%B7%E7%AE%A1%E7%90%86%2F</url>
      <content type="text"><![CDATA[记忆力不好，对于Linux最基本的账号管理都记不住，只能默默的写下来，此处列出常用的让自己记住。。。 查看用户相关信息 /etc/passwd查看用户，/etc/shadow查看密码，/etc/group查看组 id命令查看自己或他人相关的UID/GID信息# id命令看起来有点像这样[test@localhost ~]$ id test1uid=1001(test1) gid=1001(test1) 组=1001(test1)[test@localhost ~]$ id testuid=1000(test) gid=1000(test) 组=1000(test)[test@localhost ~]$ id rootuid=0(root) gid=0(root) 组=0(root)# 可以直接通过id查看当前用户的UID/GID[test@localhost ~]$ iduid=1000(test) gid=1000(test) 组=1000(test) 环境=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 新增用户# 只添加用户useradd test1# -g表示初始化的组，-G表示还可以加入的组[root@localhost ~]# useradd test2 -g test -G test2# useradd添加的账户是没有密码的，所以我们需要用passwd命令，修改账户密码[root@localhost ~]# passwd test2 更改用户 test2 的密码 。新的 密码：无效的密码： 密码少于 8 个字符重新输入新的 密码：passwd：所有的身份验证令牌已经成功更新。 修改用户# 为账户test2修改注释，接着可在/etc/passwd文件中查看相应的修改[root@localhost ~]# usermod -c &apos;just test&apos; test2该语句执行前：test2:x:1002:1000::/home/test2:/bin/bash该语句执行后：test2:x:1002:1000:just test:/home/test2:/bin/bash# 修改账户test2的初始化组，此处test1组的GID为1001[root@localhost ~]# usermod -g test1 test2该语句执行前：test2:x:1002:1000:just test:/home/test2:/bin/bash该语句执行后：test2:x:1002:1001:just test:/home/test2:/bin/bash# 修改账户test2的次要组[root@localhost ~]# usermod -G test test2 删除用户# 仅删除test2用户[root@localhost ~]# userdel test2# 删除test2用户及其home[root@localhost ~]# userdel -r test2 使用userdel需要谨慎，因为也许可能你存在文件的user是test2用户创建的，如果删除test2用户，用户名就会变成一串数字。 新增用户组# 只添加用户组[root@localhost ~]# groupadd group1# 创建用户组时，指定GID[root@localhost ~]# groupadd -g 324 group2 修改用户组# 更改用户组的GID [root@localhost ~]# groupmod -g 125 group2 删除用户组# 删除用户组[root@localhost ~]# groupdel group2# 为什么这个用户组删除不了呢[root@localhost ~]# groupdel test1groupdel：不能移除用户“test1”的主组# 该用户组删除不了的原因为，它为某个账号的初始化组即initial group,所以此时删除不了，此时可将该账号的GID修改为其他或者删除该账号[root@localhost ~]# groupdel test1groupdel：不能移除用户“test1”的主组[root@localhost ~]# usermod -g 1000 test1[root@localhost ~]# groupdel test1 组管理员# 建立群组[root@localhost ~]# groupadd testgroup# 为该群组添加密码[root@localhost ~]# gpasswd testgroup正在修改 testgroup 组的密码新密码：请重新输入新密码：# 将test设为testgroup的组管理员[root@localhost ~]# gpasswd -A test testgroup#以test登陆，将自己和test2加入到testgroup群组中[test@localhost ~]$ gpasswd -a test testgroup正在将用户“test”加入到“testgroup”组中[test@localhost ~]$ gpasswd -a test2 testgroup正在将用户“test2”加入到“testgroup”组中]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java之final关键字]]></title>
      <url>%2F2017%2F10%2F31%2FJava%E4%B9%8Bfinal%E5%85%B3%E9%94%AE%E5%AD%97%2F</url>
      <content type="text"><![CDATA[还记得前面的static关键字吗？这次我们说说final关键字。 参考：腾讯课堂 一、为什么使用final关键字继承关系中最大的弊端是破坏了封装：子类可以访问父类的实现细节，而且可以通过方法覆盖的形式修改方法的实现细节。那么final就用来让你不可以做任何的更改，只能调用，不允许修改。 final可以修饰非抽象类/非抽象方法/变量。 注意：构造方法不能使用final修饰，因为构造方法不能被继承，它是最终的一个状态。 二、关于final final修饰的非抽象类final修饰的非抽象类是不能被子类继承的。 package com.wing.finaldemo;public class FinalDemo &#123; public static void main(String[] args)&#123; &#125;&#125;final class Superclass&#123;&#125;class Subclass extends Superclass&#123;&#125;//此时会出现如下的报错Cannot inherit from final 'com.wing.finaldemo.Superclass'即com.wing.finaldemo.Superclass使用了final修饰符，导致子类无法继承。 哪些类需要使用final来修饰呢？ 该类不是专门为继承而设计的 处于安全考虑，类的实现细节不许改动，不准修改源代码 确保该类不会在被拓展。 final修饰的非抽象方法final修饰的非抽象方法被称为最终的类，该方法不能被子类修改。 package com.wing.finaldemo;public class FinalDemo &#123; public static void main(String[] args)&#123; &#125;&#125;class Superclass&#123; public final void doWork()&#123; System.out.println("SuperClass doWork"); &#125;&#125;class Subclass extends Superclass&#123; public void doWork()&#123; System.out.println("SubClass doWork"); &#125;&#125;//此时会出现以下错误Error:(17, 17) java: com.wing.finaldemo.Subclass cannot override doWork() in com.wing.finaldemo.Superclass doWork(),overriden method is final. 哪些方法需要使用final修饰呢？ 在父类中提供的统一的方法不准子类通过Override来修改，只允许子类调用，不允许子类修改。 在构造器中调用的方法(初始化方法)，初始化方法一般为final修饰。 final修饰的变量final修饰的变量被称为常量，该变量只能赋值一次，不能再次被赋值。final是唯一可以修改局部变量的修饰符。final修饰基本类型变量：表示该变量的值不能改变，即不能用”=”赋值;final修饰引用类型变量：表示该引用变量的引用地址不能改变，而不是引用地址里面的内容不能变。 // 修改final修饰引用类型变量对应的引用地址的内容package com.wing.finaldemo;public class FinalDemo &#123; public static void main(String[] args)&#123; final Person p1 = new Person(); System.out.println(p1.info); p1.info = "Second Vlaue"; System.out.println(p1.info); &#125;&#125;class Person&#123; public String info="First value";&#125;//此时输出First valueSecond Vlaue// 可见：final修饰引用变量时，其引用地址里面的内容可以改变。//修改final修饰引用类型变量对应的引用地址改变public class FinalDemo &#123; public static void main(String[] args)&#123; final Person p1 = new Person(); System.out.println(p1.info); p1.info = "Second Vlaue"; System.out.println(p1.info); p1 = new Person(); &#125;&#125;class Person&#123; public String info="First value";&#125;// 此时编译报错Cannot assign a value to final variable 'p1' 哪些变量需要使用final修饰呢？当在程序中多个地方使用一个不变的变量，就将其定义为常量。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java之多态方法中的调用问题]]></title>
      <url>%2F2017%2F10%2F31%2FJava%E4%B9%8B%E5%A4%9A%E6%80%81%E6%96%B9%E6%B3%95%E4%B8%AD%E7%9A%84%E8%B0%83%E7%94%A8%E9%97%AE%E9%A2%98%2F</url>
      <content type="text"><![CDATA[Java的多态。嗯。是个很有意思的东西。 参考：腾讯课堂 多态方法调用情况存在四类，以doWork()方法为例： doWork()存在子类中，不存在父类中，结果调用子类的doWork()； package com.wing.multidemo;public class MultiDemo2 &#123; public static void main(String[] args)&#123; SuperClass s1 = new SubClass(); s1.doWork(); &#125;&#125;// 父类class SuperClass&#123;&#125;// 子类class SubClass extends SuperClass&#123; public void doWork()&#123; System.out.println("SubClass doWork"); &#125;&#125;//此时输出结果编译报错。因为编译时期需要去编译类型(Superclass)中找是否有doWork()方法，找到则编译通过，找不到则编译失败。 dowork()不存在子类中，存在父类中，结果调用父类的doWork()； package com.wing.multidemo;public class MultiDemo2 &#123; public static void main(String[] args)&#123; SuperClass s1 = new SubClass(); s1.doWork(); &#125;&#125;// 父类class SuperClass&#123; public void doWork()&#123; System.out.println("SuperClass doWork"); &#125;&#125;// 子类class SubClass extends SuperClass&#123;&#125;// 此时输出结果SuperClass doWork doWork()存在子类中，存在父类中，结果调用子类的doWork()；[就近原则] package com.wing.multidemo;public class MultiDemo2 &#123; public static void main(String[] args)&#123; SuperClass s1 = new SubClass(); s1.doWork(); &#125;&#125;// 父类class SuperClass&#123; public void doWork()&#123; System.out.println("SuperClass doWork"); &#125;&#125;// 子类class SubClass extends SuperClass&#123; public void doWork()&#123; System.out.println("SubClass doWork"); &#125;&#125;// 此时输出结果SubClass doWork此时是先从SubClass中找是否存在doWork()方法，再去SuperClass中找是否存在doWork()方法。 doWork()存在子类中(static方法)，存在父类中(方法)，结果调用子类的doWork()。[就近原则]注意，这种方式称之为”隐藏”，而不是”方法覆盖”。 package com.wing.multidemo;public class MultiDemo2 &#123; public static void main(String[] args)&#123; SuperClass s1 = new SubClass(); s1.doWork(); &#125;&#125;// 父类class SuperClass&#123; public static void doWork()&#123; System.out.println("SuperClass doWork"); &#125;&#125;// 子类class SubClass extends SuperClass&#123; public static void doWork()&#123; System.out.println("SubClass doWork"); &#125;&#125;// 此时输出结果SuperClass doWork解释：静态方法的调用，只需要类即可。如果使用对象来调用方法，其实使用对象的类(该代码中为SuperClass)调用静态方法。 总结: 多态调用方法时，首先方法会去子类中查找方法是否存在，再去父类中查找方法是否存在[方法必须存在父类中，否则多态编译将会失败。] 多态中静态方法的”重写”不叫重写，应该叫”方法隐藏”，为什么呢？因为静态方法的调用，只需要类即可，如果使用对象来调用静态方法，其实使用的是对象的类来调用静态方法的。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hive实用函数大全]]></title>
      <url>%2F2017%2F10%2F20%2FHive%E5%AE%9E%E7%94%A8%E5%87%BD%E6%95%B0%E5%A4%A7%E5%85%A8%2F</url>
      <content type="text"><![CDATA[为什么叫实用函数，因为有些不实用的函数没有写进文档中，哈哈。 参考文档： 《Apache Hive Cookbook》 Hive分析窗口函数(一) SUM,AVG,MIN,MAX # 示例表结构salesCREATE TABLE `sales`( `id` int, `fname` string, `state` string, `zip` int, `ip` string, `pid` string)# 示例数据sales0 Zena Tennessee 21550 192.168.56.101 PI_091 Elaine Alaska 6429 192.168.56.101 PI_032 Sage Nevada 8899 192.168.56.102 PI_033 Cade Missouri 11233 192.168.56.103 PI_064 Abra New Jersry 21500 192.168.56.101 PI_095 Stone Nebraska 3560 192.168.56.104 PI_086 Regina Tennessee 21550 192.168.56.105 PI_107 Donova New York 95234 192.168.56.106 PI_058 Aileen Illinois 68284 192.168.56.106 PI_029 Maraam Hawaii 95234 192.168.56.107 PI_07# 示例表结构poCREATE TABLE `po`( `cookieid` string, `createtime` string, `pv` int)# 示例数据pocookie1 2015-04-10 1cookie1 2015-04-11 5cookie1 2015-04-12 7cookie1 2015-04-13 3cookie1 2015-04-14 2cookie1 2015-04-15 4cookie1 2015-04-16 5 一、分析型函数 ROW_NUMBER语法： ROW_NUMBER() OVER (ORDER BY col)为每个分组记录返回一个排序的数字。 ROW_NUMBER() OVER (PARTITION BY col1 ORDER BY col2)按照col1分组，在分组内对col2进行排序并返回顺序数字。 hive&gt; select fname,pid,ip from sales;OKZena PI_09 192.168.56.101Elaine PI_03 192.168.56.101Sage PI_03 192.168.56.102Cade PI_06 192.168.56.103Abra PI_09 192.168.56.101Stone PI_08 192.168.56.104Regina PI_10 192.168.56.105Donova PI_05 192.168.56.106Aileen PI_02 192.168.56.106Maraam PI_07 192.168.56.107hive&gt; select fname,pid,ip,row_number() over (order by ip) from sales;Abra PI_09 192.168.56.101 1Elaine PI_03 192.168.56.101 2Zena PI_09 192.168.56.101 3Sage PI_03 192.168.56.102 4Cade PI_06 192.168.56.103 5Stone PI_08 192.168.56.104 6Regina PI_10 192.168.56.105 7Aileen PI_02 192.168.56.106 8Donova PI_05 192.168.56.106 9Maraam PI_07 192.168.56.107 10hive&gt; select fname,pid,ip,row_number() over (partition by pid order by ip) from sales;Aileen PI_02 192.168.56.106 1Elaine PI_03 192.168.56.101 1Sage PI_03 192.168.56.102 2Donova PI_05 192.168.56.106 1Cade PI_06 192.168.56.103 1Maraam PI_07 192.168.56.107 1Stone PI_08 192.168.56.104 1Abra PI_09 192.168.56.101 1Zena PI_09 192.168.56.101 2Regina PI_10 192.168.56.105 1 RANK RANK() OVER (ORDER BY col)和ROW_NUMBER()差不多，但是排序时候相同的字段会返回相同的数字。 RANK() OVER (PARTITION by col1 ORDER BY col2)和ROW_NUMBER()差不多，但是排序时候同等级的字段会返回相同的数字。 hive&gt; select fname,ip,rank() over (order by ip) from sales;Abra 192.168.56.101 1Elaine 192.168.56.101 1Zena 192.168.56.101 1Sage 192.168.56.102 4Cade 192.168.56.103 5Stone 192.168.56.104 6Regina 192.168.56.105 7Aileen 192.168.56.106 8Donova 192.168.56.106 8Maraam 192.168.56.107 10hive&gt; select fname,pid,ip,rank() over (partition by pid order by ip) from sales;Aileen PI_02 192.168.56.106 1Elaine PI_03 192.168.56.101 1Sage PI_03 192.168.56.102 2Donova PI_05 192.168.56.106 1Cade PI_06 192.168.56.103 1Maraam PI_07 192.168.56.107 1Stone PI_08 192.168.56.104 1Abra PI_09 192.168.56.101 1Zena PI_09 192.168.56.101 1Regina PI_10 192.168.56.105 1 DENSE_RANK和RANK()差不多，但是RANK()的排序数字存在空洞，见一-2-第一个示例。而DENSE_RANK()则不会存在排序数字空洞。 hive&gt; select fname,ip,dense_rank() over (order by ip) from sales;Abra 192.168.56.101 1Elaine 192.168.56.101 1Zena 192.168.56.101 1Sage 192.168.56.102 2Cade 192.168.56.103 3Stone 192.168.56.104 4Regina 192.168.56.105 5Aileen 192.168.56.106 6Donova 192.168.56.106 6Maraam 192.168.56.107 7 PERCENT_RANK PERCENT_RANK() OVER (ORDER BY col) 当前行的rank-1/总行数-1 PERCENT_RAANK() OVER (PARTITION BY col1 ORDER BY col2) 按照col1分组后，分组内当前行的rank-1/分组内的总行数-1 hive&gt; select id,ip,percent_rank() over (order by id)from sales;0 192.168.56.101 0.01 192.168.56.101 0.11111111111111112 192.168.56.102 0.22222222222222223 192.168.56.103 0.33333333333333334 192.168.56.101 0.44444444444444445 192.168.56.104 0.55555555555555566 192.168.56.105 0.66666666666666667 192.168.56.106 0.77777777777777788 192.168.56.106 0.88888888888888889 192.168.56.107 1.0hive&gt; select id,ip,percent_rank() over (partition by ip order by id)from sales;0 192.168.56.101 0.01 192.168.56.101 0.54 192.168.56.101 1.02 192.168.56.102 0.03 192.168.56.103 0.05 192.168.56.104 0.06 192.168.56.105 0.07 192.168.56.106 0.08 192.168.56.106 1.09 192.168.56.107 0.0 CUME_DIST CUME_DIST() OVER (ORDER BY col) 小于等于col当前值的行数/总行数。 CUME_DIST() OVER (PARTITION BY col1 ORDER BY col2) 按照col1分组后，分组内部小于等于col2当前值的行数/总行数。 hive&gt; select id,ip from sales;OK0 192.168.56.1011 192.168.56.1012 192.168.56.1023 192.168.56.1034 192.168.56.1015 192.168.56.1046 192.168.56.1057 192.168.56.1068 192.168.56.1069 192.168.56.107hive&gt; select id,ip,cume_dist() over (order by id)from sales;0 192.168.56.101 0.11 192.168.56.101 0.22 192.168.56.102 0.33 192.168.56.103 0.44 192.168.56.101 0.55 192.168.56.104 0.66 192.168.56.105 0.77 192.168.56.106 0.88 192.168.56.106 0.99 192.168.56.107 1.0hive&gt; select id,ip,cume_dist() over (partition by ip order by id)from sales;0 192.168.56.101 0.33333333333333331 192.168.56.101 0.66666666666666664 192.168.56.101 1.02 192.168.56.102 1.03 192.168.56.103 1.05 192.168.56.104 1.06 192.168.56.105 1.07 192.168.56.106 0.58 192.168.56.106 1.09 192.168.56.107 1.0 NTILE用于将分组数据按照顺序切分成n片，并返回当前切片值。 hive&gt; select id,ip, ntile(2) over (partition by ip order by id)from sales;0 192.168.56.101 11 192.168.56.101 14 192.168.56.101 22 192.168.56.102 13 192.168.56.103 15 192.168.56.104 16 192.168.56.105 17 192.168.56.106 18 192.168.56.106 29 192.168.56.107 1 二、窗口型函数 LEAD LEAD() OVER (PARTITION BY col1 ORDER BY col2) 按照col1分组后，返回结果集中的下一个col2 LAG OVER (PARTITION BY col1 ORDER BY col2) 按照col1分组后，返回结果集中的上一个col2 FIRST_VALUE OVER (PARTITION BY col1 ORDER BY col2) 按照col1分组后，返回结果集中的第一个col2 LAST_VALUE OVER (PARTITION BY col1 ORDER BY col2) 按照col1分组后，返回结果集中的最后一个col2 OVER 聚集OVER COUNT MIN MAX AVG OVER WITH PARTITION BY OVER WITH PARTITION BY and ORDER BY # 首先需要了解函数OVER(PARTITION BY col1 ORDER BY col2)SELECT cookieid,createtime,pv,SUM(pv) OVER(PARTITION BY cookieid ORDER BY createtime) AS pv1, -- 默认为从起点到当前行SUM(pv) OVER(PARTITION BY cookieid ORDER BY createtime ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS pv2, --从起点到当前行，结果同pv1SUM(pv) OVER(PARTITION BY cookieid) AS pv3, --分组内所有行SUM(pv) OVER(PARTITION BY cookieid ORDER BY createtime ROWS BETWEEN 3 PRECEDING AND CURRENT ROW) AS pv4, --当前行+往前3行SUM(pv) OVER(PARTITION BY cookieid ORDER BY createtime ROWS BETWEEN 3 PRECEDING AND 1 FOLLOWING) AS pv5, --当前行+往前3行+往后1行SUM(pv) OVER(PARTITION BY cookieid ORDER BY createtime ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING) AS pv6 ---当前行+往后所有行 FROM po;cookie1 2015-04-16 5 27 27 27 14 14 5cookie1 2015-04-15 4 22 22 27 16 21 9cookie1 2015-04-14 2 18 18 27 17 21 11cookie1 2015-04-13 3 16 16 27 16 18 14cookie1 2015-04-12 7 13 13 27 13 16 21cookie1 2015-04-11 5 6 6 27 6 13 26cookie1 2015-04-10 1 1 1 27 1 6 27# 其他聚集函数方式相同。# LEADhive&gt; select fname,ip,pid,lead(pid) over (partition by ip order by ip) from sales;Abra 192.168.56.101 PI_09 PI_03 -- ip的分组中下一个pid的值为PI_03Elaine 192.168.56.101 PI_03 PI_09Zena 192.168.56.101 PI_09 NULLSage 192.168.56.102 PI_03 NULLCade 192.168.56.103 PI_06 NULLStone 192.168.56.104 PI_08 NULLRegina 192.168.56.105 PI_10 NULLAileen 192.168.56.106 PI_02 PI_05Donova 192.168.56.106 PI_05 NULLMaraam 192.168.56.107 PI_07 NULL# LAGhive&gt; select fname,ip,pid,lag(pid) over (partition by ip order by ip) from sales;Abra 192.168.56.101 PI_09 NULL Elaine 192.168.56.101 PI_03 PI_09 -- ip的分组中上一个pid的值为PI_09Zena 192.168.56.101 PI_09 PI_03Sage 192.168.56.102 PI_03 NULLCade 192.168.56.103 PI_06 NULLStone 192.168.56.104 PI_08 NULLRegina 192.168.56.105 PI_10 NULLAileen 192.168.56.106 PI_02 NULLDonova 192.168.56.106 PI_05 PI_02Maraam 192.168.56.107 PI_07 NULL# FIRST_VALUEhive&gt; select fname,ip,pid,first_value(pid) over (partition by ip order by ip) from sales;Abra 192.168.56.101 PI_09 PI_09 -- ip的分组中第一个pid的值为PI_09Elaine 192.168.56.101 PI_03 PI_09Zena 192.168.56.101 PI_09 PI_09Sage 192.168.56.102 PI_03 PI_03Cade 192.168.56.103 PI_06 PI_06Stone 192.168.56.104 PI_08 PI_08Regina 192.168.56.105 PI_10 PI_10Aileen 192.168.56.106 PI_02 PI_02 -- ip的分组中第一个pid的值为PI_02Donova 192.168.56.106 PI_05 PI_02Maraam 192.168.56.107 PI_07 PI_07# LAST_VALUEhive&gt; select fname,ip,pid,last_value(pid) over (order by ip) from sales;Abra 192.168.56.101 PI_09 PI_09 -- ip的分组中最后一个pid的值为PI_09Elaine 192.168.56.101 PI_03 PI_09Zena 192.168.56.101 PI_09 PI_09Sage 192.168.56.102 PI_03 PI_03Cade 192.168.56.103 PI_06 PI_06Stone 192.168.56.104 PI_08 PI_08Regina 192.168.56.105 PI_10 PI_10Aileen 192.168.56.106 PI_02 PI_05Donova 192.168.56.106 PI_05 PI_05 -- ip的分组中最后一个pid的值为PI_05Maraam 192.168.56.107 PI_07 PI_07 二、数值型函数 abs(x)返回x的绝对值。 hive&gt; select abs(-1);1hive&gt; select abs(1);1hive&gt; select abs(0);0 bin(x)返回x的二进制格式。 hive&gt; select bin(123);1111011hive&gt; select bin(2);10hive&gt; select bin(3);11 ceil(x),ceiling(x)返回x向上取整整数。 hive&gt; select ceil(10.01);11hive&gt; select ceil(0.01);1hive&gt; select ceil(-10.01);-10 conv(x,y,z)进制转换，将x从y进制转换为z进制。 # 将10从十进制转换为二进制hive&gt; select conv(10,10,2);1010 floor(x)返回x向下取整整数。 hive&gt; select floor(10.01);10hive&gt; select floor(11.01);11hive&gt; select floor(11.77);11 greatest(x,y,z…)返回x,y,z….中数值最大的值。 hive&gt; select greatest(1,8,2,3,-10,-12);8 least(x,y,z,…)返回x,y,z…中数值最小的值。 hive&gt; select least(1,8,2,3,-10,-12);-12 negative(x)返回x的负值。 hive&gt; select negative(1);-1hive&gt; select negative(-1);1 round(x)x取整。 hive&gt; select round(10.12);10hive&gt; select round(10.92);11hive&gt; select round(10);10hive&gt; select round(10.5);11 sign(x)当x是正数时返回1，当x是负数时返回-1，当x是0时返回0。 三、类型转换型函数 binary(x)将x以二进制的方式存储。 cast(x as T)将x转换为T类型。 hive&gt; select cast(10 as STRING);10hive&gt; select cast(10 as INT);10hive&gt; select cast('A' as INT);NULL 四、日期函数 add_month(x,y)在日期x的基础上加上y月。 hive&gt; select add_months('2017-10-09',1);2017-11-09hive&gt; select add_months('2017-10-09',4);2018-02-09 current_date()获取当前日期。 hive&gt; select current_date();OK2017-10-20 current_timestamp()获取当前时间。 hive&gt; select current_timestamp();2017-10-20 18:59:52.19 date_add(x,y)在日期x的基础上加上y天。 hive&gt; select date_add('2017-10-09',4);2017-10-13hive&gt; select date_add('2017-12-30',4);2018-01-03 date_format(x,y)将日期x的格式化为y形式的时间。y的格式请点击参考。 date_sub(x,y)在日期x的基础上减去y天。 datediff(x,y)返回日期x,y之间的时间差(天数)。 hive&gt; select datediff('2017-10-09','2017-10-01');8 unix_timestamp()获取当前时区的UNIX时间戳 from_unixtime(x)返回时间戳x的直观日期。 hive&gt; select from_unixtime(unix_timestamp());2017-10-20 19:13:11 last_day(x)返回日期x的月份的最后一天的日期。 hive&gt; select last_day('2017-02-02');2017-02-28 months_between(x,y)返回x,y之间的月份差。 hive&gt; select months_between('2017-10-01','2017-02-02');OK7.96774194 四、字符型函数 concat(x,y,….)连接x,y….字符串合并为一个字符串。 hive&gt; select concat('ABC','abc','zzz');ABCabczzz concat_ws(z,x,y,….)实用分隔符z连接x,y….字符串。 hive&gt; select concat_ws('/','ABC','abc','zzz');ABC/abc/zzz find_in_set(‘x’,’y,z,…’)检查x是否存在与y,z…中，存在则返回位置值，否则返回0。 hive&gt; select find_in_set('abc','a,b,c,ab,abc,bc');5hive&gt; select find_in_set('abc','a,b,c,ab,bc');0 in_file(x,y)检查字符串x是否为文件y的一行。 initcap(x)将字符串x的首字母大写，然后将其他字母小写。 hive&gt; select initcap('hello');Hellohive&gt; select initcap('hellO');Hellohive&gt; select initcap('hELLO');Hellohive&gt; select initcap('HeLLO');Hellohive&gt; select initcap('HELLO');Hello instr(x,y)返回y在x中的第一个位置值。 hive&gt; select instr('abc','c');3hive&gt; select instr('abc','d');0 length(x)返回x的字符个数 hive&gt; select length('abc');3hive&gt; select length('中国');2 lower(x),lcase(x)返回x的小写字符串。 hive&gt; select lower('ABC');abc locate(x,y,z)返回x在y的位置z之后第一次出现的位置。 lpad(x,y,z)将x左侧用字符串z填充，xz组合的总长度为z hive&gt; select lpad('a',2,'b');bahive&gt; select lpad('a',3,'b');bbahive&gt; select lpad('aaaa',3,'b');aaa ltrim(x)去除x左侧的空格。 hive&gt; select ltrim(' AA');AA repeat(x,y)将x重复y次。 hive&gt; select repeat('a',5);aaaaa reverse(x)将x逆向输出。 hive&gt; select reverse('SUV');VUS rpad(x,y,z)将x左侧用字符串z填充，xz组合的总长度为z。 rtrim(x)去除x右侧的空格。 split(x,y)用y分割字符串x,y为正则表达式。 substr(x,y),substring(x,y)返回x从位置y直到结尾的字符串。 substr(x,y,z),substring(x,y,z)返回x从位置y开始的字符串，长度为z的子串。 trim(x)去掉x两端的空格。 五、条件函数 case when # 方式1case when a=b then b1 when a=c then c1 else a end as col# 方式2case a when b then b1 when c then c1 else a end as col coalsce(x,y,z)当x is null时返回y，当x is not null 返回z。 if(x,y,z)当条件x成立时返回y，当条件x不成立时返回z。 isnotnull(x)当x is not null时返回true，当x i null 返回false。 isnull(x)当x is null时返回true，当x is not null 返回false。 nvl(x,y)当x is null时返回y，当x is not null 返回x。 hive&gt; select nvl(1,100);1hive&gt; select nvl(null,100);100 六、UDAF函数 avg()返回平均值 count()返回总数 max()返回最大值 min()返回最小值 sum()返回总和 variance()返回方差]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hive常见的数据导入方式]]></title>
      <url>%2F2017%2F10%2F12%2FHive%E5%B8%B8%E8%A7%81%E7%9A%84%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5%E6%96%B9%E5%BC%8F%2F</url>
      <content type="text"><![CDATA[从本地文件导入到Hive表中； 从HDFS上导入到Hive表中； 从别的表中查询出相应的数据并导入到表中； Hive多表插入； 创建表的时候通过从别的表中查询出相应的记录并插入到表中，即表和数据是在一条SQL语句中完成的。 参考文档：HIVE的四种数据导入方式 一、从本地文件导入到Hive表中# 建表语句hive&gt; create table t1 (id int,name string,age int) row format delimited fields terminated by &apos;,&apos; lines terminated by &apos;\n&apos; STORED AS TEXTFILE;# /tmp/sample.txt文件内容linux&gt; cat /tmp/sample.txt1,Technical manager,102,Proof reader,203,Wing,30# 导入本地文件/tmp/sample.txt至t1表中hive&gt; load data local inpath &apos;/tmp/sample.txt&apos; into table t1;No rows affected (0.325 seconds)hive&gt; select * from t1;+--------+--------------------+---------+| t1.id | t1.name | t1.age |+--------+--------------------+---------+| 1 | Technical manager | 10 || 2 | Proof reader | 20 || 3 | Wing | 30 |+--------+--------------------+---------+3 rows selected (0.119 seconds)# 注意# 此时只是将本地文件的数据插入到hive表中，本地文件sample.txt仍然在本地目录/tmp上，并不是被移动到HDFS上。# 此时可以再进行一次相同的load，会发现数据依然可以被加载到t1去hive&gt; load data local inpath &apos;/tmp/sample.txt&apos; into table t1;hive&gt; select * from t1;+--------+--------------------+---------+| t1.id | t1.name | t1.age |+--------+--------------------+---------+| 1 | Technical manager | 10 || 2 | Proof reader | 20 || 3 | Wing | 30 || 1 | Technical manager | 10 || 2 | Proof reader | 20 || 3 | Wing | 30 |+--------+--------------------+---------+6 rows selected (0.119 seconds) 二、从HDFS上导入到Hive表中# 查看Linux上的本地文件linux&gt; cat /tmp/sample.txt1,Technical manager,102,Proof reader,203,Wing,30# 将Linux上的本地文件上传到HDFS上linux&gt; hdfs dfs -put /tmp/sample.txt /tmp# 建表语句hive&gt; create table t2 (id int,name string,age int) row format delimited fields terminated by &apos;,&apos; lines terminated by &apos;\n&apos; STORED AS TEXTFILE;# 导入HDFS文件至t2表中hive&gt; load data inpath &apos;/tmp/sample.txt&apos; into table t2;hive&gt; select * from t2;+--------+--------------------+---------+| t2.id | t2.name | t2.age |+--------+--------------------+---------+| 1 | Technical manager | 10 || 2 | Proof reader | 20 || 3 | Wing | 30 |+--------+--------------------+---------+3 rows selected (0.117 seconds)# 注意# 此时查看HDFS目录可以发现，原本在HDFS的/tmp目录上的sample.txt文件没有了，反而在e/warehouse/test1.db/t2目录下面多出来了一个sample.txt文件，因为此时的操作是将HDFS上的sample.txt文件移动到了表目录下面。# 此时可以再进行一次相同的load，会发现报错&quot;件不存在&quot;hive&gt; load data inpath &apos;/tmp/sample.txt&apos; into table t2;Error: Error while compiling statement: FAILED: SemanticException Line 1:18 Invalid path &apos;&apos;/tmp/sample.txt&apos;&apos;: No files matching path hdfs://172.16.0.220:9000/tmp/sample.txt (state=42000,code=40000) 三、从别的表中查询出相应的数据并导入到表中# 建表语句hive&gt; create table t3(id int,name string) partitioned by (age int) row format DELIMITED FIELDS TERMINATED BY &apos;,&apos; lines terminated by &apos;\n&apos; STORED AS TEXTFILE;# 指定分区插入数据hive&gt; insert into table t3 partition (age=20) select id,name from t2;hive&gt; select * from t3;OK1 Technical manager 202 Proof reader 203 Wing 20Time taken: 0.137 seconds, Fetched: 3 row(s)# 注意# 在t2表里面，其实3条记录的age并不全是20，为什么在t3里面3条记录的age全是20呢？hive修改了我的数据吗？？# 解答：其实，下载t3的数据文件之后，可以发现，在t3的数据文件中并没有age这一栏，只是这个文件记录的所有age都为20而已。# 动态插入分区hive&gt; set hive.exec.dynamic.partition.mode=nonstrict;hive&gt; insert overwrite table t3 partition (age) select id,name,age from t2;hive&gt; select * from t3;OK1 Technical manager 102 Proof reader 203 Wing 30Time taken: 0.168 seconds, Fetched: 3 row(s)# 注意# 此时可以看到t3和t2长的一模一样了。进入t3的HDFS目录下可以看到3个文件，分别为age=10,age=20,age=30三个文件。这是hive在导入数据的时候已经将数据分区好了。hive是不是很棒棒～ 四、Hive多表插入HIve支持扫描一次源表，通过一条SQL将数据插入到多个表中。 # 创建t4,t5表hive&gt; create table t4 (id int,name string,age int) row format delimited fields terminated by &apos;,&apos; lines terminated by &apos;\n&apos; STORED AS TEXTFILE;OKTime taken: 0.073 secondshive&gt; create table t5 (id int,name string,age int) row format delimited fields terminated by &apos;,&apos; lines terminated by &apos;\n&apos; STORED AS TEXTFILE;OKTime taken: 0.043 seconds# 通过一条SQL，同时向t4,t5表插入数据。hive&gt; from t2 &gt; insert into table t4 select id,name,age &gt; insert into table t5 select id,name,age where age=30;hive&gt; select * from t4;OK1 Technical manager 102 Proof reader 203 Wing 30Time taken: 0.068 seconds, Fetched: 3 row(s)hive&gt; select * from t5;OK3 Wing 30Time taken: 0.128 seconds, Fetched: 1 row(s) 五、创建表的时候通过从别的表中查询出相应的记录并插入到表中即建表和导入数据在一条SQL中完成。 # 建表+导入数据+不带t3分区字段agehive&gt; create table t6 as select id,name from t3;hive&gt; show create table t6;OKCREATE TABLE `t6`( `id` int, `name` string)ROW FORMAT SERDE &apos;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe&apos;STORED AS INPUTFORMAT &apos;org.apache.hadoop.mapred.TextInputFormat&apos;OUTPUTFORMAT &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos;LOCATION &apos;hdfs://172.16.0.220:9000/user/hive/warehouse/test1.db/t6&apos;TBLPROPERTIES ( &apos;transient_lastDdlTime&apos;=&apos;1505904374&apos;)Time taken: 0.027 seconds, Fetched: 13 row(s)hive&gt; select * from t6;OK1 Technical manager2 Proof reader3 WingTime taken: 0.109 seconds, Fetched: 3 row(s)# 建表+导入数据+带t3分区字段agehive&gt; create table t7 as select id,name,age from t3;hive&gt; show create table t7;OKCREATE TABLE `t7`( `id` int, `name` string, `age` int)ROW FORMAT SERDE &apos;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe&apos;STORED AS INPUTFORMAT &apos;org.apache.hadoop.mapred.TextInputFormat&apos;OUTPUTFORMAT &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos;LOCATION &apos;hdfs://172.16.0.220:9000/user/hive/warehouse/test1.db/t7&apos;TBLPROPERTIES ( &apos;transient_lastDdlTime&apos;=&apos;1505904594&apos;)Time taken: 0.026 seconds, Fetched: 14 row(s)hive&gt; select * from t7;OK1 Technical manager 102 Proof reader 203 Wing 30Time taken: 0.065 seconds, Fetched: 3 row(s)# 注意，t3是分区表，但是CTAS操作并没有把分区信息copy出来，容我思索下再来解释==]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hive数据库远程模式部署]]></title>
      <url>%2F2017%2F09%2F14%2FHive%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%9C%E7%A8%8B%E6%A8%A1%E5%BC%8F%E9%83%A8%E7%BD%B2%2F</url>
      <content type="text"><![CDATA[由于Hive运行在HDFS上，所以部署Hive之前需要先部署Hadoop,Hadoop部署的三种方式可参考《Hadoop的三种模式部署-上篇》和《Hadoop的三种模式部署-下篇》，本文将不在重复Hadoop的部署方式，本次Hive的部署基于Hadoop完全分布式部署环境的前提下。 Hive共有三种部署模式，分别为：内置模式，本地模式，远程模式。此处仅介绍生产环境常用的“远程模式”部署。 远程模式需要部署数据库，此处选择MariaDB,MariaDB的部署请参考Debian8上源码安装MySQL5-6-xx。 一、基础环境 主机信息 hadoopmaster 192.168.1.1 hadoopslave1 192.168.1.2 hadoopslave2 192.168.1.3 MySQL和Hive server段将部署在hadoopmaster上。 软件信息 Linux: Debian8.2 MariaDB: 10.1.22 Java: 1.8.0_144 Hadoop: 2.8.1 Hive 2.3.0 目录信息 Hive安装目录： /usr/local/hive HDFS目录信息 由于Hive表创建之前需要在HDFS上存在相应的目录，所以目录规划如下： hdfs dfs -mkdir -p /user/hive/warehouse #Hive的数据目录hdfs dfs -mkdir -p /user/hive/tmp #Hive的临时目录hdfs dfs -mkdir -p /user/hive/log #Hive的日志目录hdfs dfs -chmod g+w /user/hive/warehousehdfs dfs -chmod a+w /usr/hive/tmphdfs dfs -chmod g+w /usr/hive/log 三台机器上分别添加环境变量 vim ~/.bashrc# 添加如下环境变量export HIVE_HOME=/usr/local/hiveexport PATH=$PATH:$HIVE_HOME/bin 二、Hive远程模式部署 在MySQL中创建hive数据库以及hive用户 mysql&gt; create database hive;mysql&gt; grant select,insert,update,delete,create,drop,index on hive.* to 'hive'@'%' identified by 'hive'; 解压hive二进制安装包到/usr/local目录下 tar apache-hive-2.3.0-bin.tar.gz -C /usr/localcd /usr/localmv apache-hive-2.3.0 hivechown -R hadoop:hadoop /usr/local/hive 重命名hive几个配置文件 cd /usr/local/hivecp conf/hive-default.xml.template conf/hive-default.xmlcp conf/hive-env.sh.template conf/hive-env.shcp conf/hive-log4j2.properties.template conf/hive-log4j2.properties 在conf目录下新增hive-site.xml文件 vim conf/hive-site.xml添加如下配置：&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://192.168.1.1:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;description&gt;username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.exec.scratchdir&lt;/name&gt; &lt;value&gt;/user/hive/tmp&lt;/value&gt; &lt;description&gt;HDFS root scratch dir for Hive jobs which gets created with write all (733) permission. For each connecting user, an HDFS scratch dir: $&#123;hive.exec.scratchdir&#125;/&amp;lt;username&amp;gt; is created, with $&#123;hive.scratch.dir.permission&#125;.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;description&gt;location of default database for the warehouse&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.querylog.location&lt;/name&gt; &lt;value&gt;/user/hive/log&lt;/value&gt; &lt;description&gt;Location of Hive run time structured log file&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; 从网上下载MySQL驱动(mysql-connector-java-5.1.44-bin.jar)放置在/usr/local/hive/lib文件夹下。 驱动下载地址：https://dev.mysql.com/downloads/connector/j/ 从 Hive 2.1 版本开始, 我们需要先运行 schematool 命令来执行初始化操作。 schematool -dbType mysql -initSchema# 初始化成功之后，可以从MySQL数据库的hive库里面看到初始化的数据表 Hive启动metastore服务 hive --service metastore &amp;# 此时通过jps命令，可以发现&quot;RunJar&quot;进程# 该进程的关闭，可以通过jps获取&quot;RunJar&quot;进程号，然后kill %jobid即可 访问Hive linux:/usr/local/hive $ hiveSLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]Logging initialized using configuration in file:/usr/local/hive/conf/hive-log4j2.properties Async: trueHive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.hive&gt; show databases;OKdefaulttest1Time taken: 4.267 seconds, Fetched: 2 row(s)hive&gt; use test1;OKTime taken: 0.024 secondshive&gt; show tables;OKtTime taken: 0.026 seconds, Fetched: 1 row(s)hive&gt; show create table t;OKCREATE TABLE `t`( `id` int, `name` string)ROW FORMAT SERDE &apos;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe&apos;STORED AS INPUTFORMAT &apos;org.apache.hadoop.mapred.TextInputFormat&apos;OUTPUTFORMAT &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos;LOCATION &apos;hdfs://172.16.0.220:9000/user/hive/warehouse/test1.db/t&apos;TBLPROPERTIES ( &apos;transient_lastDdlTime&apos;=&apos;1505356773&apos;)Time taken: 0.193 seconds, Fetched: 13 row(s)hive&gt; hiveserver2的启动和登录 hiveserver2的作用是：支持嵌入模式和远程模式，需要用beeline配合使用，此时，我们将演示一下。 # 启动hiveserver2的命令行模式linux&gt; hive --service hiveserver2 --hiveconf hive.server2.thrift.port=9999 &amp;或者linux&gt; /usr/loal/hive/bin/hiveserver2 --hiveconf hive.server2.thrift.port=9999 &amp;# 使用beeline登录hivelinux&gt; /usr/loal/hive/bin/beelineSLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]Beeline version 2.3.0 by Apache Hivebeeline&gt; !connect jdbc:hive2://192.168.1.1:9999Connecting to jdbc:hive2://192.168.1.1:9999Enter username for jdbc:hive2://192.168.1.1:9999: hadoopEnter password for jdbc:hive2://192.168.1.1:9999: ******Connected to: Apache Hive (version 2.3.0)Driver: Hive JDBC (version 2.3.0)Transaction isolation: TRANSACTION_REPEATABLE_READ0: jdbc:hive2://192.168.1.1:9999&gt;show databases;OK+----------------+| database_name |+----------------+| default || test1 |+----------------+2 rows selected (1.233 seconds)0: jdbc:hive2://192.168.1.1:9999&gt; use test1;OKNo rows affected (0.101 seconds)0: jdbc:hive2://192.168.1.1:9999&gt; show tables;OK+-----------+| tab_name |+-----------+| invites || pokes || t || t3 || tt || ttt |+-----------+6 rows selected (0.115 seconds)0: jdbc:hive2://192.168.1.1:9999&gt; ​ 三、FAQ beeline登录的时候可能会遇到“User: hadoop is not allowed to impersonate hadoop (state=08S01,code=0)”这个错误。 原因：指的是访问权限的问题。 解决方法： # 在hadoop的core-site.xml文件中添加如下配置项linux&gt; vim /usr/lcoal/hadoop/etc/hadoop/core-site.xml &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.groups&lt;/name&gt; &lt;value&gt;hadoop&lt;/value&gt; &lt;/property&gt;# 然后重启HDFS之后，即可解决sbin/stop-dfs.shsbin/start-dfs.sh 在hdfs重启之后很短的时间内登录hive可能会遇到“Error: Could not open client transport with JDBC Uri: jdbc:hive2://192.168.1.1:9999: Failed to open new session: java.lang.RuntimeException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /user/hive/tmp/hadoop/acde78a1-baea-4eb3-834b-fb4a177313cb. Name node is in safe mode.” 原因：Name node is in safe mode，说明Hadoop的Namenode在安全模式下。在分布式文件系统启动的时候，开始的时候会有安全模式，当分布式文件系统处于安全模式的情况下，文件系统中的内容不允许修改也不允许删除，直到安全模式结束。安全模式主要是为了系统启动的时候检查各个DataNode上数据块的有效性，同时根据策略必要的复制或者删除部分数据块。运行期通过命令也可以进入安全模式。在实践过程中，系统启动的时候去修改和删除文件也会有安全模式不允许修改的出错提示，只需要等待一会儿即可。 解决方式： # 方法一： 耐心等待一会即可。# 方法二：不想耐心等待，那么久老老实实敲入命令在Hadoop的安装目录下执行`bin/hadoop dfsadmin -safemode leave`即可]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hadoop的三种模式部署-下篇]]></title>
      <url>%2F2017%2F09%2F13%2FHadoop%E7%9A%84%E4%B8%89%E7%A7%8D%E6%A8%A1%E5%BC%8F%E9%83%A8%E7%BD%B2-%E4%B8%8B%E7%AF%87%2F</url>
      <content type="text"><![CDATA[该篇将演示Hadoop完全分布式的部署，并且以hadoop用户启动。 Linux: Debian8.2 Java: 1.8.0_144 Hadoop: Hadoop 2.8.1 Hadoop安装包下载地址：http://hadoop.apache.org/releases.html Java部署请参考《常用软件部署—Debian版》 该篇将演示Hadoop的完全分布式部署，并且以hadoop用户启动。 参考文档链接：https://chu888chu888.gitbooks.io/hadoopstudy/content/Content/4/chapter0404.html 一、基础环境准备 主机信息 hadoopmaster 192.168.1.1 hadoopslave1 192.168.1.2 hadoopslave2 192.168.1.3 软件信息 Debian8.2 JAVA1.8.0_144 Hadoop 2.8.1 目录信息 Java安装目录: /usr/local/jdk Hadoop安装目录: /usr/local/hadoop Hadoop数据目录: /data/hadoop 安装依赖包，三台主机分别执行如下命令： apt-get install ssh pdsh 三台机器配置hadoop:hadoop组合用户，三台主机分别执行如下命令： addgroup hadoopadduser -ingroup hadoop hadoop 三台主机分别配置/etc/hosts文件 vim /etc/hosts添加如下主机名192.168.1.1 hadoopmaster192.168.1.2 hadoopslave1192.168.1.3 hadoopslave2 三台主机使用hadoop用户登录，并添加如下环境变量： vim /home/hadoop/.bashrc添加如下变量：export JAVA_HOME=/usr/local/jdkexport JRE_HOME=/usr/local/jdk/jreexport CLASSPATH=$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/libexport HADOOP_HOME=/usr/local/hadoopexport PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin:$HADOOP_HOME/binexport PDSH_RCMD_TYPE=sshif [[ $- == *i* ]]then PS1=&quot;\[$(tput bold)\]\[$(tput setaf 3)\]\u\[$(tput setaf 7)\]@\[$(tput setaf 5)\]\h:\[$(tput setaf 2)\]\w\[$(tput setaf 4)\] \\$\[$(tput sgr0)\] &quot;fi# 立即生效环境变量source /home/hadoop/.bashrc 三台主机相互之前SSH免密码登录（主机和主机本身也需要SSH免密码登录），详细信息请参考《SSH免密码登录操作步骤》 ​ 二、Hadoop完全分布式配置以下所有操作没有特殊说明的前提下，使用的是hadoop用户进行操作。 hadoopmaster机器上解压文件包(以root用户操作) tar -zxvf hadoop-2.8.1.tar.gz -C /usr/localmv hadoop-2.8.1 hadoopchown -R hadoop:hadoop /usr/local/hadoop hadoopmaster机器上新增/data/hadoop/tmp数据目录(以root用户操作) mkdir /data/hadoop/tmpchown -R hadoop:hadoop /data/hadoop/tmp hadoopmaster机器上配置hadoop-env.sh文件 cd /usr/local/hadoopvim etc/hadoop/hadoop-env.sh更新如下变量：export JAVA_HOME=/usr/local/jdk hadoopmaster机器上配置集群环境，正常启动最小配置文件： slaves、core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml 配置etc/hadoop/slaves文件 vim etc/hadoop/slaves添加如下文件：192.168.1.2192.168.1.3 配置core-site.xml文件 &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://192.168.1.1:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/data/hadoop/tmp&lt;/value&gt; &lt;description&gt;Abase for other temporary directories.&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; 配置hfs-site.xml文件 &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;192.168.1.1:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/data/hadoop/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/data/hadoop/tmp/dfs/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置mapred-site.xml文件 &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;172.168.1.1:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;192.168.1.1:19888&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置yarn-site.xml文件 &lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;192.168.1.1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hadoopslave1和hadoopslave2可以将hadoopmaster的/usr/local/hadoop完全copy一份至本机的/usr/local/hadoop目录下。其中/usr/local/hadoop/etc/hadoop/slaves文件在两台slave机器上是无用的，但是该文件存在并无其他影响。 ​ 三、Hadoop完全分布式启动 hadoopmaster上执行初始化，执行一次即可 hdfs namenode -format# 初始化的作用是：一个全新的HDFS安装需要被初始化，初始化的过程会在存储目录下创建一个空的文件系统，以及创建namenode持久化数据结构的初始版本。 hadoopmaster上执行 sbin/start-dfs.shsbin/start-yarn.shsbin/mr-jobhistory-daemon.sh start historyserver 四、Hadoop完全分布式部署完成验证 三台主机上分别执行jps查看java进程 hadoopmaster:/usr/local/hadoop $ jps19360 JobHistoryServer19012 ResourceManager18806 SecondaryNameNode18620 NameNode20190 Jpshadoopslave1:/usr/local/hadoop $ jps17393 NodeManager17282 DataNode17907 Jpshadoopslave2:/usr/local/hadoop $ jps21720 NodeManager21609 DataNode21837 Jps hadoopmaster执行hdfs dfsadmin -report查看各个数据节点是否正常 hadoopmaster:/usr/local/hadoop $ hdfs dfsadmin -reportConfigured Capacity: 42398629888 (39.49 GB)Present Capacity: 11841167360 (11.03 GB)DFS Remaining: 11841110016 (11.03 GB)DFS Used: 57344 (56 KB)DFS Used%: 0.00%Under replicated blocks: 0Blocks with corrupt replicas: 0Missing blocks: 0Missing blocks (with replication factor 1): 0Pending deletion blocks: 0-------------------------------------------------Live datanodes (2):Name: 192.168.1.2:50010 (hadoopslave1)Hostname: hadoopslave1Decommission Status : NormalConfigured Capacity: 21199314944 (19.74 GB)DFS Used: 28672 (28 KB)Non DFS Used: 14452621312 (13.46 GB)DFS Remaining: 5654732800 (5.27 GB)DFS Used%: 0.00%DFS Remaining%: 26.67%Configured Cache Capacity: 0 (0 B)Cache Used: 0 (0 B)Cache Remaining: 0 (0 B)Cache Used%: 100.00%Cache Remaining%: 0.00%Xceivers: 1Last contact: Wed Sep 13 12:00:35 CST 2017 Name: 192.168.1.2:50010 (hadoopslave2)Hostname: hadoopslave2Decommission Status : NormalConfigured Capacity: 21199314944 (19.74 GB)DFS Used: 28672 (28 KB)Non DFS Used: 13920976896 (12.96 GB)DFS Remaining: 6186377216 (5.76 GB)DFS Used%: 0.00%DFS Remaining%: 29.18%Configured Cache Capacity: 0 (0 B)Cache Used: 0 (0 B)Cache Remaining: 0 (0 B)Cache Used%: 100.00%Cache Remaining%: 0.00%Xceivers: 1Last contact: Wed Sep 13 12:00:35 CST 2017 - 打开浏览器，验证是否能打开如下四个网址： MapReduce JobHistory Server http://192.168.1.1:19888 ResourceManager http://192.168.1.1:8088/ NameNode http://192.168.1.1:50070 DataNode http://192.168.1.2:50075 http://192.168.1.3:50075#### 五、Hadoop集群关闭 ```shell stop-yarn.sh stop-dfs.sh mr-jobhistory-daemon.sh stop historyserver]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[常用软件部署-Debian版]]></title>
      <url>%2F2017%2F09%2F13%2F%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%E9%83%A8%E7%BD%B2-Debian%E7%89%88%2F</url>
      <content type="text"><![CDATA[工作中常用软件部署集合，有如下的软件部署：Java、Tomcat、ZooKeeper….未来会越来越多。 一、jdk安装cd ~# 解压jdk压缩包tar -zxvf ~/jdk-7u45-linux-x64.tar.gz -C /usr/local/cd /usr/localmv jdk1.7.0_45/ jdk# 配置jdk环境变量vim ~/.bashrc# 添加环境变量export JAVA_HOME=/usr/local/jdkexport PATH=$PATH:$JAVA_HOME/bin# 使环境变量立即生效source ~/.bashrc# 验证jdk安装成功方式java -version 二、tomcat安装cd ~# 解压tomcat压缩包tar -zxvf apache-tomcat-7.0.68.tar.gz -C /usr/local/cd /usr/localmv apache-tomcat-7.0.68/ tomcat# 启动tomcat./tomcat/bin/start.sh# 启动成功输出信息Using CATALINA_BASE: /usr/local/tomcatUsing CATALINA_HOME: /usr/local/tomcatUsing CATALINA_TMPDIR: /usr/local/tomcat/tempUsing JRE_HOME: /usr/local/jdkUsing CLASSPATH: /usr/local/tomcat/bin/bootstrap.jar:/usr/local/tomcat/bin/tomcat-juli.jarTomcat started.# 验证tomcat启动成功方式netstat -npl | grep &apos;java&apos;# 输出信息tcp 0 0 0.0.0.0:8080 0.0.0.0:* LISTEN 13916/java tcp 0 0 127.0.0.1:8005 0.0.0.0:* LISTEN 13916/java tcp 0 0 0.0.0.0:8009 0.0.0.0:* LISTEN 13916/java 三、zookeeper安装tar -zxvf zookeeper-3.4.10.tar.gz -C /usr/local/cd /usr/local/mv zookeeper-3.4.10 zookeepercd zookeepercp conf/zoo_sample.cfg conf/zoo.cfgvim conf/zoo.cfg# 修改dataDir=/data/zk# 增加节点：server.1=ip:2888:3888# 增加节点：server.2=ip:2888:3888# 创建zk的数据目录mkdir /data/zkcd /data/zkecho $id &gt; myid# 如server1，此处需要做的神操作是：echi &apos;1&apos; &gt; myid# 启动zookeeperbin/zkServer.sh start# 启动成功输出信息# ZooKeeper JMX enabled by default# Using config: /usr/local/zookeeper/bin/../conf/zoo.cfg# Starting zookeeper ... STARTED# 查看zookeeper当前的状态bin/zkServer.sh status# 输出信息# ZooKeeper JMX enabled by default# Using config: /usr/local/zookeeper/bin/../conf/zoo.cfg# Mode: follower]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hadoop的三种模式部署-上篇]]></title>
      <url>%2F2017%2F09%2F13%2FHadoop%E7%9A%84%E4%B8%89%E7%A7%8D%E6%A8%A1%E5%BC%8F%E9%83%A8%E7%BD%B2-%E4%B8%8A%E7%AF%87%2F</url>
      <content type="text"><![CDATA[该篇将分别讲解Hadoop的本地模式和伪分布模式的部署，并且以root用户启动。 Linux: Debian8.2 Java: 1.8.0_144 Hadoop: Hadoop 3.0.0-alpha4 Hadoop安装包下载地址：http://hadoop.apache.org/releases.html Java部署请参考《常用软件部署—Debian版》 一、基本配置（三种部署模式都需要的基本步骤） 安装依赖包 apt-get install ssh pdsh 解压Hadoop安装包至指定的目录下 tar -zxvf hadoop-3.0.0-alpha4.tar.gz -C /usr/local/cd /usr/localmv hadoop-3.0.0-alpha4/ hadoopcd hadoop/ 配置环境变量 编辑etc/hadoop/hadoop-env.sh文件vim etc/hadoop/hadoop-env.sh配置以下选项# set to the root of your Java installation export JAVA_HOME=/usr/local/jdk 尝试hadoop命令，验证环境变量是否正确 bin/hadoop#此时如果显示一系列的hadoop命令说明，则环境变量正确。否则，请检查之前的步骤是否正确。 二、本地模式(Local/Standalone Mode) Hadoop默认配置下，是个local mode，即可以立即使用local operation。 操作示例 mkdir inputcp etc/hadoop/*.xml inputbin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0-alpha4.jar grep input output &apos;dfs[a-z.]+&apos;cat output/* 三、伪分布模式(Pesudo-Distributed Mode) 配置 配置/etc/hosts文件 添加：192.18.16.220 hadoop-master 配置pdsh的默认使用ssh vim ~/.bashrc添加：export PDSH_RCMD_TYPE=sshsource ~/.bashrc# 使其在当前会话中立即生效 配置本机ssh root@localhost免密登录 具体配置方式见《SSH免密码登录操作步骤 》 配置etc/hadoop/hadoop-env.sh文件 添加如下变量：(目前我使用的是root用户，所以配置root,如果使用别的用户，请将root用户替换成你使用的用户)export HDFS_NAMENODE_USER=rootexport HDFS_DATANODE_USER=rootexport HDFS_SECONDARYNAMENODE_USER=root 配置etc/hadoop/core-site.xml文件 &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://192.18.16.220:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置etc/hadoop/hdfs-site.xml文件 &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 操作示例 初始化文件系统 bin/hdfs namenode -format 启动NameNode和DataNode节点 sbin/start-dfs.sh 此时可以登录NameNode的web端：http://192.18.16.220:9870 创建hdfs的目录 bin/hdfs dfs -mkdir /userbin/hdfs dfs -mkdir /user/&lt;username&gt; 复制input文件的内容到分布式文件系统上 bin/hdfs dfs -mkdir inputbin/hdfs dfs -put etc/hadoop/*.xml input 执行一个示例操作 bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0-alpha4.jar grep input output &apos;dfs[a-z.]+&apos; 查看输出文件 bin/hdfs dfs -get output outputcat output/*或者bin/hdfs dfs -cat output/*]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java中a++和++a的区别]]></title>
      <url>%2F2017%2F08%2F01%2FJava%E4%B8%ADa-%E5%92%8C-a%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
      <content type="text"><![CDATA[第一次搞搞明白a++和++a的区别。 首先、看个代码package wing;public class justAtest &#123; public static void main(String[] args)&#123; int a=3,b; b = a++; int c=3,d; d = ++c; System.out.println("b="+b+",a="+a); System.out.println("c="+c+",d="+d); &#125;&#125; 然后、瞅瞅结果b=3,a=4c=4,d=4 再然后、、、这个结果咋和我们想象的不一样呢？为什么b和d的结果不一样呢？为什么a和c的结果也不一样呢？为什么为什么为什么呢？？？ 最后、给个解释 为什么a=4,b=3呢？因为a++的时候，a将自己初始化的值3放入临时内存空间中，然后自身加1，再将临时内存空间内存的值赋值给b，所以此时a=4,b=3 为什么c=4,d=4呢？因为++c的时候，c会自加1，然后再将加1后的c赋值给d，所以c=4,d=4。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[不听话的timestamp类型]]></title>
      <url>%2F2017%2F07%2F11%2F%E4%B8%8D%E5%90%AC%E8%AF%9D%E7%9A%84timestamp%E7%B1%BB%E5%9E%8B%2F</url>
      <content type="text"><![CDATA[今天开发提交了一个这样的表结构给我…. 一、前因后果今天开发提交了一个这样的表结构(做了无用信息的清理)给我： create table t(c1 timestamp not null, c2 timestamp not null) 但是发现，操作完毕之后，数据库中的表结构变成了这样： create table t(c1 timestamp not null DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, c2 timestamp not null DEFAULT '0000-00-00 00:00:00') 于是我百思不得其解，这是为什么为什么为什么呢？。。 二、动手实验创建如下表结构： create table t1(c1 timestamp not null, c2 timestamp not null, c3 timestamp not null, c4 timestamp null); create table t2(c1 datetime not null, c2 datetime not null, c3 datetime not null, c4 datetime null);create table t3(c1 timestamp not null DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, c2 timestamp not null, c3 timestamp not null, c4 timestamp null);create table t4(c1 timestamp not null, c2 timestamp not null, c3 timestamp not null DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, c4 timestamp null);create table t5(c1 timestamp not null DEFAULT CURRENT_TIMESTAMP, c2 timestamp not null, c3 timestamp not null, c4 timestamp null);create table t6(c1 timestamp not null, c2 timestamp not null, c3 timestamp not null DEFAULT CURRENT_TIMESTAMP, c4 timestamp null); 操作完毕后，数据库实际存在的表结构： -- 参数explicit_defaults_for_timestamp=offCREATE TABLE `t1` ( `c1` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, `c2` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00', `c3` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00', `c4` timestamp NULL DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8CREATE TABLE `t2` ( `c1` datetime NOT NULL, `c2` datetime NOT NULL, `c3` datetime NOT NULL, `c4` datetime DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8CREATE TABLE `t3` ( `c1` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, `c2` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00', `c3` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00', `c4` timestamp NULL DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8CREATE TABLE `t4` ( `c1` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, `c2` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00', `c3` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, `c4` timestamp NULL DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8CREATE TABLE `t5` ( `c1` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP, `c2` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00', `c3` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00', `c4` timestamp NULL DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8CREATE TABLE `t6` ( `c1` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, `c2` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00', `c3` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP, `c4` timestamp NULL DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8 三、得出结论 timestamp类型的not null 必须和default值共存，datetime类型的not null不需要default值共存；(t1 VS t2) 如果timestamp类型的not null没有和default值共存的情况下，MySQL/MariaDB处理方式如下：第一个timestamp not null类型自动变更为：timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP，第一个timestamp not null类型之后的所有该类型，自动变更为：timestamp NOT NULL DEFAULT ‘0000-00-00 00:00:00’；(t1) 如果timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP类型位于第一个timestamp not null类型字段，那么其他的timestamp not null类型字段自动处理为timestamp NOT NULL DEFAULT ‘0000-00-00 00:00:00’，否则第一个timestamp not null类型字段依旧会自动处理为timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP类型；（t3 VS t4） 如果timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP 类型位于第一个timestamp not null类型字段，那么其他的timestamp not null类型字段自动处理为timestamp NOT NULL DEFAULT ‘0000-00-00 00:00:00’，否则第一个timestamp not null类型字段依旧会自动处理为timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP类型；(t5 VS t6)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java之抽象类]]></title>
      <url>%2F2017%2F05%2F20%2FJava%E4%B9%8B%E6%8A%BD%E8%B1%A1%E7%B1%BB%2F</url>
      <content type="text"><![CDATA[抽象类居然和我理解的类不大一样，它居然是一种规范哎。。。 一、特点 方法只有申明没有实现时，必须定义为抽象方法，被abstract修饰；抽象方法必须定义在抽象类中，该类必须备abstract修饰。 抽象类不可以被实例化（即new）。why?因为调用抽象方法没意义。 抽象类被继承后，其子类必须覆盖所有的抽象方法，该子类才可以实例化，否则，这个字类还是抽象类。 二、疑问 抽象类有构造函数吗？有，用于给子类对象进行初始化。 抽象类可以定义非抽象方法吗？可以。但是毫无意义，因为抽象类不可以被实例化。 抽象关键字不可以和哪些关键字共存？private 不可以，抽象类的所有抽象方法是一种规范。static 不可以。static不需要对象，但是抽象类需要对象。final不可以。final不可以覆盖，而抽象类需要被覆盖。 抽象类和一般类的异同点？相同点： 抽象类和一般类都是描述事物的，都在内部定义了成员。 不同点： 一般类有足够的信息描述事物，抽象类描述事物的信息可能不足； 一般类中不能定义抽象方法，而抽象类中是可以的。 一般类可以被实例化，抽象类不可以被实例化。 抽象类一定是父类吗？是的。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java之数组]]></title>
      <url>%2F2017%2F05%2F20%2FJava%E4%B9%8B%E6%95%B0%E7%BB%84%2F</url>
      <content type="text"><![CDATA[数组这个工具，存在于无数种语言中，让我们一起了解下java中的数组又是怎么玩的呢？ 数组的内存空间 数组其实是对象的一种，所以它存在堆内存空间中。 数组的三种定义格式// 数组定义格式1int[] arr1= new int[3];//数组定义格式2int[] arr2= new int[]&#123;9,8,7,6&#125;;//数组定义格式3int[] arr3=&#123;0,1,2,3&#125;; 数组的操作方法 数组遍历 int[] arr3=&#123;0,1,2,3&#125;;System.out.println(arr3[2]);System.out.println(arr3.length);for(int x=0;x&lt;arr3.length;x++)&#123; System.out.println(arr3[x]);&#125;// 结果0123 获取最值（最大值、最小值） // 最大值public static void main(String[] args)&#123; int[] arr1=&#123;-34,-24,-98,-10,-100,-55&#125;; int maxvalues = GetMaxArr(arr1); System.out.println(maxvalues);&#125;public static int GetMaxArr(int[] arr)&#123; int max=arr[0]; for(int x=1;x&lt;arr.length;x++)&#123; if(arr[x]&gt;max)&#123; max=arr[x]; &#125; &#125; return max;&#125;// 结果100 选择排序 public static void SelectSort(int[] arr)&#123; for(int x=0;x&lt;arr.length-1;x++)&#123; for(int y=x+1;y&lt;arr.length;y++)&#123; if(arr[x]&gt;arr[y])&#123; int temp=arr[x]; arr[x]=arr[y]; arr[y]=temp; &#125; &#125; &#125;&#125; 冒泡排序 public static void BubbleSort(int[] arr)&#123; for(int x=0;x&lt;arr.length-1;x++)&#123; for(int y=0;y&lt;arr.length-1-x;y++)&#123; if(arr[y]&gt;arr[y+1])&#123; int temp=arr[y]; arr[y]=arr[y+1]; arr[y+1]=temp; &#125; &#125; &#125;&#125; Java内置数组排序功能 import java.util.*;Arrays.sort(arr1) 折半查找 /** 二分查找法（折半查找法）*/public static int halfSearch(int[] arr,int value)&#123; int minindex,midindex,maxindex; minindex=0; maxindex=arr.length-1; midindex=(maxindex+minindex)/2; while(arr[midindex]!=value)&#123; if(arr[midindex]&gt;value)&#123; maxindex=midindex-1; &#125;else&#123; minindex=midindex+1; &#125; if(maxindex&lt;minindex)&#123; return -1; &#125; midindex=(maxindex+minindex)/2; &#125; return midindex;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java之static关键字]]></title>
      <url>%2F2017%2F05%2F20%2FJava%E4%B9%8Bstatic%E5%85%B3%E9%94%AE%E5%AD%97%2F</url>
      <content type="text"><![CDATA[魔性的static参数，你想了解它吗？ 一、特点 static是一种修饰符，用于修饰成员（成员变量+成员函数）； 数据共享：static修饰后的成员可以被每个对象所共享； static修饰的成员优于对象之前存在，因为static的成员随着类的加载就已经存在了； static修饰后的成员可以被类名直接调用，而不仅仅被对象调用； static修饰的是共享数据，对象中存储的是特有数据。 二、成员变量 PK 静态变量 两个变量的生命周期不同。成员变量随着对象的创建而创建，随着对象的回收而释放；静态变量随着类的加载而加载，随着类的消失而消失（类一般在虚拟机结束了，则消失了）。 调用方式不同。成员变量只能被对象调用；静态变量可以被对象调用，也可以被类调用（推荐这种方式使用静态变量）。 别名不同。成员变量称为实例变量；静态变量称为类变量。 数据存储位置不同。成员变量数据存储在堆内存的对象中，所以也叫对象的特有数据；静态变量数据存储在共享区（方法区）中，所以也叫对象的共享数据。 三、注意事项 静态方法只能访问静态变量。 // 错误代码public class staticDemo &#123; public static void main(String[] args)&#123; Person.show(); &#125;&#125;class Person&#123; String name; static String country = "CN"; public static void show()&#123; System.out.println(country+":"+name); // 此处编译会报错，因为show是静态方法，但是name是非静态变量 &#125;// 正确代码public class staticDemo &#123; public static void main(String[] args)&#123; Person.show(); &#125;&#125;class Person&#123; String name; static String country = "CN"; public static void show()&#123; System.out.println(country); &#125; 静态方法中不可以使用this/super关键字。（this是因为没有对象==） 主函数是静态的。 主函数特殊之处： 格式是固定的public static void main(String[] args){}public： 因为权限必须是最大的；static： 不需要创建对象，直接用主函数所属类名调用即可，如java staticDemo.java时，直接用java staticDemo.main调用方法即可； void：主函数没有具体的返回值；main: 函数名，不是关键字，只是一个被jvm识别的固定名字；String[] args：主函数的参数列表，是一个数组类型的参数。而且元素都是字符串类型。 被jvm识别和调用 四、静态代码块 // 代码public class mainTest &#123; public static void main(String[] args)&#123; new Demo().show(); new Demo().show(); new Demo().show(); &#125;&#125;class Demo&#123; void show()&#123; System.out.println("show run"); &#125; // 静态代码块 static&#123; System.out.println("hahahhahah"); &#125;&#125;// 结果hahahhahahshow runshow runshow run 特点：随着类的加载而执行，并且只执行一次。 作用： 用于给类进行初始化。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java中接口和抽象类的区别]]></title>
      <url>%2F2017%2F05%2F20%2FJava%E4%B8%AD%E6%8E%A5%E5%8F%A3%E5%92%8C%E6%8A%BD%E8%B1%A1%E7%B1%BB%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
      <content type="text"><![CDATA[接口和抽象类傻傻分不清的我== 相同点都是不断向上抽取而来的。 不同点 抽象类需要被继承，而且只能单继承；接口可以被实现，而且可以多实现。 抽象类中可以定义抽象方法和非抽象方法，子类继承后，可以直接使用非抽象方法；接口中只能定义抽象方法，必须由子类去实现。 抽象类的继承，是is a 关系，在定义该体系的基本共性内容；接口的实现，是like a关系，在定义该体系的额外功能。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java之构造函数]]></title>
      <url>%2F2017%2F05%2F20%2FJava%E4%B9%8B%E6%9E%84%E9%80%A0%E5%87%BD%E6%95%B0%2F</url>
      <content type="text"><![CDATA[Java的构造函数方便又很不同于其他语言（其实我也只学过python，==），这个构造函数需要好好的总结下，毕竟我觉得这是个奇怪的家伙，虽然它真的很方便== 一、特点 函数名与类名相同； 不用定义返回值类型； 没有具体的返回值； 构造函数具有重载的特点。 二、作用给对象进行初始化。 三、注意事项一个类中如果没有定义构造函数，则会默认一个空参数的构造函数；如果类中定义了构造函数，那么类中默认构造函数就没有了。 四、构造函数与一般函数的区别 构造函数：对象创建时，就会调用与之对应的构造函数，对对象进行初始化；一般函数：对象创建后，需要函数功能的时候才会调用。 构造函数：对象创建时，只会调用一次；一般函数：对象创建后，可以被调用多次。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java中成员变量和局部变量的区别]]></title>
      <url>%2F2017%2F05%2F20%2FJava%E4%B8%AD%E6%88%90%E5%91%98%E5%8F%98%E9%87%8F%E5%92%8C%E5%B1%80%E9%83%A8%E5%8F%98%E9%87%8F%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
      <content type="text"><![CDATA[Java中的成员变量是什么？局部变量又是什么？它们之间有什么不同呢？ 成员变量定义在类中，整个类都可以访问;局部变量定义在函数、语句、局部代码块中，只在所属的区域有效。 public class Car &#123; int num;//成员变量 String color; public void run()&#123; int numlocal = 5;// 局部变量 System.out.println(num+","+color); &#125;&#125; 成员变量存在与堆内存的对象中;局部变量存在与栈内存的方法中。因为成员变量是对象的属性，对象是存储在堆内存中的，而局部变量是存储在栈中的。 成员变量随着对象的创建而存在，随着对象的消失而消失;局部变量随着所属区域的执行而存在，随着所属区域的结束而释放。 public class Car &#123; int num=4;//轮胎数 String color;//颜色 public void run()&#123; int num=8; System.out.println(num);//此时的num为8 &#125; System.out.println(num);// 此时的num为4&#125; 成员变量具有默认初始化值;局部变量没有默认初始化值。 当成员变量和局部变量重名时，可以使用this关键字区分。（this代表对象）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java中基本数据类型参数传递和引用数据类型参数传递]]></title>
      <url>%2F2017%2F05%2F15%2FJava%E4%B8%AD%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E5%8F%82%E6%95%B0%E4%BC%A0%E9%80%92%E5%92%8C%E5%BC%95%E7%94%A8%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E5%8F%82%E6%95%B0%E4%BC%A0%E9%80%92%2F</url>
      <content type="text"><![CDATA[Java中的基本数据类型参数传递和引用数据类型传递大有不同，下面让我们一起见证下吧～ 基本数据类型参数传递// 基本数据类型参数传递demopublic class Demo1 &#123; public static void main(String[] args)&#123; int x=3; show(x); System.out.println("x="+x); &#125; public static void show(int x)&#123; x=4; &#125;&#125;// 结果x=3 解析： main函数进入栈内存； x为局部变量，所以在栈内存中main函数初始化x=3; show函数进入栈内存； x为局部变量，所以在栈内存中show函数初始化x=4; show函数执行完毕，show函数将出栈，此时show函数中局部变量x=4也会跟着出栈； 此时栈内存中留下main函数，以及man函数初始化的局部变量x=3，所以此时打印出来的x为3。 引用数据类型参数传递// 引用数据类型参数传递demopublic class Demo2 &#123; int x=3; public static void main(String[] args)&#123; Demo2 d = new Demo2(); d.x = 9; show(d); System.out.println("x="+d.x); &#125; public static void show(Demo2 d)&#123; d.x = 4; &#125;&#125;// 结果x=4 解析： 成员变量(属性)x=3，进入堆内存； main函数进入栈内存，存在一个局部变量d; d指向堆内存中的Demo2对象; 堆内存中Demo2存在属性d.x=9,此时将堆内存中原有的x成员变量替换为9； show函数进入栈内存； main函数将d局部变量指向show函数的d局部变量； 此时show函数的d局部变量和main函数的d局部变量一样，指向堆内存的Demo2对象； show函数的d.x=4，将堆内存的Demo2属性x=9替换为x=4; 此时show函数执行完毕，show函数以及d局部变量出栈； 此时堆内存中的x=4，所以打印出此时的x值为4。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java中Object.equals与String.equals的区别]]></title>
      <url>%2F2017%2F04%2F25%2FJava%E4%B8%ADObject-equals%E4%B8%8EString-equals%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
      <content type="text"><![CDATA[Java中的堆和常量池的区别是什么呢？Object.equals与String.equals的区别呢？一个小示例让你明白它～ 基础知识Java的存储空间：寄存器、栈、堆、静态存储区、常量存储区(常量池)、其他存储位置。 此处重点介绍堆和常量存储区：堆：存储new的对象;常量池：用来存储final static、String的常量。 Object.equals与String.equals的区别Object.equals(==)：比较内存地址；String.equals: 比较内容即可，不管内存地址。 总结：Object.equals相等，String.equals一定相等；String.equals相等，Object.equals不一定相等。 实战演练 public class TestString &#123; public static void main(String[] args)&#123; // 维护在常量池里面 String a="hello"; String b="hello"; // new出来的所有对象都在堆内存中 // 只要是new出现来的都是新对象 String c=new String("hello"); String d=new String("hello"); // 对比内存地址 //true System.out.println(a==b); //false System.out.println(a==c); //false System.out.println(c==d); //对比内容 //true System.out.println(a.equals(b)); //true System.out.println(a.equals(c)); //true System.out.println(c.equals(d)); &#125;&#125; 解释：a,b都是常量，a和b都是指向常量存储区中的常量’hello’，所以无论内容还是内存地址都是一样的，因此a==b以及a.equals(b)都是true;c,d都是变量，他们都是new出来的对象，里面存在两个hello变量，c和d分别指向自己的hello变量，所以c和d内容一样，但是内存地址不一样，因此c==d是true，但是c.equals(d)为true。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[MariaDB性能调优工具mytop]]></title>
      <url>%2F2017%2F03%2F20%2FMariaDB%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E5%B7%A5%E5%85%B7mytop%2F</url>
      <content type="text"><![CDATA[mytop为MariaDB自带工具，但MySQL并没有携带该工具，需要自行安装。安装方式请自行google== 一、使用方式./mytop --prompt -u xxx -h xxx -P xxx -d xxx 二、结果解释 常用结果展示 MariaDB on localhost (10.1.18-MariaDB) up 77+08:53:14 [20:08:00]Queries: 17.6G qps: 2833 Slow: 106.7k Se/In/Up/De(%): 48/07/11/00Sorts: 380 qps now: 3196 Slow qps: 0.2 Threads: 199 ( 3/ 102) 52/05/12/00Handler: (R/W/U/D) 9372/ 5371/ 709/ 0 Tmp: R/W/U: 297/ 278/ 0ISAM Key Efficiency: 99.7% Bps in/out: 586.6k/ 2.1M Now in/out: 521.9k/ 1.8MReplication IO:No SQL:No Id User Host/IP DB Time Cmd Query or State -- ---- ------- -- ---- --- ---------- 2 root localhost mysql 0 Query show full processlist 16 root localhost 0 Sleep 17 root localhost testdb 0 Query SELECT * FROM dept_emp 第一行：MariaDB版本；数据库运行时间； 第二行：Queries为数据库启动之后处理的总queries；qps:数据库启动之后平均的qps；Slow：数据库启动之后的慢查询总数；Se/In/Up/De(%): SELECT/INSERT/UPDATE/DELETE所占的比例； 第三行：Sorts：没看明白；qps now:自mytop上次刷新后的平均qps；Slow qps: 自mytop上次刷新后的平均慢查询qps；Threads: 连接数据库线程总数(活跃的线程数/SLEEP状态的线程数)；52/05/12/00：自mytop上次刷新后SELECT/INSERT/UPDATE/DELTE的比例； 第四行：个人目前没有看明白，也没有找到相关的描述； 第五行: ISAM Key Efficiency:myisam的key buffer的命中率，Bps：数据库启动之后平均的网络流量，Now：自mytop上次刷新后的平均网络流量。 ​ 三、常用快捷键? 显示帮助信息。 c 命令的总结视图(基于Com_*的统计)。 C 关闭／开启颜色模式。 d 仅仅显示指定的数据库。 e 将指定的thread_id对应的query在数据库上的explain结果展示出来。 E 展示当前复制的error信息。 f 显示指定query的完整信息。 h 显示指定的host的连接信息。 H 只显示mytop的头信息。 I show innodb status的信息。（大写i） k kill指定的thread id。 p 显示暂停。 l 高亮慢查询。（小写L） m 只展示qps的信息。 M 显示状态信息。 o 反序排序。 s 显示信息的refresh间隔。 u 显示指定用户的连接信息。 V show variables的相关信息。（大写v）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[pt-table-checksum-MySQL主从校验工具使用(DSN方式)]]></title>
      <url>%2F2017%2F03%2F08%2Fpt-table-checksum-MySQL%E4%B8%BB%E4%BB%8E%E6%A0%A1%E9%AA%8C%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8-DSN%E6%96%B9%E5%BC%8F%2F</url>
      <content type="text"><![CDATA[pt-table-checksum是校验主从是否一致的校验工具。 本文为什么使用的是DSN方式呢，因为喜欢自由，DSN方式可以按照自己指定的方式发现从库，并且查阅网上教程很难发现完整的DSN使用方式，导致我研究了好久，所以总结下自己的研究成果。 一、原理pt-table-checksum在基于binlog_format=row的模式下分别主库和从库上执行checksum的SQL语句，分别计算主库和从库表数据块的checksum值。而后比较主从的checksum值是否一致来判断主从数据是否一致。数据块来自于根据主键／唯一索引将表的记录(row)切分成单个数据库(chunck)，同时也降低了数据库的负载。 二、安全性 pt-table-checksum一次检查一个表，并且将每个表的数据切分成多个单个的数据块，从而降低数据库的压力，并且在pt-table-checksum的执行过程中，它会自动检测数据库的负载、主从复制延迟，如果负载过大或者主从延迟过大，此时pt-table-checksum会自动暂停，进而降低数据库的负载或者主从复制延迟； pt-table-checksum自动设置session级别的 innodb_lock_wait_timeout 为1s，避免影响业务SQL的lock wait timeout； pt-table-checksum使用--max-load参数控制数据库负载，当数据库负载超过设置的阀值时，pt-table-checksum将会暂停自己，降低数据库压力。默认情况下，--max-load的主从复制延迟阀值为1s，并发线程(Threads_running)为25； 当用 Ctrl+C 停止任务后，工具会正常的完成当前 chunk 检测，下次使用 --resume 选项启动可以恢复继续下一个 chunk。 三、工作原理 pt-table-checksum连接到主库，并执行一系列变更参数语句，如 SET SESSION innodb_lock_wait_timeout=1;SET @@SQL_QUOTE_SHOW_CREATE = 1/*!40101, @@SQL_MODE='NO_AUTO_VALUE_ON_ZERO,NO_ENGINE_SUBSTITUTION'*/;SET SQL_MODE='NO_AUTO_VALUE_ON_ZERO,NO_ENGINE_SUBSTITUTION'/*!50108 SET @@binlog_format := 'STATEMENT'*/;SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ; 从主库上查找它的从库，默认通过show full processlist方式，还可以通过show slave hosts以及DSN方式发现从库； 查找从库是否有复制过滤规则，可以通过--nocheck-replication-filters跳过该项检查； 检查／创建checksums数据表，在确认数据表已经创建的前提下，可以通过--[no]create-replicate-table跳过该项检查； 获取一个个数据表，并依次检查； 如果是表的第一个chunk，那么chunk-size一般为1000；如果不是表的第一个chunk，那么采用19步中分析出的结果； 检查表结构，进行数据类型转换等，生成checksum的sql语句； 根据表上的索引和数据的分布，选择最合适的split表的方法； 开始对表做checksum操作； 删除该表的上一次checksum值，除非使用--resume参数； 进入需要做checksum的数据库中，USE database; 获取下一个chunk的数据记录； explain该chunk的checksum语句； 根据explain的结果判断该chunk的大小是否超过自定义的--chunk-size的大小，如果超过，将会忽略计算出来的chunk-size，使用自定义的--chunk-size； EXPLAIN SELECT COUNT(*) AS cnt, COALESCE(LOWER(CONV(BIT_XOR(CAST(CRC32(CONCAT_WS('#', `id`, CONCAT(ISNULL(`id`)))) AS UNSIGNED)), 10, 16)), 0) AS crc FROM `xxxx`.`xxxx` /*explain checksum table*/ 执行该chunk的checksum语句，该过程中会把该chunk上的记录加上for update锁； REPLACE INTO `percona`.`checksums` (db, tbl, chunk, chunk_index, lower_boundary, upper_boundary, this_cnt, this_crc) SELECT 'wing', 't', '1', NULL, NULL, NULL, COUNT(*) AS cnt, COALESCE(LOWER(CONV(BIT_XOR(CAST(CRC32(CONCAT_WS('#', `id`, CONCAT(ISNULL(`id`)))) AS UNSIGNED)), 10, 16)), 0) AS crc FROM `xxxx`.`xxxx` /*checksum table*/ show warnings查看是否有警告信息等； 获取chunk的checksum值 SELECT this_crc, this_cnt FROM `percona`.`checksums` WHERE db = 'xxx' AND tbl = 'xxx' AND chunk = '1' 更新主库chunk的checksum值 UPDATE `percona`.`checksums` SET chunk_time = 'xxx', master_crc = 'xxxx', master_cnt = 'xxx' WHERE db = 'xxx' AND tbl = 'xxx' AND chunk = '1' 调整下一个chunk的大小； 等待从库追上主库，此时如果主从延迟时间超过--max-load的设置值，pt工具将会暂停自己； 如果发现主库的max-load超过某个阈值，pt工具在这里将暂停； 继续下一个chunk，直到这个table被chunk完毕； 等待从库执行完checksum，便于生成汇总的统计结果； 发现并输出该表的主从是否存在差异； 循环每张表，直到所有表被检查完毕； 如果有warn/err/diff，则退出代码为1，正常退出为0。 四、使用示例# 本示例为DSN方式检查# 主库上创建percona数据库CREATE DATABASE percona;# 主库上创建checksums和dsns表CREATE TABLE checksums ( db CHAR(64) NOT NULL, tbl CHAR(64) NOT NULL, chunk INT NOT NULL, chunk_time FLOAT NULL, chunk_index VARCHAR(200) NULL, lower_boundary TEXT NULL, upper_boundary TEXT NULL, this_crc CHAR(40) NOT NULL, this_cnt INT NOT NULL, master_crc CHAR(40) NULL, master_cnt INT NULL, ts TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (db, tbl, chunk), INDEX ts_db_tbl (ts, db, tbl)) ENGINE=InnoDB DEFAULT CHARSET=utf8;CREATE TABLE `dsns` ( `id` int(11) NOT NULL AUTO_INCREMENT, `parent_id` int(11) DEFAULT NULL, `dsn` varchar(255) NOT NULL, PRIMARY KEY (`id`));# 主库上创建一个用户，从pt测试的服务器上既可以通过该用户连接主库，也可以通过该用户连接从库GRANT ALL PRIVILEGEES on percona.* to &apos;percona_user&apos;@&apos;%&apos; IDENTIFIED BY &apos;percona_pass&apos;;GRANT SELECT,LOCK TABLES,PROCESS,SUPER on *.* to &apos;percona_user&apos;@&apos;%&apos;;# 主库上dsns表插入从库信息INSERT INTO dsns(dsn) VALUES(&apos;h=repl_host,P=repl_port&apos;)# 连接主库执行pt工具pt-table-checksum h=&apos;master_host&apos;,u=&apos;percona_user&apos;,p=&apos;percona_pass&apos;,P=master_port --databases=xxx --tables=xx,xxx --nocheck-replication-filters --nocreate-replicate-table --no-check-binlog-format --recursion-method dsn=h=repl_host,P=repl_port,D=&apos;percona&apos;,t=&apos;dsns&apos;# 返回结果 TS ERRORS DIFFS ROWS CHUNKS SKIPPED TIME TABLE03-08T20:31:22 0 1 xx xx 0 1.058 xxx.xx03-08T20:32:22 0 0 xx xx 0 1.058 xxx.xxx 五、结果解析# pt-table-checksum工具检测返回结果 TS ERRORS DIFFS ROWS CHUNKS SKIPPED TIME TABLE03-08T20:31:22 0 1 xx xx 0 1.058 xxx.xx03-08T20:32:22 0 0 xx xx 0 1.058 xxx.xxx TS：pt工具结束执行的时间； ERRORS：checksum表的时候出现的error和warn的次数； DIFFS：当为0时，该表主从不存在差异，当为1时，该表主从存在差异； ROWS：进行checksum的表的记录数； CHUNKS：表被split为N个chunk，N为chunk的数量； SKIPPED：由于以下原因，pt跳过的chunk数； &gt;* MySQL not using the --chunk-index&gt;* MySQL not using the full chunk index (--[no]check-plan)&gt;* Chunk size is greater than --chunk-size * --chunk-size-limit&gt;* Lock wait timeout exceeded (--retries)&gt;* Checksum query killed (--retries)&gt; TIME：checksum表的时间长度； TABLE：checksum相应的表名。 六、发现差异那么如何通过checksums表发现差异呢，在每一个从库上执行如下SQL，即可选出有差异的表： SELECT db, tbl, SUM(this_cnt) AS total_rows, COUNT(*) AS chunksFROM percona.checksumsWHERE ( master_cnt &lt;&gt; this_cnt OR master_crc &lt;&gt; this_crc OR ISNULL(master_crc) &lt;&gt; ISNULL(this_crc))GROUP BY db, tbl; 七、常用参数–nocheck-binlog-format 不检查所有数据库的binlog_format是否一致，请在确认所有数据库的binlog_format一致的情况下，使用该参数。 –chunk-index=s 使用该索引将表split到多个chunk中。 –chunk-size=z 指定chunk的大小，默认为1000，即一个chunk容纳1000条数据记录。 –chunk-time=f 动态调整–chunk-size，当一个chunk的checksum时间超过该–chunk-time的时间。 –nocreate-replicate-table 不创建--replicate指定的database和table。 –function=s 指定checksum的hash函数，选择项：FNV1A_64,MURMUR_HASH, SHA1, MD5, CRC32等。 –recursion-method=a 指定发现slave的方式，选择项： –replicate=s 将checksum的指定结果写入到该表中，默认为percona.checksums。 –resume 从上一次中断的已完成的chunk开始继续执行下一个chunk，最终完成整个表的checksum。 –where=s 仅仅校验该where条件匹配的行数。 –ask-pass 交互式输入数据库验证码，命令行推荐使用。 –host 数据库host。 –port 数据库port。 –password 数据库密码。 –user 数据库用户。 –socket 数据库的socket文件。 –columns=a 仅仅校验指定的列。 –databases=h 仅仅校验指定的数据库。 –databases-regex=s 仅仅校验与该参数匹配的数据库。 –engines=h 仅仅校验指定engine的表。 –tables=h 仅仅校验指定的表。 –tables-regex=s 仅仅校验与该参数匹配的表。 参考文档：用pt-table-checksum校验数据一致性]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[SELECT … FOR UPDATE如何影响INNODB的锁级别]]></title>
      <url>%2F2017%2F03%2F01%2FSELECT-%E2%80%A6-FOR-UPDATE%E5%A6%82%E4%BD%95%E5%BD%B1%E5%93%8DINNODB%E7%9A%84%E9%94%81%E7%BA%A7%E5%88%AB%2F</url>
      <content type="text"><![CDATA[如果SELECT ... FOR UPDATE生效，需要在noautocommit的环境下，即BEGIN;COMMIT/ROLLBACK;或者SET AUTOCOMMIT=0的前提下。本文使用BEGIN;COMMIT/ROLLBACK;创造noautocommit的环境研究SELECT ... FOR UPDATE对于INNODB的锁级别影响。 约定 表结构如下 CREATE TABLE `t` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(16) NOT NULL, `num1` int(11) NOT NULL, `num2` int(11) NOT NULL, `num3` int(11) NOT NULL, PRIMARY KEY (`id`), UNIQUE KEY `ux_name` (`name`), KEY `ix_num1_num2` (`num1`,`num2`)) ENGINE=InnoDB AUTO_INCREMENT=67 DEFAULT CHARSET=utf8 表数据如下 select * from t;+----+-------+------+------+------+| id | name | num1 | num2 | num3 |+----+-------+------+------+------+| 1 | AAAAA | 0 | 2 | 2 || 2 | BBBBB | 1 | 2 | 2 || 3 | CCCCC | 0 | 0 | 0 || 4 | DD | 4 | 1 | 1 || 5 | EE | 0 | 5 | 5 || 66 | FFFFF | 0 | 5 | 5 |+----+-------+------+------+------+6 rows in set (0.00 sec) 实验一、WHERE条件使用主键 # session 1mysql&gt; begin;Query OK, 0 rows affected (0.00 sec)mysql&gt; explain select * from t where id=1;+----+-------------+-------+-------+---------------+---------+---------+-------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+-------+---------------+---------+---------+-------+------+-------+| 1 | SIMPLE | t | const | PRIMARY | PRIMARY | 4 | const | 1 | NULL |+----+-------------+-------+-------+---------------+---------+---------+-------+------+-------+1 row in set (0.00 sec)mysql&gt; select * from t where id=1 for update;+----+-------+------+------+------+| id | name | num1 | num2 | num3 |+----+-------+------+------+------+| 1 | AAAAA | 0 | 2 | 2 |+----+-------+------+------+------+1 row in set (0.00 sec)# session 2mysql&gt; begin;Query OK, 0 rows affected (0.00 sec)mysql&gt; select * from t where id=2 for update;+----+-------+------+------+------+| id | name | num1 | num2 | num3 |+----+-------+------+------+------+| 2 | BBBBB | 1 | 2 | 2 |+----+-------+------+------+------+1 row in set (0.00 sec)mysql&gt; select * from t where name="DD" for update;+----+------+------+------+------+| id | name | num1 | num2 | num3 |+----+------+------+------+------+| 4 | DD | 4 | 1 | 1 |+----+------+------+------+------+1 row in set (0.00 sec)mysql&gt; select * from t where id=1 for update;ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transactionmysql&gt; select * from t where name="AAAAA" for update;ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transactionmysql&gt; select * from t where num1=0 for update;ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transactionmysql&gt; select * from t where num3=2 for update;ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction 二、WHERE条件使用唯一索引 # session 1mysql&gt; begin;Query OK, 0 rows affected (0.00 sec)mysql&gt; explain select * from t where name='AAAAA' ;+----+-------------+-------+-------+---------------+---------+---------+-------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+-------+---------------+---------+---------+-------+------+-------+| 1 | SIMPLE | t | const | ux_name | ux_name | 50 | const | 1 | NULL |+----+-------------+-------+-------+---------------+---------+---------+-------+------+-------+1 row in set (0.00 sec)# session 2mysql&gt; begin;Query OK, 0 rows affected (0.00 sec)mysql&gt; select * from t where name ="DD" for update;+----+------+------+------+------+| id | name | num1 | num2 | num3 |+----+------+------+------+------+| 4 | DD | 4 | 1 | 1 |+----+------+------+------+------+1 row in set (0.00 sec)mysql&gt; select * from t where id=2 for update;+----+-------+------+------+------+| id | name | num1 | num2 | num3 |+----+-------+------+------+------+| 2 | BBBBB | 1 | 2 | 2 |+----+-------+------+------+------+1 row in set (0.00 sec)mysql&gt; select * from t where name='AAAAA' for update;ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transactionmysql&gt; select * from t where id=1 for update;ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transactionmysql&gt; select * from t where num1=0 for update;ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transactionmysql&gt; select * from t where num3=2 for update;ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction 三、WHERE条件使用普通索引 # session 1mysql&gt; begin;Query OK, 0 rows affected (0.00 sec)mysql&gt; explain select * from t where num1=0 and num2=5;+----+-------------+-------+------+---------------+--------------+---------+-------------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+------+---------------+--------------+---------+-------------+------+-------+| 1 | SIMPLE | t | ref | ix_num1_num2 | ix_num1_num2 | 8 | const,const | 2 | NULL |+----+-------------+-------+------+---------------+--------------+---------+-------------+------+-------+1 row in set (0.00 sec)mysql&gt; select * from t where num1=0 and num2=5 for update;+----+-------+------+------+------+| id | name | num1 | num2 | num3 |+----+-------+------+------+------+| 5 | EE | 0 | 5 | 5 || 66 | FFFFF | 0 | 5 | 5 |+----+-------+------+------+------+2 rows in set (0.00 sec)# session 2mysql&gt; begin;Query OK, 0 rows affected (0.00 sec)mysql&gt; select * from t where id=1 for update;+----+-------+------+------+------+| id | name | num1 | num2 | num3 |+----+-------+------+------+------+| 1 | AAAAA | 0 | 2 | 2 |+----+-------+------+------+------+1 row in set (0.00 sec)mysql&gt; select * from t where name="AAAAA" for update;+----+-------+------+------+------+| id | name | num1 | num2 | num3 |+----+-------+------+------+------+| 1 | AAAAA | 0 | 2 | 2 |+----+-------+------+------+------+1 row in set (0.00 sec)mysql&gt; select * from t where num1=0 and num2=5 for update;ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transactionmysql&gt; select * from t where num2=5 for update;ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transactionmysql&gt; select * from t where num3=5 for update;ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction 四、WHERE条件使用联合索引的前缀索引 # session 1mysql&gt; begin;Query OK, 0 rows affected (0.00 sec)mysql&gt; explain select * from t where num1=1 ;+----+-------------+-------+------+---------------+--------------+---------+-------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+------+---------------+--------------+---------+-------+------+-------+| 1 | SIMPLE | t | ref | ix_num1_num2 | ix_num1_num2 | 4 | const | 1 | NULL |+----+-------------+-------+------+---------------+--------------+---------+-------+------+-------+1 row in set (0.00 sec)mysql&gt; select * from t where num1=1 for update;+----+-------+------+------+------+| id | name | num1 | num2 | num3 |+----+-------+------+------+------+| 2 | BBBBB | 1 | 2 | 2 |+----+-------+------+------+------+1 row in set (0.00 sec)# session 2mysql&gt; begin;Query OK, 0 rows affected (0.00 sec)mysql&gt; select * from t where id =3;+----+-------+------+------+------+| id | name | num1 | num2 | num3 |+----+-------+------+------+------+| 3 | CCCCC | 0 | 0 | 0 |+----+-------+------+------+------+1 row in set (0.00 sec)mysql&gt; select * from t where id =3 for update -&gt; ;+----+-------+------+------+------+| id | name | num1 | num2 | num3 |+----+-------+------+------+------+| 3 | CCCCC | 0 | 0 | 0 |+----+-------+------+------+------+1 row in set (0.00 sec)mysql&gt; select * from t where num1=4 for update; -- 使用了普通索引+----+------+------+------+------+| id | name | num1 | num2 | num3 |+----+------+------+------+------+| 4 | DD | 4 | 1 | 1 |+----+------+------+------+------+1 row in set (0.00 sec)mysql&gt; explain select * from t where num1=4 for update;+----+-------------+-------+------+---------------+--------------+---------+-------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+------+---------------+--------------+---------+-------+------+-------+| 1 | SIMPLE | t | ref | ix_num1_num2 | ix_num1_num2 | 4 | const | 1 | NULL |+----+-------------+-------+------+---------------+--------------+---------+-------+------+-------+1 row in set (0.00 sec)mysql&gt; select * from t where num1=0 for update; -- 使用了全表扫描ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transactionmysql&gt; explain select * from t where num1=0 for update;+----+-------------+-------+------+---------------+------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+------+---------------+------+---------+------+------+-------------+| 1 | SIMPLE | t | ALL | ix_num1_num2 | NULL | NULL | NULL | 6 | Using where |+----+-------------+-------+------+---------------+------+---------+------+------+-------------+1 row in set (0.00 sec) 五、WHERE条件不使用索引 # session 1mysql&gt; begin;Query OK, 0 rows affected (0.00 sec)mysql&gt; explain select * from t where num3=1 ;+----+-------------+-------+------+---------------+------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+------+---------------+------+---------+------+------+-------------+| 1 | SIMPLE | t | ALL | NULL | NULL | NULL | NULL | 6 | Using where |+----+-------------+-------+------+---------------+------+---------+------+------+-------------+1 row in set (0.00 sec)mysql&gt; select * from t where num3=1 for update;+----+------+------+------+------+| id | name | num1 | num2 | num3 |+----+------+------+------+------+| 4 | DD | 4 | 1 | 1 |+----+------+------+------+------+1 row in set (0.00 sec)# session 2mysql&gt; begin;Query OK, 0 rows affected (0.00 sec)mysql&gt; select * from t where id=1 for update;ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transactionmysql&gt; select * from t where name='BBBBB' for update;ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction 总结 WHERE条件使用主键，SELECT ... FOR UPDATE为行级锁； WHERE条件使用唯一索引，SELECT ... FOR UPDATE为行级锁； WHERE条件使用普通索引，SELECT ... FOR UPDATE为行级锁； WHERE条件使用联合索引的前缀索引，SELECT ... FOR UPDATE为行级锁； WHERE条件不使用索引，SELECT ... FOR UPDATE为表级锁； 即：WHERE条件能使用索引时，SELECT ... FOR UPDATE表现为行级锁；WHERE条件不使用索引，SELECT ... FOR UPDATE表现为表级锁；]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Flask-Migrate的使用]]></title>
      <url>%2F2017%2F02%2F26%2FFlask-Migrate%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
      <content type="text"><![CDATA[既然说了flask-sqlalchemy，怎么着也得说说flask-migration了，毕竟平时数据库变更对于非DBA来说还是挺困难的呢。。。那么，接下来请点击全文吧～ 安装命令 pip install flask-migrate 注意⚠️ 相关的表结构示例，请点击该链接。 一、配置flask-migrate# 文件名 hello.pyfrom flask_migrate import Migrate,MigrateCommandmigrate = Migrate(app,db)manager.add_command('db',MigrateCommand) 二、使用init命令创建迁移仓库python hello.py db init Creating directory /Users/domain/mycoding/web/migrations ... done Creating directory /Users/domain/mycoding/web/migrations/versions ... done Generating /Users/domain/mycoding/web/migrations/alembic.ini ... done Generating /Users/domain/mycoding/web/migrations/env.py ... done Generating /Users/domain/mycoding/web/migrations/env.pyc ... done Generating /Users/domain/mycoding/web/migrations/README ... done Generating /Users/domain/mycoding/web/migrations/script.py.mako ... done Please edit configuration/connection/logging settings in '/Users/domain/mycoding/web/migrations/alembic.ini' before proceeding.teardown requesthttp://localhost/ 三、使用migrate命令创建迁移脚本python hello.py db migrate -m "initial migrateion"INFO [alembic.runtime.migration] Context impl MySQLImpl.INFO [alembic.runtime.migration] Will assume non-transactional DDL.INFO [alembic.autogenerate.compare] Detected added table 'roles'INFO [alembic.autogenerate.compare] Detected added table 'users'INFO [alembic.autogenerate.compare] Detected added index 'ix_users_username' on '['username']' Generating /Users/domain/mycoding/web/migrations/versions/e89e2c8b3174_initial_migrateion.py ... done 该步骤生成的迁移脚本，可在./migrations/versions/的文件夹下面查看，迁移脚本中的upgrade()和downgrade()函数，它们的作用分别是： upgrade()：将迁移中的改动应用到数据库中。 downgrade()：将迁移中的改动从数据库中删除，即具有回滚到某个迁移点的功能。 该步骤还会在数据库中生成alembic_version表，该表作为flask-migrateion的迁移记录表。 四、使用upgrade命令将迁移中的改动应用到数据库中python hello.py db upgradeINFO [alembic.runtime.migration] Context impl MySQLImpl.INFO [alembic.runtime.migration] Will assume non-transactional DDL.INFO [alembic.runtime.migration] Running upgrade -&gt; e89e2c8b3174, initial migrateion 五、使用downgrade命令回滚迁移中的数据库改动 # 首先在之前flaks-sqlalchemy表结构的user表里面添加一个email字段# class User(db.Model):# ...# email = db.Column(db.String(32), nullable=False, server_default='')# ...# 使用python hello.py db migrate -m 'add email column to user table'命令生成迁移脚本python hello.py db migrate -m 'add email column to user table'INFO [alembic.runtime.migration] Context impl MySQLImpl.INFO [alembic.runtime.migration] Will assume non-transactional DDL.INFO [alembic.autogenerate.compare] Detected added column 'users.email' Generating /Users/domain/mycoding/web/migrations/versions/c779bf595f79_add_email_column_to_user_table.py ... done# 使用python hello.py db upgrade将迁移脚本中的改动应用到python hello.py db upgradeINFO [alembic.runtime.migration] Context impl MySQLImpl.INFO [alembic.runtime.migration] Will assume non-transactional DDL.INFO [alembic.runtime.migration] Running upgrade d303dfaaefba -&gt; c779bf595f79, add email column to user table# 此时发现并不想做这个变更，想要回滚到上一个版本咋办呢，来来来，我教你个大招python hello.py db downgrade d303dfaaefbaINFO [alembic.runtime.migration] Context impl MySQLImpl.INFO [alembic.runtime.migration] Will assume non-transactional DDL.INFO [alembic.runtime.migration] Running downgrade c779bf595f79 -&gt; d303dfaaefba, add email column to user table# d303dfaaefba为上一个版本号，可以从数据库中的alembic_version表中查询到，也可以从上面python hello.py db upgrade的最后一个INFO信息中查看到，当然也可以用./migrations/versions/的文件夹下面查看到，总之看到版本号的地方很多，就看你是否是个有心人了～ 至此，Flask-Migrateion的用法就整理完毕了，当然Flask-Migration还有很多用法，但是上面这些是最常用的用法，更相信的用法请参考Flask-Migrationg官方文档。当然也请小伙伴们放心，在整个flask-migration迁移过程中，你的数据也会跟着迁移的，所以完全不用担心哈～ 本文整理参考文档：《Flask web开发》]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[使用flask-sqlalchemy玩转MySQL]]></title>
      <url>%2F2017%2F02%2F25%2F%E4%BD%BF%E7%94%A8flask-sqlalchemy%E7%8E%A9%E8%BD%ACMySQL%2F</url>
      <content type="text"><![CDATA[每个项目都离不开数据库，所以要是打算用Flask写一个程序，自然也离不开flask-sqlalchemy。作为一个MySQL DBA，当然会首选MySQL作为程序的数据库，毕竟我擅长于此。那么就来总结下，怎么用flask-sqlalchemy来玩转MySQL吧。 安装命令 pip install mysql-pythonpip install flask-sqlalchemy 相关示例Python代码 # 文件名hello.pyfrom flask import Flaskfrom flask_sqlalchemy import SQLAlchemyapp = Flask(__name__)app.config['SECRET_KEY'] = 'Fianna'app.config['SQLALCHEMY_DATABASE_URI'] = 'mysql://user:password@host:port/dbname'app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = Truedb = SQLAlchemy(app)class Role(db.Model): __tablename__ = 'roles' id = db.Column(db.Integer, nullable=False, primary_key=True, autoincrement=True) name = db.Column(db.String(16), nullable=False, server_default='', unique=True) def __repr__(self): return '&lt;Role %r&gt;' % self.nameclass User(db.Model): __tablename__ = 'users' id = db.Column(db.Integer, nullable=False, primary_key=True, autoincrement=True) username = db.Column(db.String(32), nullable=False, unique=True, server_default='', index=True) role_id = db.Column(db.Integer, nullable=False, server_default='0') def __repr__(self): return '&lt;User %r,Role id %r&gt;' %(self.username,self.role_id) 一、连接数据库app.config['SQLALCHEMY_DATABASE_URI'] = 'mysql://user:password@host:port/dbname' 二、创建所有表python&gt;&gt;&gt; from hello import db,Role,User&gt;&gt;&gt; db.create_all() 三、删除所有表python&gt;&gt;&gt; from hello import db,Role,User&gt;&gt;&gt; db.drop_all() 四、插入行# 插入单行python&gt;&gt;&gt; from hello import db,Role,User&gt;&gt;&gt; db.session.add(Role(name='Admin'))&gt;&gt;&gt; db.session.commit()&gt;&gt;&gt; db.session.add(Role(name='Moderator'))&gt;&gt;&gt; db.session.add(Role(name='User'))&gt;&gt;&gt; db.session.commit()# 插入多行python&gt;&gt;&gt; from hello import db,Role,User&gt;&gt;&gt; db.session.add_all([User(username='john',role_id=1),User(username='susan',role_id=3),User(username='david',role_id=3)])&gt;&gt;&gt; db.session.commit() 五、更新行python&gt;&gt;&gt; from hello import db,Role,User&gt;&gt;&gt; admin = Role.query.filter_by(name='Admin').first()&gt;&gt;&gt; admin.name='Administrator'&gt;&gt;&gt; db.session.commit() 六、删除行python hello.py shell&gt;&gt;&gt; from hello import db,Role,User&gt;&gt;&gt; mod = Role.query.filter_by(name='Moderator').first()&gt;&gt;&gt; db.session.delete(mod)&gt;&gt;&gt; db.session.commit() 七、查询表 查询表中全部数据 # 注意，此处的查询结果完全取决于代码示例中的# def __repr__(self)# 这段函数，请各位小主们，好好斟酌下～python&gt;&gt;&gt; from hello import db,Role,User&gt;&gt;&gt; Role.query.all()[&lt;Role u'Administrator'&gt;, &lt;Role u'User'&gt;]&gt;&gt;&gt; User.query.all()[&lt;User u'john',Role id 1L&gt;, &lt;User u'susan',Role id 3L&gt;, &lt;User u'david',Role id 3L&gt;] 按照一个条件过滤数据记录(where) python&gt;&gt;&gt; from hello import db,Role,User&gt;&gt;&gt; Role.query.filter_by(name='Administrator').first()&lt;Role u'Administrator'&gt;&gt;&gt;&gt; User.query.filter_by(role_id=3).all()[&lt;User u'susan',Role id 3L&gt;, &lt;User u'david',Role id 3L&gt;]&gt;&gt;&gt; User.query.filter_by(role_id=3).first()&lt;User u'susan',Role id 3L&gt; 按照两个条件过滤数据记录(where and) python&gt;&gt;&gt; from hello import db,Role,User&gt;&gt;&gt; User.query.filter_by(role_id=3,username='susan').first()&lt;User u'susan',Role id 3L&gt;&gt;&gt;&gt; User.query.filter_by(role_id=3,username='susan').all()[&lt;User u'susan',Role id 3L&gt;] 聚合(count) python hello.py shell&gt;&gt;&gt; from hello import db,Role,User&gt;&gt;&gt; User.query.filter_by(role_id=3,username='susan').count()1L&gt;&gt;&gt; User.query.filter_by(role_id=3).count()2L&gt;&gt;&gt; User.query.count()3L 求和(sum) python hello.py shell&gt;&gt;&gt; from hello import db,Role,User&gt;&gt;&gt; from sqlalchemy.sql import func&gt;&gt;&gt; User.query.with_entities(func.sum(User.id)).all()[(Decimal('6'),)]&gt;&gt;&gt; User.query.with_entities(func.sum(User.role_id)).all()[(Decimal('7'),)] 平均数(avg) python&gt;&gt;&gt; from hello import db,Role,User&gt;&gt;&gt; from sqlalchemy.sql import func&gt;&gt;&gt; User.query.with_entities(func.avg(User.role_id)).all()[(Decimal('2.3333'),)]&gt;&gt;&gt; User.query.with_entities(func.avg(User.id)).all()[(Decimal('2.0000'),)] 排序(order by) python&gt;&gt;&gt; from hello import db,Role,User# 升序(asc)&gt;&gt;&gt; User.query.order_by(User.role_id).all()[&lt;User u'john',Role id 1L&gt;, &lt;User u'susan',Role id 3L&gt;, &lt;User u'david',Role id 3L&gt;]# 降序(desc)&gt;&gt;&gt; User.query.order_by(User.role_id.desc()).all()[&lt;User u'susan',Role id 3L&gt;, &lt;User u'david',Role id 3L&gt;, &lt;User u'john',Role id 1L&gt;] 分组(group by) python hello.py shell&gt;&gt;&gt; from hello import db,Role,User&gt;&gt;&gt; User.query.group_by(User.role_id).all()[&lt;User u'john',Role id 1L&gt;, &lt;User u'susan',Role id 3L&gt;] 限制(limit) python&gt;&gt;&gt; from hello import db,Role,User&gt;&gt;&gt; User.query.all()[&lt;User u'john',Role id 1L&gt;, &lt;User u'susan',Role id 3L&gt;, &lt;User u'david',Role id 3L&gt;]# limit 1&gt;&gt;&gt; User.query.limit(1).all()[&lt;User u'john',Role id 1L&gt;]# limit 2,1&gt;&gt;&gt; User.query.limit(1).offset(2).all()[&lt;User u'david',Role id 3L&gt;]&gt;&gt;&gt; User.query.filter_by(role_id=3).all()[&lt;User u'susan',Role id 3L&gt;, &lt;User u'david',Role id 3L&gt;]# limit 1&gt;&gt;&gt; User.query.filter_by(role_id=3).limit(1).all()[&lt;User u'susan',Role id 3L&gt;]# limit 1,1&gt;&gt;&gt; User.query.filter_by(role_id=3).limit(1).offset(1).all()[&lt;User u'david',Role id 3L&gt;] 八、将Flask-SQLAlchemy的查询语句转换为SQLpython&gt;&gt;&gt; from hello import db,Role,User&gt;&gt;&gt; User.query.all()[&lt;User u'john',Role id 1L&gt;, &lt;User u'susan',Role id 3L&gt;, &lt;User u'david',Role id 3L&gt;]&gt;&gt;&gt; str(User.query)'SELECT users.id AS users_id, users.username AS users_username, users.role_id AS users_role_id \nFROM users'&gt;&gt;&gt; User.query.limit(1).all()[&lt;User u'john',Role id 1L&gt;]&gt;&gt;&gt; str(User.query.limit(1))'SELECT users.id AS users_id, users.username AS users_username, users.role_id AS users_role_id \nFROM users \n LIMIT %s'&gt;&gt;&gt; User.query.limit(1).offset(2).all()[&lt;User u'david',Role id 3L&gt;]&gt;&gt;&gt; str(User.query.limit(1).offset(2))'SELECT users.id AS users_id, users.username AS users_username, users.role_id AS users_role_id \nFROM users \n LIMIT %s, %s' 本文示例参考文档：《Flask web开发》]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python的requirements.txt文件生成与安装]]></title>
      <url>%2F2017%2F02%2F22%2Fpython%E7%9A%84requirements-txt%E6%96%87%E4%BB%B6%E7%94%9F%E6%88%90%E4%B8%8E%E5%AE%89%E8%A3%85%2F</url>
      <content type="text"><![CDATA[总是别人项目中看到requirements.txt文件，打开一看，全是项目依赖包的名字，后来通过google发现这东西居然可以安装依赖包。于是便有了这篇文章。 生成requirements.txtpip freeze &gt; requirements.txt 安装requirements.txtpip install -r requirements.txt]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[使用Python将IP在整型与字符串之间来去自如]]></title>
      <url>%2F2017%2F02%2F15%2F%E4%BD%BF%E7%94%A8Python%E5%B0%86IP%E5%9C%A8%E6%95%B4%E5%9E%8B%E4%B8%8E%E5%AD%97%E7%AC%A6%E4%B8%B2%E4%B9%8B%E9%97%B4%E6%9D%A5%E5%8E%BB%E8%87%AA%E5%A6%82%2F</url>
      <content type="text"><![CDATA[对于字符串型的IP存入数据库中，实在是个即浪费空间又浪费性能的家伙，所以可爱的人们想出来将IP转换为整型存储。MySQL中存在INET_ATON()、INET_NTOA()函数进行IP整型和字符串之间的转换，那么Python中存在什么方法可以实现MySQL中INET_ATON()、INET_NTOA()的功能呢？方法肯定是有的～ # 导入相关模块包import socketimport struct# 将IP从字符串转为整型&gt;&gt;&gt; int(socket.inet_aton('127.0.0.1').encode('hex'),16)2130706433# 将IP从整型转为字符串&gt;&gt;&gt; socket.inet_ntoa(struct.pack("!I",2130706433))'127.0.0.1']]></content>
    </entry>

    
    <entry>
      <title><![CDATA[sysbench参数详解]]></title>
      <url>%2F2017%2F02%2F07%2Fsysbench%E5%8F%82%E6%95%B0%E8%AF%A6%E8%A7%A3%2F</url>
      <content type="text"><![CDATA[之前介绍过sysbench部署，根据习惯，当然要介绍一下它的参数。sysbench版本：1.0 一、sysbench功能测试参数fileio 磁盘IO测试。 cpu CPU性能测试。 memory 内存分配及传输速度测试。 threads 线程性能测试。 mutex 互斥性能测试。 提示： 还存在oltp测试，为sysbench默认测试。 二、通用参数–num-threads=N 使用的线程数量，默认值为1。 –max-requests=N 总请求数，与--max-time选择一个设置即可，默认值为10000。 –max-time=N 总执行时间，与--max-requests选择一个设置即可，单位为s，默认值为0。 –forced-shutdown=STRING 超过--max-time后强制中断，默认为off。 –thread-stack-size=SIZE 每个线程的stack大小，默认为64K。 –tx-rate=N sysbench尝试像数据库发送的事务数tps。 –report-interval=N 表示N秒输出一次测试进度报告，0表示关闭测试进度报告输出，仅输出最终的报告结果，默认值为0。 –report-checkpoints=[LIST,…] dump full statistics and reset all counters at specified points in time. The argument is a list of comma-separated values representing the amount of time in seconds elapsed from start of test when report checkpoint(s) must be performed. Report checkpoints are off by default. [] –test=STRING 测试类型，可选项：fileio/cpu/memory/threads/mutex/oltp脚本路径。 –debug=[on|off] debug模式输出，默认值为off。 –validate=[on|off] 在可能的情况下执行验证检查，默认为off。 –help=[on|off] 输出help信息，默认为off。 –version=[on|off] 输出版本信息，默认为off。 –rand-type=STRING 表示随机类型的模式，共有4种模式：uniform(固定)，gaussian(高斯)，special(特定)，pareto(帕雷特)，默认值为：special。 –rand-spec-iter number of iterations used for numbers generation [12] –rand-spec-pct=N 对于’special’随机模式中指定值的比例，默认值为75。 –rand-seed=N seed for random number generator, ignored when 0 [0] –rand-pareto-h=N parameter h for pareto distibution [0.2] –config-file sysbench配置文件路径。 三、日志参数–verbosity=N 初测试报告信息之外的信息输出级别，5为debug信息，0位仅仅输出严重信息，默认值为3。 –percentile=N 查询相应时间采样的百分比，默认值为95%。 四、通用数据库参数–db-driver=STRING 特殊的数据库驱动。 –db-ps-mode=STRING SQL是否需要预编译，模式有：auto/disable，默认为disable。 –db-debug=[on|off] 输出数据库层面的debug信息，默认为off。 五、MySQL相关参数–mysql-host=[LIST,…] MySQL服务器IP/hostname，默认：localhost。 –mysql-port=[LIST,…] MySQL端口号，默认：3306。 –mysql-socket=[LIST,…] MySQL的socket文件。 –mysql-user=STRING MySQL的用户名，默认：sbtest –mysql-password=STRING MySQL用户密码。 –mysql-db=STRING MySQL数据库。 –mysql-table-engine=STRING 用户测试表的表结构引擎，可选项：myisam/innodb/bdb/heap/ndbcluster/federated，默认值：innodb。 –mysql-engine-trx=STRING 存储引擎是否使用事务，可选项：yes,no,auto，默认值：auto。 –mysql-ssl=[on|off] 使用SSL连接，默认值：off。 –mysql-ssl-cipher=STRING 为SSL连接指定密码。 –mysql-compression=[on|off] 使用压缩，默认值：off。 –myisam-max-rows=N MyISAM表的最大记录数，默认值：1000000。 –mysql-debug=[on|off] 输出MySQL的debug信息，默认值：off。 –mysql-ignore-errors=[LIST,…] MySQL忽略的错误代码，可选项：1213/1020/1205 –mysql-dry-run=[on|off] 假装MySQL所有客户端API都被调用，但实际并不执行它们，默认值：off 六、fileio相关参数sysbench --test=fileio help –file-num=N 创建文件的数量，默认值：128。 –file-block-size=N 每次IO操作的block大小，默认值：16K。 –file-total-size=SIZE 所有文件大小总和，默认值：2G。 –file-test-mode=STRING 测试模式：seqwr(顺序写), seqrewr(顺序读写), seqrd(顺序读), rndrd(随机读), rndwr(随机写), rndrw(随机读写)。 –file-io-mode 文件操作模式：sync(同步),async(异步),mmap(快速map映射)，默认值：sync。 –file-async-backlog number of asynchronous operatons to queue per thread [128]。 –file-extra-flags=STRING 使用额外的标志符来打开文件{sync,dsync,direct}。 –file-fsync-freq=N 在完成N次请求之后，执行fsync()，0表示不使用fsync，默认值：100。 –file-fsync-all=[on|off] 每次写操作后执行fsync()，默认值：off。 –file-fsync-end=[on|off] 测试结束后执行fsync()，默认值：on。 –file-fsync-mode=STRING 使用fsync或fdatasync方法进行同步，默认值：fsync。 –file-merged-requests=N 尽可能的合并N个IO请求数，0表示不合并，默认值：0。 –file-rw-ratio=N 测试时候的读写比例，默认值：1.5(即3:2)。 七、cpu相关参数sysbench --test=cpu help –cpu-max-prime=N 最大质数生成器的上限，默认值：10000。 八、memory相关参数sysbench --test=memory help –memory-block-size=SIZE 测试时内存块大小，默认值：1K。 –memory-total-size=SIZE 传输数据可使用的最大内存大小，默认值：100G。 –memory-scope=STRING 内存访问范围：global/local，默认值：global。 –memory-hugetlb=[on|off 从HugeTLB池分配内存，默认值：off。 –memory-oper=STRING 内存操作类型：read/ write/none，默认值：write。 –memory-access-mode=STRING 内存访问方式：seq(顺序)/rnd(随机)，默认值：seq。 九、threads相关参数sysbench --test=threads help –thread-yields=N 每个请求产生多少线程，默认值：1000。 –thread-locks=N 每个线程的锁的数量，默认值：8。 十、mutex相关参数sysbench --test=mutex help –mutex-num=N 数组互斥的总大小，默认值：4096。 –mutex-locks=N 每个线程互斥锁的数量，默认值：50000。 –mutex-loops=N 内部互斥锁的空循环数量，默认值：10000 十一、oltp相关参数待研究。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[sysbench部署]]></title>
      <url>%2F2017%2F02%2F07%2Fsysbench%E9%83%A8%E7%BD%B2%2F</url>
      <content type="text"><![CDATA[sysbench作为每一个系统管理员，都应该被用过，因为它实在太有点能耐～操作系统：Debian8 数据库版本：MySQL 5.6.X sysbench版本：1.0 获取sysbench源码 将sysbench源码放置在/usr/local目录下 &gt; mv sysbench /usr/local&gt; &gt; 安装依赖包 &gt; apt-get install m4 autoconf automake libtool&gt; &gt; 切换至sysbench安装目录，运行autogen.sh脚本 &gt; cd /usr/local/sysbench&gt; ./autogen.sh&gt; &gt; 运行configure &gt; # /usr/local/mysql为MySQL安装目录&gt; ./configure --with-mysql-includes=/usr/local/mysql/include --with-mysql-libs=/usr/local/mysql/lib&gt;&gt; # 如果此处使用的是Mariadb，则includes路径为/usr/local/mysql/include/mysql&gt; &gt; 运行make &gt; make&gt; &gt; 软链sysbench &gt; ln -s /usr/local/sysbench/sysbench/sysbench /usr/bin/sysbench&gt; 至此，sysbench安装完成啦～]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[[译]Triggerless design]]></title>
      <url>%2F2017%2F02%2F06%2F%E8%AF%91-Triggerless-design%2F</url>
      <content type="text"><![CDATA[个人对gh-ost官方文档中的Triggerless design一文的中文翻译。 不使用Trigger的设计理念介绍gh-ost不使用Trigger背后的逻辑和算法，其次是这样设计的含义，优点，以及缺点。 基于触发器迁移的理念这里有两个比较流行的online DDL工具： pt-online-schema-change Facebook OSC 前者使用同步设计原理：它在原表上创建三个触发器(AFTER INSERT，AFTER UPDATE，AFTER DELETE)。每个触发器将原表上的操作重播到ghost表中。因此，原表上的每一个UPDATE都会被重播到gh-ost表中，INSERT／DELETE也是如此。触发器和原表的操作存在与同一个事务空间里。 后者使用异步设计原理：它在原表上创建三个触发器(AFTER INSERT，AFTER UPDATE，AFTER DELETE)。它也会创建一个changelog表。触发器不会直接将原表操作重播到ghost表中。取而代之的是，它会在changelog中添加一条记录。原表上的UPDATE操作在changelog表中插入一条记录像”原表上有一个UPDATE操作，将这个值变为了另外一个值”，INSERT／DELETE也是如此。一个后台的线程会实时去查看changelog表的变化，并应用所有的变更应用到ghost表上。这是一个异步的操作，因为gh-ost重播操作的过程与原表的操作不在相同的事务空间中了，并且gh-ost的重播操作可能会延迟。值得注意的是，changelog的记录写入操作还是与原表操作存在于相同的事务空间中。 不基于触发器的异步迁移理念gh-os使用不基于触发器的异步迁移方案。但是它不需要触发器，因为它不像FB工具那样需要一个changelog表。因为它不是基于changelog表做数据迁移，而是基于MySQL二进制日志(binlog)。 gh-ost基于binlog_format=row(RBR)获取来源于原表的记录，你也可以基于binlog_format=statement(SBR)获取记录，详情请点击。 RBR格式的二进制日志将复杂的SQL语句(可能存在多表)分解为不同的单表单行记录，这样更易于将binlog应用到gh-ost上。 gh-ost将自己伪装成MySQL从库：gh-ost连上主库，并且从主库获取binlog。因此，它获取binlog日志，并且通过过滤得到原表的操作事件。 gh-ost可以直接连接到主库上，但是gh-ost更喜欢连接到一个从库上。但是该从库需要设置参数log_slave_updates=1和binlog_format=row(binlog_format也可以通过调整gh-ost参数来进行自动设置)。 读取二进制日志，特别是在在从库上读取二进制，进一步强调了算法的异步性。当有一个事物写入到了binlog中，它依旧需要等gh-ost伪装成一个从库之后，才开始获取binlog并且应用到gh-ost中。 异步设计也意味着有很多值得注意的因素，稍后将做讨论。 工作流程概览整个工作流程包括：原表上读取表数据，binlog读取操作事件，检查主从延迟以及其它的节流参数，将变更应用到ghost表中(通常是master上的ghost表)，通过二进制日志流发送提示等等。 工作流程如下： 初始化设置&amp;验证 初始化设置不是一个并发操作。 连接从库(推荐)／主库，检查主库标志 预验证ALTER语句 初始化验证：权限以及表是否存在 创建changelog和ghost表 在ghost表上执行ALTER语句 对比原表和ghost表的结构，检查共享列，共享唯一键，验证是否有外键，选择共享的唯一键，这个键用于处理表的唯一标识，比如数据迁移等 开始监听binlog，监听changelog表的事件 在changelog表上注入”good to go”的记录(被二进制日志拦截) 开始监听原表DML的binlog事件 获取之前原表和ghost表的共享唯一键在原表上的最小值和最大值 数据复制流程 该步骤包括多个移动部分，所有操作相互协调并发执行。 设置一个心跳机制：频繁的写入到changelog表(这是一个低负载的操作) 心跳机制不断的更新状态 定期(频繁)检查潜在的节流信息或者提示 在原表上通过行范围控制，将原表数据拆分为一个chunk一个chunk，并且添加到数据迁移任务队列中 通过binlog获取原表的DML语句，并且添加到binlog重播任务队列中 处理数据迁移任务队列和binlog重播任务队列，并将其顺序的应用到ghost表上(当遇到节流操作或者hint提示时，将会暂停该操作) 当数据迁移与binlog重播完成后，将会在changelog表上注入／拦截”copy all done”的记录 当-postpone-cut-over-flag-file参数设置的文件存在时，将会推迟接下来的cut-over操作(但是原表的DML操作依旧会通过binlog应用到ghost表上) 结束操作：交换表流程 将原表加上写锁，binlog事件会继续应用在gh-ost上(该步骤是个异步操作，因此即使表被锁住，gh-ost仍然可以处理队列中未处理完的binlog事件) 将原表rename为_tablename_del表，ghost则rename为tablename表 &gt; rename /* gh-ost */ table `wing`.`t` to `wing`.`_t_del`, `wing`.`_t_gho` to `wing`.`t`&gt; &gt; 清理工作：删除需要被清除的表 异步设计优点 Cut-over阶段 异步设计最复杂的地方在于cut-over阶段：原表和ghost表的交换。在同步设计中，由于原表操作与触发器操作是在同一个事物空间中的，所以原表和ghost表的数据始终是同步的，因此一个简单的原表和ghost表交换(Rename)是可以存在的。 在异步设计中，即使我们对原表加锁，管道中仍然会存在一些事件，binlog日志依旧会将来自原表的事件重播到ghost表上。采用同步设计中交换表的方式是不可取的，因为这意味着，即使还没有对来自原表的事件重播完毕，就开始使用重命名后的ghost表了，这将造成数据不一致。 Facebook的使用”中断机制”，二步重命名法： 锁住原表，处理积压的事件(backlog) 将原表重命名 将ghost重命名为原表的名字 在两个表交换的时候，会有一个表不存在的阶段，因此会存在”表中断”的问题。 gh-ost通过”two-step”算法解决这个问题，”two-step”算法会阻塞表写入，然后将两表交换。它使操作很安全，要么成功，要么失败则回滚到cut-over阶段之前。 更多的信息请阅读cut-over 分离去耦 不使用触发器的异步设计最大的影响在于工作负载的去耦。使用触发器的设计，不管是同步还是异步方法，原表上的每一个写入意味着需要立刻在另外一张表上写入。 We will break down the meaning of workload decoupling, shortly. But it is important to understand that gh-ost interprets the situation in its own time and acts in its own time, yet still makes this an online operation. The decoupling is important not only as the tool’s logic goes, but very importantly as the master server sees it. As far as the master knows, write to the table and writes to the ghost table are unrelated 写负载 不使用触发器意味着主库不需要有过多的写负载用在存储过程上，以及对gh-ost表的锁争用上。 将原表操作重播到ghost表上完全由gh-ost工具完成。因此gh-ost可以决定何时将数据写入到ghost表中。为了分解原表的写负载，gh-ost工具选择使用一个单独的线程将原表操作重播到ghost表上。 MySQL对一个表大量并发写入的时候性能不是很好，这个时候锁变成了一个很大的问题。This is why we choose to alternate between the massive row-copy and the ongoing binlog events backlog such that the server only sees writes from a single connection。 更有趣的是，gh-ost是唯一写入到ghost表的程序，没有人意识到ghost表的存在。因此，gh-ost工具不存在由触发器产生的高并发性问题以及资源高争用问题。 可暂停性 当gh-ost暂停工作(节流导致)，此时将会没有任何数据写入到ghost表中。因为gh-ost不存在触发器，写负载是与gh-ost写负载分开的。并且由于gh-ost使用异步设计的方法，gh-ost算法已经处理了主库写入与ghost表写入的时间差。对于gh-ost来说，几毫秒的时间差与几小时的时间差并没有任何区别，对于gh-ost的运行并没有任何的影响。 当gh-ost进行节流操作的时候，不管是因为主从复制延迟，还是达到max-load设置阀值等，原表还是正常操作。仅仅只是对ghost表没有任何的写操作。除了changelog表上的heartbeat在不断的更新，但这个操作带来的性能影响是可以忽略不计的。 可测试性 我们甚至可以测试数据迁移(migration)步骤：就像我们将数据迁移的操作与主库的负载分开一样，我们不在主库上应用所有的变更，我们选择一个从库，这样我们可以在从库上进行表的数据迁移(migration)。 这本身是一个很不错的功能；它为我们提供了测试的可能性：正如我们完成数据迁移一样，我们从库的复制(stop slave)。gh-ost会进行cut-over阶段，但是gh-ost也会回滚回去。gh-ost测试时不会删除任何的表。结果是从库上的原表和ghost表都会存在，也不会做进一步的变更操作(因为此时主从复制已经停止)。我们可以对两张表进行对比。 这个方法可以用于验证gh-ost工具的正确性：在多个生产从库上不断的重复的做”数据迁移”(实际上并不会修改列)。每次数据迁移后都对原表和ghost表做一次数据校验。gh-ost预计是所有的表数据校验都是一致的。 多表并发操作 gh-ost可以运行多个不同的表并发操作(当然不是在相同的表上并发操作)。gh-ost异步设计的方法是支持多个不同的表并发操作的。事实上没有触发器的存在，多个不同的表并发操作gh-ost对于主库来说，只是并发的多个连接而已。每个gh-ost都可以控制自己的节流，或者全部一次性控制它们的节流操作。 Going outside the server space More to come as we make progress 异步设计缺点 增加流量 现有的工具都是通过触发器来重播原表上的操作。gh-ost是通过自己来获取原表操作，然后重播到gh-ost表上。gh-ost当然更喜欢在从库上获取原表操作，然后在主库上重播到gh-ost表上。这也意味着，主库机器和从库机器之间存在数据的传输。并且gh-ost使用的MySQL客户端不支持压缩功能， and so during a migration you can expect the full volume of a table to transfer on the wire。 增加代码复杂性 基于触发器同步方法的online DDL工具相对来说代码较少。大量的数据迁移是基于触发器完成的。回滚，数据类型以及cut-over阶段都是由数据库隐式处理的。gh-ost的异步方法让它的代码变的更复杂。它分别连接到主库和从库，伪装成从库，向主库写入心跳事件，在从库上读取binlog事件并写入到主库上，它还需要管理连接失败，主从复制延迟等等。 因此，gh-ost拥有更大的代码库以及更复杂的异步并发逻辑。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[GitHub开源MySQL Online DDL工具gh-ost参数解析]]></title>
      <url>%2F2017%2F02%2F06%2FGitHub%E5%BC%80%E6%BA%90MySQL-Online-DDL%E5%B7%A5%E5%85%B7gh-ost%E5%8F%82%E6%95%B0%E8%A7%A3%E6%9E%90%2F</url>
      <content type="text"><![CDATA[之前整理了一篇GitHub开源MySQL Online DDL工具gh-ost安装文档，现在就让大家了解下gh-ost里面的各种可爱的参数吧。gh-ost版本：1.0.28 -allow-master-master 允许gh-ost运行在双主复制架构中，一般与-assume-master-host参数一起使用。 -allow-nullable-unique-key 允许gh-ost在数据迁移(migrate)依赖的唯一键可以为NULL，默认为不允许为NULL的唯一键。如果数据迁移(migrate)依赖的唯一键允许NULL值，则可能造成数据不正确，请谨慎使用。 -allow-on-master 允许gh-ost直接运行在主库上。默认gh-ost连接的从库。 -alter string ALTER语句的body部分，如”ALTER TABLE wing ADD COLUMN id int not null default 0”，使用gh-ost的-alter参数时，写成-alter ADD COLUMN id int not null default 0即可。 -approve-renamed-columns ALTER 如果你修改一个列的名字(如change column)，gh-ost将会识别到并且需要提供重命名列名的原因，默认情况下gh-ost是不继续执行的，除非提供-approve-renamed-columns ALTER。 e.g: 没有添加-approve-renamed-columns ALTER参数，并且修改列名： gh-ost -user=&quot;wing&quot; -host=&quot;127.0.0.1&quot; -port=3306 -database=&quot;wing&quot; -table=&quot;t&quot; -password=&quot;wing&quot; -alter=&quot;change column col1 c1 int not null default 0&quot; -assume-rbr -execute2016-12-07 16:39:55 FATAL gh-ost believes the ALTER statement renames columns, as follows: map[col1:col11]; as precation, you are asked to confirm gh-ost is correct, and provide with --approve-renamed-columns, and we’re all happy. Or you can skip renamed columns via --skip-renamed-columns, in which case column data may be lost 没有添加-approve-renamed-columns ALTER参数，并且修改列名为相同名称: gh-ost -user=&quot;wing&quot; -host=&quot;127.0.0.1&quot; -port=3306 -database=&quot;wing&quot; -table=&quot;t&quot; -password=&quot;wing&quot; -alter=&quot;change column c1 c1 int not null default 0&quot; -assume-rbr -execute Migrating wing.t; Ghost table is wing._t_ghoMigrating wing-01:3306; inspecting wing-02:3306; executing on wing-02Migration started at Wed Dec 07 17:03:22 +0800 2016chunk-size: 1000; max-lag-millis: 1500ms; max-load: ; critical-load: ; nice-ratio: 0.000000throttle-additional-flag-file: /tmp/gh-ost.throttleServing on unix socket: /tmp/gh-ost.wing.t.sockCopy: 0/3 0.0%; Applied: 0; Backlog: 0/100; Time: 0s(total), 0s(copy); streamer: mysql-bin.000005:62874; State: migrating; ETA: N/ACopy: 0/3 0.0%; Applied: 0; Backlog: 0/100; Time: 1s(total), 1s(copy); streamer: mysql-bin.000005:64686; State: migrating; ETA: N/ACopy: 3/3 100.0%; Applied: 0; Backlog: 0/100; Time: 1s(total), 1s(copy); streamer: mysql-bin.000005:64686; State: migrating; ETA: dueCopy: 3/3 100.0%; Applied: 0; Backlog: 1/100; Time: 2s(total), 1s(copy); streamer: mysql-bin.000005:69035; State: migrating; ETA: dueMigrating wing.t; Ghost table is wing._t_ghoMigrating wing-01:3306; inspecting wing-02:3306; executing on wing-02Migration started at Wed Dec 07 17:03:22 +0800 2016chunk-size: 1000; max-lag-millis: 1500ms; max-load: ; critical-load: ; nice-ratio: 0.000000throttle-additional-flag-file: /tmp/gh-ost.throttleServing on unix socket: /tmp/gh-ost.wing.t.sockCopy: 3/3 100.0%; Applied: 0; Backlog: 0/100; Time: 2s(total), 1s(copy); streamer: mysql-bin.000005:69759; State: migrating; ETA: due 添加-approve-renamed-columns ALTER参数，并且修改列名： gh-ost -user=&quot;wing&quot; -host=&quot;127.0.0.1&quot; -port=3306 -database=&quot;wing&quot; -table=&quot;t&quot; -password=&quot;wing&quot; -alter=&quot;change column col1 c1 int not null default 0&quot; -assume-rbr -execute -approve-renamed-columns ALTER Migrating wing.t; Ghost table is wing._t_ghoMigrating wing-01:3306; inspecting wing-02:3306; executing on wing-02Migration started at Wed Dec 07 16:42:16 +0800 2016chunk-size: 1000; max-lag-millis: 1500ms; max-load: ; critical-load: ; nice-ratio: 0.000000throttle-additional-flag-file: /tmp/gh-ost.throttleServing on unix socket: /tmp/gh-ost.wing.t.sockCopy: 0/3 0.0%; Applied: 0; Backlog: 0/100; Time: 0s(total), 0s(copy); streamer: mysql-bin.000005:41767; State: migrating; ETA: N/ACopy: 0/3 0.0%; Applied: 0; Backlog: 0/100; Time: 1s(total), 1s(copy); streamer: mysql-bin.000005:43576; State: migrating; ETA: N/ACopy: 3/3 100.0%; Applied: 0; Backlog: 0/100; Time: 1s(total), 1s(copy); streamer: mysql-bin.000005:43576; State: migrating; ETA: dueCopy: 3/3 100.0%; Applied: 0; Backlog: 1/100; Time: 2s(total), 1s(copy); streamer: mysql-bin.000005:47927; State: migrating; ETA: dueMigrating wing.t; Ghost table is wing._t_ghoMigrating wing-01:3306; inspecting wing-02:3306; executing on wing-02Migration started at Wed Dec 07 16:42:16 +0800 2016chunk-size: 1000; max-lag-millis: 1500ms; max-load: ; critical-load: ; nice-ratio: 0.000000throttle-additional-flag-file: /tmp/gh-ost.throttleServing on unix socket: /tmp/gh-ost.wing.t.sockCopy: 3/3 100.0%; Applied: 0; Backlog: 0/100; Time: 2s(total), 1s(copy); streamer: mysql-bin.000005:48651; State: migrating; ETA: due -assume-master-host string 为gh-ost指定一个主库，格式为”ip:port”或者”hostname:port”。默认推荐gh-ost连接从库。 -assume-rbr 确认gh-ost连接的数据库实例的binlog_format=ROW的情况下，可以指定-assume-rbr，这样可以禁止从库上运行stop slave,start slave,执行gh-ost用户也不需要SUPER权限。 -chunk-size int 在每次迭代中处理的行数量(允许范围：100-100000)，默认值为1000。 -concurrent-rowcount 该参数如果为True(默认值)，则进行row-copy之后，估算统计行数(使用explain select count(*)方式)，并调整ETA时间，否则，gh-ost首先预估统计行数，然后开始row-copy。 -conf string gh-ost的配置文件路径。 -critical-load string 一系列逗号分隔的status-name=values组成，当MySQL中status超过对应的values，gh-ost将会退出。 e.g: -critical-load Threads_connected=20,Connections=1500 指的是当MySQL中的状态值Threads_connected&gt;20,Connections&gt;1500的时候，gh-ost将会由于该数据库严重负载而停止并退出。 -critical-load-interval-millis int 当值为0时，当达到-critical-load，gh-ost立即退出。当值不为0时，当达到-critical-load，gh-ost会在-critical-load-interval-millis秒数后，再次进行检查，再次检查依旧达到-critical-load，gh-ost将会退出。 -cut-over string 选择cut-over类型:atomic/two-step，atomic(默认)类型的cut-over是github的算法，two-step采用的是facebook-OSC的算法。 -cut-over-lock-timeout-seconds int gh-ost在cut-over阶段最大的锁等待时间，当锁超时时，gh-ost的cut-over将重试。(默认值：3) -database string 数据库名称。 -debug debug模式。 -default-retries int 各种操作在panick前重试次数。(默认为60) -discard-foreign-keys 很危险的参数，慎用！ 该参数针对一个有外键的表，在gh-ost创建ghost表时，并不会为ghost表创建外键。该参数很适合用于删除外键，除此之外，请谨慎使用。 -exact-rowcount 准确统计表行数(使用select count(*)的方式)，得到更准确的预估时间。 -execute 实际执行alter&amp;migrate表，默认为不执行，仅仅做测试并退出，如果想要ALTER TABLE语句真正落实到数据库中去，需要明确指定-execute。 -force-named-cut-over When true, the ‘unpostpone|cut-over’ interactive command must name the migrated table。 -heartbeat-interval-millis int gh-ost心跳频率值，默认为500。 -help 显示gh-ost的帮助信息。 -hooks-hint string arbitrary message to be injected to hooks via GH_OST_HOOKS_HINT, for your convenience。 -hooks-path string hook文件存放目录(默认为empty，即禁用hook)。hook会在这个目录下寻找符合约定命名的hook文件来执行。 -host string 数据库的ip/hostname。(默认值：127.0.0.1)。 -initially-drop-ghost-table gh-ost操作之前，检查并删除已经存在的ghost表。该参数不建议使用，请手动处理原来存在的ghost表。默认不启用该参数，gh-ost直接退出操作。 -initially-drop-old-table gh-ost操作之前，检查并删除已经存在的旧表。该参数不建议使用，请手动处理原来存在的ghost表。默认不启用该参数，gh-ost直接退出操作。 -initially-drop-socket-file gh-ost强制删除已经存在的socket文件。该参数不建议使用，可能会删除一个正在运行的gh-ost程序，导致DDL失败。 -max-lag-millis int 主从复制最大延迟时间，当主从复制延迟时间超过该值后，gh-ost将采取节流(throttle)措施，默认值：1500s。 -max-load string 一系列逗号分隔的status-name=values组成，当MySQL中status超过对应的values，gh-ost将采取节流(throttle)措施。 e.g: -max-load Threads_connected=20,Connections=1500 指的是当MySQL中的状态值Threads_connected&gt;20,Connections&gt;1500的时候，gh-ost将采取节流(throttle)措施。 -migrate-on-replica gh-ost的数据迁移(migrate)运行在从库上，而不是主库上。Have the migration run on a replica, not on the master. This will do the full migration on the replica including cut-over (as opposed to –test-on-replica) -nice-ratio float 每次chunk时间段的休眠时间，范围[0.0…100.0]。 e.g: 0：每个chunk时间段不休眠，即一个chunk接着一个chunk执行； 1：每row-copy 1毫秒，则另外休眠1毫秒； 0.7：每row-copy 10毫秒，则另外休眠7毫秒。 -ok-to-drop-table gh-ost操作结束后，删除旧表，默认状态是不删除旧表，会存在_tablename_del表。 -panic-flag-file string 当这个文件被创建，gh-ost将会立即退出。 -password string MySQL密码。 -port int MySQL端口。 -postpone-cut-over-flag-file string 当这个文件存在的时候，gh-ost的cut-over阶段将会被推迟，直到该文件被删除。 -quiet 静默模式。 -replication-lag-query string 检查主从复制延迟的SQL语句，默认gh-ost通过show slave status获取Seconds_behind_master作为主从延迟时间依据。如果使用pt-heartbeat工具，检查主从复制延迟的SQL语句类似于: SELECT ROUND(UNIX_TIMESTAMP() - MAX(UNIX_TIMESTAMP(ts))) AS delay FROM my_schema.heartbeat; -serve-socket-file string gh-ost的socket文件绝对路径。 -serve-tcp-port int gh-ost使用端口，默认为关闭端口。 -skip-renamed-columns ALTER 如果你修改一个列的名字(如change column)，gh-ost将会识别到并且需要提供重命名列名的原因，默认情况下gh-ost是不继续执行的。该参数告诉gh-ost跳该列的数据迁移，让gh-ost把重命名列作为无关紧要的列。该操作很危险，你会损失该列的所有值。 e.g: 原始表数据： &gt; mysql&gt; select * from t;&gt; +----+------+----+------+------+------+-------+&gt; | id | name | c1 | col2 | col3 | col4 | col11 |&gt; +----+------+----+------+------+------+-------+&gt; | 1 | | 22 | 0 | 0 | 0 | 0 |&gt; | 2 | | 22 | 0 | 0 | 0 | 0 |&gt; | 3 | | 22 | 0 | 0 | 0 | 0 |&gt; +----+------+----+------+------+------+-------+&gt; &gt; 执行命令： gh-ost -user=&quot;wing&quot; -host=&quot;127.0.0.1&quot; -port=3306 -database=&quot;wing&quot; -table=&quot;t&quot; -password=&quot;wing&quot; -alter=&quot;change column c1 col1 int not null default 0&quot; -assume-rbr -execute -skip-renamed-columns ALTER 操作后表数据: &gt; mysql&gt; select * from t;&gt; +----+------+------+------+------+------+-------+&gt; | id | name | col1 | col2 | col3 | col4 | col11 |&gt; +----+------+------+------+------+------+-------+&gt; | 1 | | | 0 | 0 | 0 | 0 |&gt; | 2 | | | 0 | 0 | 0 | 0 |&gt; | 3 | | | 0 | 0 | 0 | 0 |&gt; +----+------+------+------+------+------+-------+&gt; 3 rows in set (0.00 sec)&gt; -stack 添加错误堆栈追踪。 -switch-to-rbr 让gh-ost自动将从库的binlog_format转换为ROW格式。 -table string 表名称。 -test-on-replica 在从库上测试gh-ost，包括在从库上数据迁移(migration)，数据迁移完成后stop slave，原表和ghost表立刻交换而后立刻交换回来。继续保持stop slave，使你可以对比两张表。 -test-on-replica-skip-replica-stop 当-test-on-replica执行时，该参数表示该过程中不用stop slave。 -throttle-additional-flag-file string 当该文件被创建后，gh-ost操作立即停止。该参数可以用在多个gh-ost同时操作的时候，创建一个文件，让所有的gh-ost操作停止，或者删除这个文件，让所有的gh-ost操作恢复。 -throttle-control-replicas string 列出所有需要被检查主从复制延迟的从库。 e.g: -throttle-control-replica=192.16.12.22:3306,192.16.12.23:3307,192.16.13.12:3308 -throttle-flag-file string 当该文件被创建后，gh-ost操作立即停止。该参数适合控制单个gh-ost操作。-throttle-additional-flag-file string适合控制多个gh-ost操作。 -throttle-query string 节流查询。每秒钟执行一次。当返回值=0时不需要节流，当返回值&gt;0时，需要执行节流操作。该查询会在数据迁移(migrated)服务器上操作，所以请确保该查询是轻量级的。 -tungsten 告诉gh-ost你正在运行的是一个tungsten-replication拓扑结构。 -user string MySQL用户。 -verbose gh-ost运行时输出详细信息。 -version 输出gh-ost版本信息并退出。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[GitHub开源MySQL Online DDL工具gh-ost安装文档]]></title>
      <url>%2F2017%2F02%2F06%2FGitHub%E5%BC%80%E6%BA%90MySQL-Online-DDL%E5%B7%A5%E5%85%B7gh-ost%E5%AE%89%E8%A3%85%E6%96%87%E6%A1%A3%2F</url>
      <content type="text"><![CDATA[查看GitHub开源的MySQL在线DDL工具gh-ost官方文档，以及google一圈都没有发现gh-ost的安装文档，于是，还是自己动手，丰衣足食吧==Linux系统：Debian8.5 go版本：1.5 gh-ost版本：1.0.28 注：gh-ost是基于go1.5编译的。 go源码安装# 安装go依赖包sudo apt-get install bison ed gawk gcc libc6-dev make# 配置go环境变量，GOROOT为go源码目录，GOPATH为gh-ost这个工程的目录vim ~/.bashrcexport GOROOT=/usr/local/goexport PATH=$PATH:$GOROOT/binexport GOPATH=/usr/local/go/src/github.com/github/gh-ost# 使环境变量生效source ~/.bashrc# 获取+解压go源码安装包，go下载地址：https://golang.org/dl/wget https://storage.googleapis.com/golang/go1.5.linux-amd64.tar.gztar -zxvf go1.5.linux-amd64.tar.gz -C /usr/local/# 此时是go的安装目录为/usr/local/go# 验证go安装成功go env# 结果展示GOARCH=&quot;amd64&quot;GOBIN=&quot;&quot;GOEXE=&quot;&quot;GOHOSTARCH=&quot;amd64&quot;GOHOSTOS=&quot;linux&quot;GOOS=&quot;linux&quot;GOPATH=&quot;/usr/local/gh-ost/go&quot;GORACE=&quot;&quot;GOROOT=&quot;/usr/local/go&quot;GOTOOLDIR=&quot;/usr/local/go/pkg/tool/linux_amd64&quot;GO15VENDOREXPERIMENT=&quot;&quot;CC=&quot;gcc&quot;GOGCCFLAGS=&quot;-fPIC -m64 -pthread -fmessage-length=0&quot;CXX=&quot;g++&quot;CGO_ENABLED=&quot;1&quot; gh-ost源码安装# 在go安装目录下创建github.com/github/目录mkdir -p /usr/local/go/src/github.com/github# 别问我为什么要这样做，我这种go小白，只要安装出来就行，深入原因自己解决吧==# 获取+解压gh-ost源码安装包，gh-ost下载地址：https://github.com/github/gh-ost/releases/tag/v1.0.28wget https://codeload.github.com/github/gh-ost/tar.gz/v1.0.28unzip gh-ost-1.0.28.zip -d /usr/local/go/src/github.com/githubcd /usr/local/go/src/github.com/githubmv gh-ost-1.0.28 gh-ost# gh-ost源码安装cd /usr/local/go/src/github.com/github/gh-ost/bin/bash build.sh# 结果展示Building GNU/Linux binaryBinaries found in:/tmp/gh-ost/gh-ost-binary-linux-20161201195143.tar.gztar -zxvf /tmp/gh-ost/gh-ost-binary-linux-20161201195143.tar.gz -C /usr/localln -s /usr/local/gh-ost /usr/bin/gh-ost# 验证gh-ost安装成功gh-ost -version1.0.28gh-ost --help# 结果会输出一堆参数，gh-ost参数待以后详解]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[bzip2、pbzip2压缩工具比较]]></title>
      <url>%2F2017%2F02%2F03%2Fbzip2%E3%80%81pbzip2%E5%8E%8B%E7%BC%A9%E5%B7%A5%E5%85%B7%E6%AF%94%E8%BE%83%2F</url>
      <content type="text"><![CDATA[bzip2和pbzip2的性能差别到底有多大呢？让我们一起来围观一下吧～Linux版本：Debian8.5 pbzip2安装：apt-get install pbzip2 pbzip2详情请见：pbzip2的安装与使用 bzip2(单线程压缩工具)# 压缩单个文件测试# 单个文件大小root@wing:/data # du -h 2016.sql3.4G 2016.sql# tar bzip2 压缩命令time tar -jcf 2016.sql.bz2 2016.sql# 单个文件压缩时间real 10m7.996suser 10m4.632ssys 0m13.276s# 压缩后文件大小root@wing:/data # du -sh 2016.sql.bz2220M 2016.sql.bz2# 压缩目录测试# 目录文件大小root@wing:/data # du -sh 20161122/6.9G 20161122/# tar bzip 只能使用一个核进行压缩time tar -jcvf 20161122_bzip.bz2 20161122/*# 目录压缩时间real 24m30.013suser 22m51.936ssys 0m23.872s# 压缩后文件大小root@wing:/data # du -h 20161122.bz2356M 20161122.bz2 pbzip2(多线程压缩工具)# 压缩单个文件测试# 单个文件大小root@wing:/data # du -h 2016.sql3.4G 2016.sql# pbzip2压缩命令time pbzip2 -p3 -k 2016.sql # 单个文件压缩时间real 3m22.909suser 9m55.092ssys 0m16.284s# 压缩后文件大小root@wing:/data # du -sh 2016.pbzip.bz2221M 2016.pbzip.bz2# 压缩目录测试# 目录文件大小root@wing:/data # du -sh 20161122/6.9G 20161122/# tar bzip pbzip 使用3个核进行压缩time tar -c 20161122 | pbzip2 -p3 -c &gt; 20161122.tar.bz2# 目录压缩时间real 7m31.688suser 22m5.736ssys 0m42.520s# 压缩后文件大小root@wing:/data # du -h 20161122.tar.bz2358M 20161122.tar.bz2 总结： bzip pbzip（3个线程） 原文件大小 3.4G 3.4G 文件压缩时间( real) 10m7.996s 3m22.909s 文件压缩大小 220M 221M 原目录大小 6.9G 6.9G 目录压缩时间(real) 24m30.013s 7m31.688s 目录压缩大小 356M 358M 注意：压缩时间使用real计算，而不使用user+sys计算的原因是，多线程下user的时间是每个线程时间之和，与我们可以感知到的时间偏差较大，所以选择real，该服务器上都是初始化的job，所以real更接近用户感知的时间。 从上面表格可以得出，pbzip2开启3个线程压缩的前提下，无论是压缩单个文件还是压缩目录，时间上比单线程bzip2压缩快了接近3倍，而压缩比也基本相同。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[pbzip2安装及使用]]></title>
      <url>%2F2017%2F02%2F03%2Fpbzip2%E5%AE%89%E8%A3%85%E5%8F%8A%E4%BD%BF%E7%94%A8%2F</url>
      <content type="text"><![CDATA[平时大文件的压缩喜欢使用bzip2，虽然bzip2的压缩率很高，但是压缩时长实在无法忍受，于是，通过强大的Google找到了pbzip2这款工具，pbzip2作为多线程版本的bzip2工具，压缩比和bzip2基本相当，但是压缩时间比bzip2减少了线程数倍数，毕竟bzip2是单线程工作，pbzip2是多线程工作。详情请见：bzip2与pbzip2压缩工具比较 Linux版本：Debian8.5 pbzip2安装apt-get install pbzip2 pbzip2参数详解 Usage: pbzip2 [-1 .. -9][-b#cdfhklm#p#qrS#tVz] \ \ \ -1…-9 设置BWT(一种压缩技术算法)的block大小为100k…900k(默认为900k) -b# block大小，单位是100k（默认9=900k） -c,–stdout 输出到stdout -d,–decompress 解压文件 -f,–force 覆盖已经存在的输出文件 -h,–help 输出帮助信息 -k,–keep 保留被压缩的文件(默认删除被压缩文件)，这里是个大坑，所以使用pbzip2压缩时，切记一定要携带-k参数 -l,–loadavg 由load average(平均负载)决定使用CPU的最大数量 -m# 最大内存使用量，单位：1MB(默认 100=100MB) -p# 指定CPU数，即线程数(默认自动检测，检测失败后为2) -q,–quiet 静默模式 -r,–read 读取整个文件进入内存，并在各个CPU分开处理 -S#子线程的stack(堆栈)大小，单位：1KB -t,–test完整的测试压缩文件 -v,–verbose详细信息模式 -V,–version 输出pbzip2的版本信息 -z,–compress 压缩文件(默认值) –ignore-trailing-garbage=#是否忽略文件末尾对齐数据块(1忽略，0禁止) pbzip2常用示例 压缩单个文件（指定3个线程） pbzip2 test.sql -z -p3 -k &gt; test.sql.bz2 压缩目录（指定3个线程） tar -c test_dir/* | pbzip2 -c -p3 -k &gt; test_dir.tar.bz2 解压文件（指定3个线程） pbzip2 -d -p3 -k test.sql.bz2 解压目录（指定3个线程） pbzip2 -d -p3 -k test_dir.tar.bz2tar -xf test_dir.tar# 或者pbzip2 -d -p3 -k test_dir.tar.bz2 &amp;&amp; tar -xf test_dir.tar pbzip2限制由于pbzip2只能压缩文件，不能对目录进行压缩，所以如果想使用pbzip2压缩目录，则需要借助tar工具。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux那些控制进程的命令]]></title>
      <url>%2F2017%2F01%2F31%2FLinux%E9%82%A3%E4%BA%9B%E6%8E%A7%E5%88%B6%E8%BF%9B%E7%A8%8B%E7%9A%84%E5%91%BD%E4%BB%A4%2F</url>
      <content type="text"><![CDATA[Linux中有哪些控制进程的命令呢，我们来看看先～ 操作系统：Debian8.5 注意：下述中的n，代表的是jobs中的序号 查看Linux中有哪些在后台运行的进行命令：jobs root@wing-01:~ # jobs[1]+ Stopped python backup_mysql.py wing 1.2.3.4 8888 wing[2]- Running python backup_mysql.py wing 1.2.3.5 8888 wing &amp;root@wing-01:~ # 让进程后台运行：cmd &amp; root@wing-01:~ # python backup_mysql.py wing 1.2.3.5 8888 wing &amp;[2] 27716root@wing-01:~ # 让后台进程n到前台运行: fg %n root@wing-01:~ # jobs[1]+ Stopped python backup_mysql.py wing 1.2.3.4 8888 wing[2]- Running python backup_mysql.py wing 1.2.3.5 8888 wing &amp;root@wing-01:~ # fg %2python backup_mysql.py wing 1.2.3.5 8888 wing 让前台n到后台运行：bg %n # 该命令适用于通过ctrl-z暂停的进程# 如下面job中的job1是通过ctrl-z暂停的，该程序将不在占用CPU，暂停执行，可以通过top查看其占用CPU率为0%，此时不仅可以通过fg %n让其前台继续运行，也可以通过bg %n让其后台继续运行root@wing-01:~ # jobs[1]+ Stopped python backup_mysql.py wing 1.2.3.4 8888 wing[2]- Running python backup_mysql.py wing 1.2.3.5 8888 wing &amp;root@wing-01:~ # bg %1[1]+ python backup_mysql.py wing 1.2.3.4 8888 wing &amp;root@wing-01:~ ## 此时再通过top,发现它开始占用CPU,CPU使用率不在为0，说明其已经在后台运行了。 暂停当前程序运行：ctrl-z 此时程序是处于不适用CPU执行任何任务状态，即程序是暂停的状态，等待使用其他命令将其唤醒. root@wing-01:~ # python backup_mysql.py yumin 172.16.33.227 3333 yumin platform test^Z[1]+ Stopped python backup_mysql.py yumin 172.16.33.227 3333 yumin platform testroot@wing-01:~ # 通过PID将程序暂停：kill -STOP pid root@wing-01:~ # kill -STOP 28021[1]+ Stopped python backup_mysql.py yumin 172.16.33.227 3333 yuminroot@wing-01:~ ## 此时可以通过top查看其占用CPU率为0%，即进程已经停止。 通过PID将程序恢复到后台运行：kill -CONT pid root@wing-01:~ # kill -CONT 28021root@wing-01:~ ## 此时再通过top,发现它开始占用CPU,CPU使用率不在为0，说明其已经在后台运行了。 ​ 本文参考文档请点击此处]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux文本处理工具之sort命令]]></title>
      <url>%2F2017%2F01%2F29%2FLinux%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E5%B7%A5%E5%85%B7%E4%B9%8Bsort%E5%91%BD%E4%BB%A4%2F</url>
      <content type="text"><![CDATA[sort命令将每一行作为一个单位进行比较，比较原则是从首字符向后，依次按ASCII码值进行比较，最后将他们按一定的顺序进行输出。 一、sort实战演练 sort.txt root,x,0,0,root,/root,/bin/bashdaemon,x,1,1,daemon,/usr/sbin,/usr/sbin/nologinbin,x,2,2,bin,/bin,/usr/sbin/nologinbin,x,2,2,bin,/bin,/usr/sbin/nologinsys,x,3,3,sys,/dev,/usr/sbin/nologin number.txt 9890822 human_readable.txt 2k2G2M20M 实战演练 输出升序排序文件 &gt; root@wing:~/wing # sort sort.txt&gt; bin,x,2,2,bin,/bin,/usr/sbin/nologin&gt; bin,x,2,2,bin,/bin,/usr/sbin/nologin&gt; daemon,x,1,1,daemon,/usr/sbin,/usr/sbin/nologin&gt; root,x,0,0,root,/root,/bin/bash&gt; sys,x,3,3,sys,/dev,/usr/sbin/nologin&gt; &gt; 输出降序排序文件 &gt; root@wing:~/wing # sort -r sort.txt&gt; sys,x,3,3,sys,/dev,/usr/sbin/nologin&gt; root,x,0,0,root,/root,/bin/bash&gt; daemon,x,1,1,daemon,/usr/sbin,/usr/sbin/nologin&gt; bin,x,2,2,bin,/bin,/usr/sbin/nologin&gt; bin,x,2,2,bin,/bin,/usr/sbin/nologin&gt; &gt; 将sort文件按照第三列升序输出 &gt; root@wing:~/wing # sort -t , -k 3 sort.txt&gt; root,x,0,0,root,/root,/bin/bash&gt; daemon,x,1,1,daemon,/usr/sbin,/usr/sbin/nologin&gt; bin,x,2,2,bin,/bin,/usr/sbin/nologin&gt; bin,x,2,2,bin,/bin,/usr/sbin/nologin&gt; sys,x,3,3,sys,/dev,/usr/sbin/nologin&gt; &gt; 去掉重复行降序输出 &gt; root@wing:~/wing # sort -ur sort.txt&gt; sys,x,3,3,sys,/dev,/usr/sbin/nologin&gt; root,x,0,0,root,/root,/bin/bash&gt; daemon,x,1,1,daemon,/usr/sbin,/usr/sbin/nologin&gt; bin,x,2,2,bin,/bin,/usr/sbin/nologin&gt; 二、sort常用参数 -b, –ignore-leading-blanks 忽略每一行开头的空格，从第一个不是空格的字符开始比较。 -c, –check, –check=diagnose-first 检查文件是否已经排序，如果没有排序，则输出第一个未排序的行，如果已经排序，则返回1. &gt;root@wing:~/wing # sort -c sort.txt&gt;sort: sort.txt:2: disorder: daemon,x,1,1,daemon,/usr/sbin,/usr/sbin/nologin&gt; -C, –check=quiet, –check=silent 与-c参数功能一致，不同的是，如果没有排序，该参数并不会输出第一个未排序的行，二是什么都不会输出。 &gt; root@wing:~/wing # sort -c sort.txt&gt; sort: sort.txt:2: disorder: daemon,x,1,1,daemon,/usr/sbin,/usr/sbin/nologin&gt; root@wing:~/wing #&gt; -f, –ignore-case 忽略大小写，将所有的小写字母转换为大写字母进行比较。 -h, –human-numeric-sort 以人类可读的方式排序，如对k,G进行排序。 &gt; root@wing:~/wing # sort human_readable.txt&gt; 20M&gt; 2G&gt; 2k&gt; 2M&gt; root@wing:~/wing # sort -h human_readable.txt&gt; 2k&gt; 2M&gt; 20M&gt; 2G&gt; -M, –month-sort 按月份排序，如JAN,MAR等等。 -n, –numeric-sort 将数字转换为数值的方式排序。 &gt; root@wing:~/wing # sort number.txt&gt; 2&gt; 8&gt; 82&gt; 9&gt; 90&gt; root@wing:~/wing # sort -n number.txt&gt; 2&gt; 8&gt; 9&gt; 82&gt; 90&gt; -R, –random-sort 以随机的方式进行排序。 &gt; root@wing:~/wing # sort -R sort.txt&gt; daemon,x,1,1,daemon,/usr/sbin,/usr/sbin/nologin&gt; sys,x,3,3,sys,/dev,/usr/sbin/nologin&gt; root,x,0,0,root,/root,/bin/bash&gt; bin,x,2,2,bin,/bin,/usr/sbin/nologin&gt; bin,x,2,2,bin,/bin,/usr/sbin/nologin&gt;&gt; root@wing:~/wing # sort -R sort.txt&gt; root,x,0,0,root,/root,/bin/bash&gt; sys,x,3,3,sys,/dev,/usr/sbin/nologin&gt; daemon,x,1,1,daemon,/usr/sbin,/usr/sbin/nologin&gt; bin,x,2,2,bin,/bin,/usr/sbin/nologin&gt; bin,x,2,2,bin,/bin,/usr/sbin/nologin&gt;&gt; root@wing:~/wing # sort -R sort.txt&gt; root,x,0,0,root,/root,/bin/bash&gt; bin,x,2,2,bin,/bin,/usr/sbin/nologin&gt; bin,x,2,2,bin,/bin,/usr/sbin/nologin&gt; daemon,x,1,1,daemon,/usr/sbin,/usr/sbin/nologin&gt; sys,x,3,3,sys,/dev,/usr/sbin/nologin&gt; -r, –reverse 默认是升序排序，加上-r参数是降序排序。 -o, –output=FILE 将sort命令的结果输出到另一个文件中。 -u, –unique 和-c参数一起时，并没有什么用; 不与-c参数一起时，将所有的行去重后排序输出。 -t, –field-separator 分隔符。 -k, –key 根据key去排序，可以是列的位置或者类型。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux文本处理工具之cut命令]]></title>
      <url>%2F2017%2F01%2F29%2FLinux%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E5%B7%A5%E5%85%B7%E4%B9%8Bcut%E5%91%BD%E4%BB%A4%2F</url>
      <content type="text"><![CDATA[cut命令指定输出一行中的选取部分。 一、cut实战演练 cut.txt文件 root,x,0,0,root,/root,/bin/bashdaemon,x,1,1,daemon,/usr/sbin,/usr/sbin/nologinbin,x,2,2,bin,/bin,/usr/sbin/nologinsys,x,3,3,sys,/dev,/usr/sbin/nologin 实战演练 输出每一行的第二个字节内容 &gt; root@wing:~/wing # cut -b 2 cut.txt&gt; o&gt; a&gt; i&gt; y&gt; &gt; 输出每一行的第二个字符内容 &gt; root@wing:~/wing # cut -c 2 cut.txt&gt; o&gt; a&gt; i&gt; y&gt; &gt; 输出每一行第一列内容 &gt; root@wing:~/wing # cut -d , -f 1 cut.txt&gt; root&gt; daemon&gt; bin&gt; sys&gt; &gt; 输出每一行第一、二列内容 &gt; root@wing:~/wing # cut -d , -f 1,2 cut.txt&gt; root,x&gt; daemon,x&gt; bin,x&gt; sys,x&gt; 二、cut常用参数详解-b,–bytes 选取字节的列表，即选取每行的第N个字节。 -c,–characters 选取字符的列表，即选取每个的第N个字符。(英文字符下与-b没有区别，中文字符下，一个中文占据2-3个字节，所以存在中文的时候更倾向于用-c)。 -d,–delimiter 分隔符，默认为TAB。 -f,–field 选取列的列表，即选取每行的第N列。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux监视进程之ps命令]]></title>
      <url>%2F2017%2F01%2F13%2FLinux%E7%9B%91%E8%A7%86%E8%BF%9B%E7%A8%8B%E4%B9%8Bps%E5%91%BD%E4%BB%A4%2F</url>
      <content type="text"><![CDATA[ps命令日常工作中最常用的命令行工具之一了，每天敲了那么多次的ps，你真的了解你的命令行输出信息中的每一行含义吗？ 本文参考《UNIX/Linux系统管理技术手册》。 操作系统：Debian8.5 仅仅以下常用的三种组合命令的解释，更加齐全的命令请自行ps --help all 查看。 一、ps -ef输出展示： UID PID PPID C STIME TTY TIME CMDroot 1 0 0 2016 ? 00:00:28 /lib/systemd/systemd --system --deserialize 15root 2 0 0 2016 ? 00:00:00 [kthreadd]root 3 2 0 2016 ? 00:00:25 [ksoftirqd/0]root 5 2 0 2016 ? 00:00:00 [kworker/0:0H]root 7 2 0 2016 ? 00:04:57 [rcu_sched]root 8 2 0 2016 ? 00:00:00 [rcu_bh]...... 输出字段说明： UID: 进程所属的用户。 PID: 进程的ID。 PPID: 父进程的PID。 C: CPU的使用/调度信息。 STIME: 启动进程的时间。 TTY: 控制终端。 TIME: 消耗CPU的时间。 CMD: 进程执行的命令行。 二、ps aux输出展示： USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1 0.0 0.0 176040 3272 ? Ss 2016 0:28 /lib/systemd/systemd --system --deserialize 15root 2 0.0 0.0 0 0 ? S 2016 0:00 [kthreadd]root 3 0.0 0.0 0 0 ? S 2016 0:25 [ksoftirqd/0]root 5 0.0 0.0 0 0 ? S&lt; 2016 0:00 [kworker/0:0H]root 7 0.0 0.0 0 0 ? S 2016 4:57 [rcu_sched]root 8 0.0 0.0 0 0 ? S 2016 0:00 [rcu_bh]...... 输出字段说明： USER: 进程的所属用户。 PID: 进程的ID。 %CPU: 该进程正在使用的CPU百分比。 %MEM: 该进程正在使用的内存百分比。 VSZ: 进程的虚拟大小。 RSS: 内存中页的数量。 TTY: 控制终端。 STAT: 当前进程状态。 ​ R=可运行；D=不可中断的休眠状态(如正在等待磁盘)；S=休眠状态(Sleep)；T=被跟踪或者被停止(Stop)；Z=僵尸进程(Zombie)； ​ 附加标志： ​ W=进程被交换出去(Progress is swapping out)；&lt;=进程优先级高于普通优先级；N=进程优先级低于普通优先级；L=有些页面被锁在内存中；s=进程是会话的先导进程(Process is a session leader) START: 进程开始时间。 TIME: 进程已经消耗掉的CPU时间。 COMMAND: 进程执行的命令行。 三、ps laxps lax运行速度比ps aux快，因为它不必将每个UID转换成用户名。 输出展示： F UID PID PPID PRI NI VSZ RSS WCHAN STAT TTY TIME COMMAND4 0 1 0 20 0 176040 3272 - Ss ? 0:28 /lib/systemd/systemd --system --deserialize 151 0 2 0 20 0 0 0 - S ? 0:00 [kthreadd]1 0 3 2 20 0 0 0 - S ? 0:25 [ksoftirqd/0]1 0 5 2 0 -20 0 0 - S&lt; ? 0:00 [kworker/0:0H]1 0 7 2 20 0 0 0 - S ? 4:58 [rcu_sched]1 0 8 2 20 0 0 0 - S ? 0:00 [rcu_bh] 输出字段说明： F: 进程标志。 UID: 进程所属用户的ID。 PID: 进程ID。 PPID: 父进程ID。 PRI: 进程的优先级，值越小代表优先级越高。 NI: 进程的谦和度，也可以理解为进程的优先级，值越小代表优先级越高。 VSZ: 进程的虚拟大小。 RSS: 内存中页的数量。 WCHAN: 进程正在等待的对象地址。 STAT: 当前进程状态。 ​ R=可运行；D=不可中断的休眠状态(如正在等待磁盘)；S=休眠状态(Sleep)；T=被跟踪或者被停止(Stop)；Z=僵尸进程(Zombie)； ​ 附加标志： ​ W=进程被交换出去(Progress is swapping out)；&lt;=进程优先级高于普通优先级；N=进程优先级低于普通优先级；L=有些页面被锁在内存中；s=进程是会话的先导进程(Process is a session leader) TTY: 控制终端。 TIME: 进程已经消耗掉的CPU时间。 COMMAND: 进程执行的命令行。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CentOS7下源码安装MySQL5.7.6+]]></title>
      <url>%2F2016%2F12%2F12%2FCentOS7%E4%B8%8B%E6%BA%90%E7%A0%81%E5%AE%89%E8%A3%85MySQL5-7-6%2F</url>
      <content type="text"><![CDATA[MySQL5.7.6+以后的安装方式真的与以前版本的MySQL安装方式大大的不同呀。。。不自己安装一把，你都不知道不同之处在哪。。。 Linux版本：Centos7 MySQL版本：MySQL5.7.16 该文档适用于MySQL版本&gt;=5.7.6 卸载CentOS7默认携带的mariadb包 # 检查mariadb安装包[root@wing ~]# rpm -qa | grep -i mysql[root@wing ~]# rpm -qa | grep -i mariadbmariadb-libs-5.5.50-1.el7_2.x86_64# 卸载mariadb安装包[root@wing ~]# rpm -e mariadb-libs-5.5.50-1.el7_2.x86_64error: Failed dependencies: libmysqlclient.so.18()(64bit) is needed by (installed) postfix-2:2.10.1-6.el7.x86_64 libmysqlclient.so.18(libmysqlclient_18)(64bit) is needed by (installed) postfix-2:2.10.1-6.el7.x86_64[root@wing ~]# rpm -e mariadb-libs-5.5.50-1.el7_2.x86_64 postfix-2:2.10.1-6.el7.x86_64 获得MySQL所有版本(5.0.15-latest)地址传送门http://downloads.mysql.com/archives/community/ 安装编译软件 yum install -y cmake make gcc gcc-c++ 创建MySQL安装目录 # 如MySQL安装目录为：/usr/local/mysqlmkdir -p /usr/local/mysql 解压MySQL源码包 tar -zxvf mysql-5.7.16.tar.gz 创建mysql用户和用户组 # 创建用户组groupadd mysql# 创建mysql用户，所属组为mysqluseradd -s /bin/bash -m -g mysql mysql 安装MySQL相关依赖包 yum install -y ncurses-devel openssl-devel bison-devel libaio libaio-devel boost库安装 # 该步骤可以省略，在cmake阶段添加参数-DDOWNLOAD_BOOST=1 -DWITH_BOOST=/usr/local/boost即可# boost库安装wget http://sourceforge.net/projects/boost/files/boost/1.59.0/boost_1_59_0.tar.gztar -zxvf boost_1_59_0.tar.gz -C /usr/localmv /usr/local/boost_1_59_0 /usr/local/boostcd /usr/local/boost./bootstrap.sh./b2 stage threading=multi link=shared./b2 install threading=multi link=shared 创建MySQL相关目录 | 目录 | 含义 | 配置参数 || :——— | ————————————– | —————————————- || bin_log | 二进制日志目录 | log_bin_basenamelog_bin_index || mydata | 数据文件目录 | datadir || innodb_log | InnoDB重做日志目录 | innodb_log_group_home_dir || innodb_ts | InnoDB共享表空间目录 | innodb_data_home_dir || log | 日志文件目录(error log+general log+slow log) | log_errorgeneral_log_fileslow_query_log_file || relay_log | InnoDB中继日志目录 | relay_log_basenamerelay_log_index || tmpdir | 临时文件目录 | tmpdir || undo_log | InnoDB回滚日志目录 | innodb_undo_directory | mkdir -p /data/mysql/mysql3306/bin_logmkdir -p /data/mysql/mysql3306/db_filemkdir -p /data/mysql/mysql3306/innodb_logmkdir -p /data/mysql/mysql3306/innodb_tsmkdir -p /data/mysql/mysql3306/logmkdir -p /data/mysql/mysql3306/relay_logmkdir -p /data/mysql/mysql3306/tmpdirmkdir -p /data/mysql/mysql3306/undo_log 修改步骤9创建的目录的所属用户与所属组为mysql:mysql chown -R mysql:mysql /data/mysql/mysql3306 将MySQL配置文件my.cnf放置到/etc目录下 默认情况下，MySQL会依次按顺序查找如下几个路径来获取MySQL配置问文件： /etc/my.cnf /etc/mysql/my.cnf /etc/my.cnf/my.cnf /usr/local/mysql/my.cnf ~/.my.cnf 使用过程中可通过–defaults-file=xxx来指定配置文件。 # 修改MySQL配置文件所属用户与所属组chown -R mysql:mysql my.cnf 编译安装MySQL5.7.6+ # 切换到mysql-5.7.16源码目录下cd /path/mysql-5.7.16# cmakecmake -DCMAKE_BUILD_TYPE=RelWithDebInfo -DCMAKE_INSTALL_PREFIX=/usr/local/mysql -DMYSQL_DATADIR=/data/mysql/mysql3306/mydata -DSYSCONFDIR=/etc/my.cnf -DWITH_INNOBASE_STORAGE_ENGINE=1 -DWITH_PARTITION_STORAGE_ENGINE=1 -DDEFAULT_CHARSET=utf8 -DDEFAULT_COLLATION=utf8_general_ci -DENABLE_DEBUG_SYNC=0 -DENABLED_LOCAL_INFILE=1 -DENABLED_PROFILING=1 -DMYSQL_TCP_PORT=3306 -DMYSQL_UNIX_ADDR=/data/mysql/mysql3306/tmpdir/my-3306.sock -DWITH_DEBUG=0 -DWITH_SSL=yes -DWITH_SAFEMALLOC=OFF -DDOWNLOAD_BOOST=1 -DWITH_BOOST=/usr/local/boost # make # 该命令中可以通过添加-j参数指定多线程工作，如make -j2 &amp;&amp; make install -j2 则使用2个CPU核进行make# 该步骤执行完毕后，可以到CMAKE_INSTALL_PREFIX参数指定的目录下，即MySQL安装目录下查看到mysql相关目录与文件make &amp;&amp; make install# 修改MySQL安装目录的所属用户与用户组为mysql:mysqlchown -R mysql:mysql /usr/local/mysql 初始化MySQL ​```shell # 进入到MySQL安装目录下 cd /usr/local/mysql # 初始化MySQL，切记--defaults-file=/etc/my.cnf要放在参数的第一位，初始化信息可以在MySQL的errorlog中查看，并且在errorlog会生成一个root的随机密码，该随机密码仅仅为root@localhost用户所有。 mysqld --defaults-file=/etc/my.cnf --initialize --basedir=/usr/local/mysql --datadir=/data/mysql/mysql3306/mydata --user=mysql​ 1. 添加MySQL环境变量 ​ vim /etc/profile # 在~/.bashrc文件下添加如下语句 export MYSQL_HOME=/usr/local/mysql export PATH=$&#123;MYSQL_HOME&#125;/bin:$PATH # 保存后，使环境变量生效 source /etc/profile​ 1. 启动MySQL ​mysqld_safe --defaults-file=/etc/my.cnf &amp;# 此时可以通过ps -ef | grep mysql看到相关进程​ 1. 登陆MySQL ​mysql -uroot -S /data/mysql/mysql3306/tmpdir/mysql.sock -p# 输入errorlog中生成的随机密码，即可登陆MySQL# 登陆mysql需要修改root密码，否则会出现下列情况：root@localhost : (none) 11:16:52&gt; show databases;ERROR 1820 (HY000): You must reset your password using ALTER USER statement before executing this statement.ERROR 1820 (HY000): You must reset your password using ALTER USER statement before executing this statement.ERROR 1820 (HY000): You must reset your password using ALTER USER statement before executing this statement.# 修改root密码set password=&apos;MYSQL&apos;;# 目前版本可以使用直接的字符串代替以前password(&apos;xxx&apos;)的加密方式，目前版本提示如下：root@localhost : (none) 11:16:54&gt; set password=password(&apos;MYSQL&apos;);Query OK, 0 rows affected, 1 warning (0.00 sec)Warning (Code 1287): &apos;SET PASSWORD = PASSWORD(&apos;&lt;plaintext_password&gt;&apos;)&apos; is deprecated and will be removed in a future release. Please use SET PASSWORD = &apos;&lt;plaintext_password&gt;&apos; insteadroot@localhost : (none) 11:19:27&gt; set password=&apos;MYSQL&apos;;​ 1. 关闭MySQL ​mysqladmin shutdown -uroot -S /data/mysql/mysql3306/tmpdir/mysql.sock -p# 使用新密码​ 1. 初始化的MySQL5.7.6+与MySQL5.6.xx不同之处 初始化工具不同 MySQL5.6.xx使用的是mysql_install_db，MySQL5.7.6+官方推荐使用mysqld –initialize。 初始化数据库不同 MySQL5.6.xx初始化之后存在mysql,information_schema,performance_schema,test四个数据库，MySQL5.7.6+初始化之后存在mysql,information_schema,performance_schema,sys四个数据库。 初始化用户不同 MySQL5.6.xx初始化之后存在root@localhost,root@’::1’,root@’hostname’,’’@’localhost’,’’@’hostname’五个用户，MySQL5.7.6+初始化之后存在mysql.sys,root@localhost用户 初始化root密码 MySQL5.6.xx初始化之后root用户密码为空，MySQL5.7.6+初始化之后会为root@localhost用户生成随机密码。```]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Mariadb安装Tokudb引擎]]></title>
      <url>%2F2016%2F11%2F02%2FMariadb%E5%AE%89%E8%A3%85Tokudb%E5%BC%95%E6%93%8E%2F</url>
      <content type="text"><![CDATA[zabbix数据库数据量之大真的让人非常非常的困扰，但是好在这个世界上有一种存储引擎叫做tokudb，这个tokudb压缩性能真的是好的不要不要的，也是我第一次接触到除InnoDB和MyISAM之外的第三种存储引擎：tokudb。 以二进制安装mariadb为例（注意mariadb 有两种二进制安装包，安装tokudb必须是glibc2.14版本以上，比如mariadb二进制安装包长得像这样：mariadb-10.1.18-linux-glibc_214-x86_64.tar.gz） 操作系统：Debian 8.5 Mariadb：10.1.18（二进制版本） 如何确定操作系统glibc的版本为2.14以上 dd --version 二进制安装Mariadb参考官方文档：https://mariadb.com/kb/en/mariadb/installing-mariadb-binary-tarballs/ 安装tokudb # 安装依赖包apt-get install -y libjemalloc-dev#关闭操作系统HugePagecat /sys/kernel/mm/transparent_hugepage/enabled-- 显示always madvise [never]，则HugePage已经关闭# 否则关闭HugePage的方法echo never &gt; /sys/kernel/mm/transparent_hugepage/enabledecho never &gt; /sys/kernel/mm/transparent_hugepage/defrag# 启动MySQL后，在MySQLINSTALL SONAME 'ha_tokudb';# 卸载tokudbUNINSTALL SONAME 'ha_tokudb'; 配置tokudb参数 # 注意Tokudb的参数在mysql_install_db阶段，不能配置到my.cnf文件中，否则在数据库初始化阶段会报错# 在INSTALL SONAME &apos;ha_tokudb&apos;;之后，关闭数据库，在my.cnf文件中写上需要配置的参数，例如：tokudb-data-dir = /data/mysql/3306/tokudb_datatokudb-log-dir = /data/mysql/3306/tokudb_logtokudb_row_format = tokudb_fasttokudb_cache_size = 64Gtokudb_commit_sync = 0tokudb_fsync_log_period = 5000tokudb_directio = 1tokudb_read_block_size = 128Ktokudb_read_buf_size = 128K# 然后将INSTALL SONAME &apos;ha_tokudb&apos;;生成的部分文件拷贝到对应的tokudb目录中去# tokudb-data-dir中应有的文件：__tokudb_lock_dont_delete_me_data__tokudb_lock_dont_delete_me_temp# tokudb-log-dir应有的文件:log000000000001.tokulog29__tokudb_lock_dont_delete_me_logs__tokudb_lock_dont_delete_me_recovery# 此时重新启动MySQL，则相应的配置参数即可生效。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Mac终端加密压缩文件]]></title>
      <url>%2F2016%2F10%2F25%2FMac%E7%BB%88%E7%AB%AF%E5%8A%A0%E5%AF%86%E5%8E%8B%E7%BC%A9%E6%96%87%E4%BB%B6%2F</url>
      <content type="text"><![CDATA[今天发现了在Mac如何给压缩文件加密，于是记录下来，留待后续需要的时候使用。 命令zip -e test.zip test1.txt test2.txt test3.txt 解释 -e 为加密参数，添加该参数后，会有输入密码的提示。 test.zip为压缩后的文件名称。 test1.txt test2.txt test3.txt为需要添加的压缩文件，即把这几个文件压缩起来。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[mydumper多线程备份工具]]></title>
      <url>%2F2016%2F10%2F18%2Fmydumper%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%A4%87%E4%BB%BD%E5%B7%A5%E5%85%B7%2F</url>
      <content type="text"><![CDATA[最近在迁移zabbix数据库，500G+的数据，使用mysqldump备份花了3H+，于是想到mydumper多线程备份，备份时间缩短至一半，于是对mydumper多线程备份工具顿生好感。 一、mydumper安装apt-get install libglib2.0-dev libmysqlclient15-dev zlib1g-dev libpcre3-dev libssl-devcd /usr/localwget https://launchpadlibrarian.net/225370879/mydumper-0.9.1.tar.gztar -zxvf mydumper-0.9.1.tar.gzcd mydumper-master/cmake .makemake install 二、mydumper使用# 数据库备份：mydumper -u username -h host -P port -p password -B database -o back_dir -t thread# 数据库恢复：myloader -u username -h host -P port -p passwd -B database -d back_dir -t thread 三、mydumper常用参数 -B,—database 备份的数据库。 -T,—table-list 备份的表，使用’,分割。 -o,—outputdir 备份文件的目录，这是一个目录。 -m,—no-schemas 不备份表结构。 -d,—no-data 不备份表数据。 -G,—triggers 备份触发器。 -E,—events 备份事件。 -R,—routines 备份存储过程和函数。 -W,—no-views 不备份视图。 -L,—logfile mydumper日志文件，默认是stdout。 -t,—threads 线程数，默认为4。 四、myloader常用参数 -d,—directory 恢复的备份目录 -o,—overwrites-tables 覆盖写，当表存在时则删除。 -B,—database 恢复的数据库名称。 -e,—enale-binlog 允许将备份数据记录binlog中。 -t,—threads 线程数，默认为4 五、注意事项 主上写入很大的时候，可将innodb_flush_log_at_trx_commit和sync_binlog为0。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[一个我认为是bug的UPDATE语句]]></title>
      <url>%2F2016%2F08%2F25%2F%E4%B8%80%E4%B8%AA%E6%88%91%E8%AE%A4%E4%B8%BA%E6%98%AFbug%E7%9A%84UPDATE%E8%AF%AD%E5%8F%A5%2F</url>
      <content type="text"><![CDATA[这个我认为的bug，反馈给MySQL官方，但是MySQL官方认为这并不是一个bug，并给出了解释，我认为这个解释是合理的，但是不可避免的是这条语句实在太危险了。 问题描述示例表结构与表数据： # 表结构mysql&gt; show create table t;+-------+---------------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+-------+---------------------------------------------------------------------------------------------------------------------------------------------------+| t | CREATE TABLE `t` ( `id` int(11) DEFAULT NULL, `c1` int(11) DEFAULT NULL, `c2` varchar(16) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8 |+-------+---------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.01 sec)# 表数据mysql&gt; select * from t;+------+------+------+| id | c1 | c2 |+------+------+------+| 1 | 1 | A || 2 | 2 | B || 3 | 3 | C |+------+------+------+3 rows in set (0.00 sec) 正常的UPDATE语句应该长这样子的，SET后接的并列字段分隔符为”逗号(,)”： mysql&gt; select * from t;+------+------+------+| id | c1 | c2 |+------+------+------+| 1 | 1 | A || 2 | 2 | B || 3 | 3 | C |+------+------+------+3 rows in set (0.00 sec)mysql&gt; update t set c1=11,c2='AA' where id=1;Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; select * from t;+------+------+------+| id | c1 | c2 |+------+------+------+| 1 | 11 | AA || 2 | 2 | B || 3 | 3 | C |+------+------+------+3 rows in set (0.00 sec) 不合理的UPDATE应该是什么样子的呢，是将SET后接的并列字段分隔符改为”AND”，注意这样写的话，MySQL并不会报错，还会执行成功，但是语义完全和”逗号”作为分隔符是两码事： # 先来个实验mysql&gt; select * from t;+------+------+------+| id | c1 | c2 |+------+------+------+| 1 | 1 | A || 2 | 2 | B || 3 | 3 | C |+------+------+------+3 rows in set (0.00 sec)mysql&gt; update t set c1=11 and c2='AA' where id=1;Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; select * from t;+------+------+------+| id | c1 | c2 |+------+------+------+| 1 | 0 | A || 2 | 2 | B || 3 | 3 | C |+------+------+------+3 rows in set (0.00 sec)# 实验走到这里，觉得这个update不是我想要的，但是这个c1咋变为0了呢，c2咋没变呢？# 再来个实验mysql&gt; select * from t;+------+------+------+| id | c1 | c2 |+------+------+------+| 1 | 1 | A || 2 | 2 | B || 3 | 3 | C |+------+------+------+3 rows in set (0.00 sec)mysql&gt; update t set c2='AA' and c1= 11 where id=1;Query OK, 1 row affected, 1 warning (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 1mysql&gt; select * from t;+------+------+------+| id | c1 | c2 |+------+------+------+| 1 | 1 | 0 || 2 | 2 | B || 3 | 3 | C |+------+------+------+3 rows in set (0.00 sec)# 咦？这里c2变了，咋变为了0呢？我明明是个字母啊，再怎么改也不能改成一个字符串型数字啊？c2咋依旧没变呢？ 问题原因原来在UPDATE … SET后接分隔符为”AND”的语句，由于AND的优先级较高，所以先处理“AND”，再处理“＝”，于是“＝”后面的值只有逻辑运算的结果true(1) / false(0)，直接上实验好了＝＝ # 第一个实验解析update t set c1=11 and c2='AA' where id=1;# 在这条语句中MySQL将c1=11 and c2='AA'解析成了c1=(11 and c2='AA'),而在这张表中(11 and c2='AA')是一个假语句(false),所以MySQL将c1值解析为c1=0mysql&gt; select 11 and c2='AA' from t where id=1;+----------------+| 11 and c2='AA' |+----------------+| 0 |+----------------+1 row in set (0.00 sec)# 所以最终出现了c1变为0，c2没有任何改变的现象# 第二个实验解析update t set c2='AA' and c1= 11 where id=1;# 在这条语句中MySQL将c2='AA' and c1= 11解析成了c2=('AA' and c1= 11),而在这张表中('AA' and c1= 11)是一个假语句(false),所以MySQL将c2值解析为c2=0,然后隐式转换，将'0'存储到c2列mysql&gt; select 'AA' and c1= 11 from t where id=1;+-----------------+| 'AA' and c1= 11 |+-----------------+| 0 |+-----------------+1 row in set, 1 warning (0.01 sec)# 所以最终出现了c2变为'0'，c1依旧没变 问题总结这个问题告诉我们，SQL语句一定要写规范了，执行SQL语句前一定要清楚这条SQL的运行逻辑，对于UPDATE／DELETE手动执行之前，一定要按照如下顺序来＝＝ begin;update / delete ...;select 校验数据;commit; --数据校验成功rollback; --数据校验失败 我投递的bug地址：https://bugs.mysql.com/bug.php?id=82647]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[pt-online-schema-change使用]]></title>
      <url>%2F2016%2F08%2F16%2Fpt-online-schema-change%E4%BD%BF%E7%94%A8%2F</url>
      <content type="text"><![CDATA[看了一遍pt-osc文档，也用了几天，并且会持续用下去，所以做了一些笔记，方便以后工作吧＝＝ 原理 根据原表结构创建一个新表； 按照pt-osc的alter语句修改新表； 将原表中的数据copy到新表中去； 将原表copy数据期间的数据更新应用到新表中去（通过触发器实现）； 将原表重命名，将新表重命名到原表名称，默认情况下删除原表。 # USAGEpt-online-schema-change --host=xxx.xx.xx.xx --port=xxxx --user=xxxx --ask-pass --charset=utf8 --alter &quot;add column id int not null default 0&quot; D=wing,t=t --execute 优点执行Alter阶段不阻塞读和写。 限制 原表不能存在触发器，因为pt-osc需要通过触发器将原表copy数据阶段产生的数据应用到新表去。 表必须具有primary key/unique key，为了准确应用原表copy数据期间产生的数据到新表中去。通过primary key/unique key定位数据是否存在新表中。 原表不能是其他外键的父表，需要添加—alter-foreign-keys-method参数即可。 字段属性为NOT NULL时，必须有DEFAULT属性，否则会报错。 常用参数 —alter-foreign-keys-method如果原表为其他外键的父表，需要添加该参数。该参数存在如下几个方法：auto：自动选择rebuild_constraints/drop_swap方式；rebuild_constraints：使用alter table 删除字表的外键约束，在父表结束pt-osc操作后，再在表上创建新的约束（个人不推荐，特别是当子表很大的时候）;drop_swap：关闭外键检查约束，即FOREIGN_KEY_CHECKS=0(存在数据不一致的可能性)；none：会使foreign key失效，极度不推荐。 —dry-run创建以及修改新表，但是不创建触发器，数据复制以及重命名原表。 —execute执行alter语句。 —print输出pt-osc中copy data的详细过程。 —quiet静默方式输出，即不输出STDOUT信息，只输出STDERR信息。 —statistics输出pt-osc中copy data中INSERT执行的总次数。 PTDEBUG以debug的方式输出pt-osc的输出信息 PTDEBUG=1 pt-online-schema-change --host=xxx.xx.xx.xx --port=xxxx --user=xxxx --ask-pass --charset=utf8 --alter &quot;add column id int not null default 0&quot; D=wing,t=t --execute 使用pt-osc之前需要做的检查 原表是否存在触发器，存在：no，不存在：yes； 原表是否具有pk/uk，不存在：no,存在：yes； 原表是否为其他外键的父表，是：—alter-foreign-keys-method=’xxx’，不是：yes； 字段是否为NOT NULL 属性，不是：yes,是：DEFAULT；]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Debian8上源码安装MySQL5.6.19]]></title>
      <url>%2F2016%2F08%2F12%2FDebian8%E4%B8%8A%E6%BA%90%E7%A0%81%E5%AE%89%E8%A3%85MySQL5-6-19%2F</url>
      <content type="text"><![CDATA[最近换了份新工作，以前一直接触的是CentOS操作系统，并且是RPM安装MySQL的，现在新公司使用Debian系统，并用源码安装MySQL，所以赶紧将新知识纪录下来＝＝ 安装编译软件cmake,make # 安装cmakeapt-get install -y cmake# 安装makeapt-get install -y make 获得MySQL源码 http://dev.mysql.com/downloads/mysql/ 创建MySQL根目录 # 创建MySQL根目录mkdir -p /usr/local/mysql 解压MySQL源码 tar -zxvf mysql-5.6.19.tar.gz 创建mysql用户与用户组 # 创建用户组groupadd mysql# 创建mysql用户，所属组为mysqluseradd -s /bin/bash -m -g mysql mysql 安装MySQL依赖包 apt-get install -y libssl-dev libjemalloc-dev libncurses5-dev 创建MySQL相关目录 | 目录 | 含义 || :——— | ————- || bin_log | 二进制日志目录 || db_file | 数据文件目录 || innodb_log | InnoDB重做日志目录 || innodb_ts | InnoDB共享表空间目录 || log | 日志文件目录 || relay_log | InnoDB中继日志目录 || tmpdir | 临时文件目录 || undo_log | InnoDB回滚日志目录 | mkdir -p /data/mysql/3306/bin_logmkdir -p /data/mysql/3306/db_filemkdir -p /data/mysql/3306/innodb_logmkdir -p /data/mysql/3306/innodb_tsmkdir -p /data/mysql/3306/logmkdir -p /data/mysql/3306/relay_logmkdir -p /data/mysql/3306/tmpdirmkdir -p /data/mysql/3306/undo_log 修改步骤6创建的目录所属用户与组为mysql chown -R mysql:mysql /data/mysql/3306 将MySQL配置文件my-3306.cnf文件放置指定目录/etc # 将my.cnf文件放置指定目录mv ~/my.cnf /etc 编译安装MySQL5.6.19 # 切换到源码目录cd ~/mysql-5.6.19/# cmakecmake -DCMAKE_BUILD_TYPE=RelWithDebInfo -DCMAKE_INSTALL_PREFIX=/usr/local/mysql -DMYSQL_DATADIR=/data/mysql/3306/db_file -DSYSCONFDIR=/etc/my.cnf -DWITH_INNOBASE_STORAGE_ENGINE=1 -DWITH_FEDERATED_STORAGE_ENGINE=1 -DWITH_PARTITION_STORAGE_ENGINE=1 -DDEFAULT_CHARSET=utf8 -DDEFAULT_COLLATION=utf8_general_ci -DENABLE_DEBUG_SYNC=0 -DENABLED_LOCAL_INFILE=1 -DENABLED_PROFILING=1 -DMYSQL_TCP_PORT=3306 -DMYSQL_UNIX_ADDR=/data/mysql/3306/tmpdir/my-3306.sock -DWITH_DEBUG=0 -DWITH_SSL=yes -DCMAKE_EXE_LINKER_FLAGS=&quot;-ljemalloc&quot; -DWITH_SAFEMALLOC=OFF# makemake -j$&#123;cpu_core_num&#125;&amp;&amp;make install -j$&#123;cpu_core_num&#125;&amp;&amp;make clean#cpu_core_num = (grep &quot;processor&quot; /proc/cpuinfo|uniq|wc -l)］# 修改MySQL根目录的所属用户与组chown -R mysql:mysql /usr/local/mysql 初始化MySQL # 修改mysql_install_db脚本权限chmod 755 scripts/mysql_install_db# 初始化MySQLscripts/mysql_install_db --basedir=/usr/local/mysql --datadir=/data/mysql/3306/db_file --defaults-file=/etc/my-.cnf --force --skip-name-resolve --user=mysql 添加MySQL环境变量 vim ~/.bashrc# 在~/.bashrc文件下添加如下语句export MYSQL_HOME=/usr/local/mysqlexport PATH=$&#123;MYSQL_HOME&#125;/bin:$PATH# 保存后，使环境变量生效source ~/.bashrc 启动MySQL mysqld_safe --defaults-file=/etc/my.cnf &amp; 登陆MySQL mysql -uroot -S /data/mysql/3306/tmpdir/my-3306.sock -p 处理MySQL安全漏洞 # 删除匿名账户DELETE FROM mysql.user WHERE user=&apos;&apos;;# 更新root密码UPDATE mysql.user SET password=password(&apos;new_password&apos;) WHERE user=&apos;root&apos;;# 删除test数据库DROP DATABASE test；# 刷新权限FLUSH PRIVILEGES; 关闭MySQL mysqladmin shutdown -uroot -S /data/mysql/3306/tmpdir/my-3306.sock -p ​]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[手动制造GTID空洞]]></title>
      <url>%2F2016%2F07%2F13%2F%E6%89%8B%E5%8A%A8%E5%88%B6%E9%80%A0GTID%E7%A9%BA%E6%B4%9E%2F</url>
      <content type="text"><![CDATA[在网上看了下同名文章，但是宝宝没有实现。后来情急之下，通过宝宝的小领导一提醒，宝宝就实现了如何手动制造GTID空洞。 首先，要创建基于GTID的主从复制哒，该步骤省略。。 建立GTID同步，从库上show slave status\G可以看到如下结果： mysql&gt; show slave status\G*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 192.168.0.101 Master_User: repl Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000003 Read_Master_Log_Pos: 1167 Relay_Log_File: mysql-relay-log.000007 Relay_Log_Pos: 464 Relay_Master_Log_File: mysql-bin.000003 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 1167 Relay_Log_Space: 5364 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 330601 Master_UUID: 80544330-385c-11e6-8262-005056900269 Master_Info_File: /data/mysql/3333/db_file/master.info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for the slave I/O thread to update it Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: 80544330-385c-11e6-8262-005056900269:32:34-41 Executed_Gtid_Set: 80544330-385c-11e6-8262-005056900269:1-41 Auto_Position: 11 row in set (0.00 sec) 开始搞破坏啦，制作GTID空洞 mysql&gt; stop slave;Query OK, 0 rows affected (0.02 sec)mysql&gt; reset master;Query OK, 0 rows affected (0.08 sec)mysql&gt; show global variables like '%gtid%';+--------------------------+-------+| Variable_name | Value |+--------------------------+-------+| enforce_gtid_consistency | ON || gtid_executed | || gtid_mode | ON || gtid_owned | || gtid_purged | |+--------------------------+-------+5 rows in set (0.00 sec)mysql&gt; set global gtid_purged='80544330-385c-11e6-8262-005056900269:1-22:33-41';Query OK, 0 rows affected (0.05 sec)mysql&gt; start slave;Query OK, 0 rows affected (0.01 sec) 由于主库上我将22-33之间的事物的binlog都已经purge掉了，因为每次start slave;，基于GTID复制会去找断点的GTID，尝试进行修复，所以此时，我这边的show slave status\G会报如下错误： mysql&gt; show slave status\G*************************** 1. row *************************** Slave_IO_State: Master_Host: 192.168.0.101 Master_User: repl Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000003 Read_Master_Log_Pos: 1167 Relay_Log_File: mysql-relay-log.000007 Relay_Log_Pos: 464 Relay_Master_Log_File: mysql-bin.000003 Slave_IO_Running: No Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 1167 Relay_Log_Space: 5364 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: NULLMaster_SSL_Verify_Server_Cert: No Last_IO_Errno: 1236 Last_IO_Error: Got fatal error 1236 from master when reading data from binary log: 'The slave is connecting using CHANGE MASTER TO MASTER_AUTO_POSITION = 1, but the master has purged binary logs containing GTIDs that the slave requires.' Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 330601 Master_UUID: 80544330-385c-11e6-8262-005056900269 Master_Info_File: /data/mysql/3333/db_file/master.info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for the slave I/O thread to update it Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: 160713 19:51:33 Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: 80544330-385c-11e6-8262-005056900269:32:34-41 Executed_Gtid_Set: 80544330-385c-11e6-8262-005056900269:1-22:33-41 Auto_Position: 1 关于这个error又要如何修复呢？让我门一起挑战吧～ mysql&gt; stop slave;Query OK, 0 rows affected (0.00 sec)mysql&gt; reset master;Query OK, 0 rows affected (0.06 sec)mysql&gt; show global variables like '%gtid%';+--------------------------+-------+| Variable_name | Value |+--------------------------+-------+| enforce_gtid_consistency | ON || gtid_executed | || gtid_mode | ON || gtid_owned | || gtid_purged | |+--------------------------+-------+5 rows in set (0.00 sec)mysql&gt; set global gtid_purged='80544330-385c-11e6-8262-005056900269:1-41';Query OK, 0 rows affected (0.05 sec)mysql&gt; start slave;Query OK, 0 rows affected (0.01 sec)mysql&gt; show slave status\G*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 192.168.0.101 Master_User: repl Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000003 Read_Master_Log_Pos: 1167 Relay_Log_File: mysql-relay-log.000008 Relay_Log_Pos: 464 Relay_Master_Log_File: mysql-bin.000003 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 1167 Relay_Log_Space: 5881 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 330601 Master_UUID: 80544330-385c-11e6-8262-005056900269 Master_Info_File: /data/mysql/3333/db_file/master.info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for the slave I/O thread to update it Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: 80544330-385c-11e6-8262-005056900269:32:34-41 Executed_Gtid_Set: 80544330-385c-11e6-8262-005056900269:1-41 Auto_Position: 1 好啦，大功告成啦，那我们去主库执行一个事物，看看同步是否正常～ # master:mysql&gt; show tables -&gt; ;+--------------------+| Tables_in_bilibili |+--------------------+| tt |+--------------------+1 row in set (0.00 sec)mysql&gt; insert into tt values();Query OK, 1 row affected (0.02 sec)# slave:mysql&gt; show slave status\G*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 192.168.0.101 Master_User: repl Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000003 Read_Master_Log_Pos: 1411 Relay_Log_File: mysql-relay-log.000008 Relay_Log_Pos: 708 Relay_Master_Log_File: mysql-bin.000003 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 1411 Relay_Log_Space: 6125 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 330601 Master_UUID: 80544330-385c-11e6-8262-005056900269 Master_Info_File: /data/mysql/3333/db_file/master.info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for the slave I/O thread to update it Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: 80544330-385c-11e6-8262-005056900269:32:34-42 Executed_Gtid_Set: 80544330-385c-11e6-8262-005056900269:1-42 Auto_Position: 11 row in set (0.00 sec) 看完上面的步骤，是正常同步哦，在此我们就解决啦～]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[SSH免密码登录操作步骤]]></title>
      <url>%2F2016%2F07%2F13%2FSSH%E5%85%8D%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95%E6%93%8D%E4%BD%9C%E6%AD%A5%E9%AA%A4%2F</url>
      <content type="text"><![CDATA[每次SSH免密码登录都要去Google上搜索，其实也就是那么几条命令嘛。所以我决定最后一次Google怎么SSH免密码登录，然后记录下来。 原文链接： http://blog.csdn.net/leexide/article/details/17252369 准备机器server1：192.168.0.101 server2：192.168.0.102 server3：291.168.0.103 SSH目标server1可以通过SSH免密码登录的方式登录到server2和server3上。 SSH步骤 server1上使用ssh-keygen创建公钥 ssh-keygen -t rsa -N &quot;xxx&quot;# 而后按下一系列的Enter键即可 ​说明： ​ -t 指定算法 ​ -N 私钥加密，xxx为所输入的密码，当你第一次使用私钥登陆不同的机器的时候，需要输入个人的密码，才能登录，相当于私钥的加密。 查看server1钥匙 ls -l ~/.ssh # 存在两个文件id_rsa(私钥)、id_rsa.pub(公钥) 将server1的公钥(id_rsa.pub)的内容复制粘贴到server2和server3的~/.ssh/ authorized_keys中 vim ~/.ssh/ authorized_keys# 每个公钥占用一行# 也可以使用ssh-copy-id# server1上ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.0.102ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.0.103 设置server2和server3的相关权限 chmod 600 authorized_keyschmod 700 -R .ssh# 如果使用ssh-copy-id工具，此处权限设置已经由ssh-copy-id自动完成，无须再手动操作 此时可以实现server1通过SSH免密码登录server2和server3了 [root@192.168.0.101 .ssh]# ssh 192.168.0.102[root@192.168.0.101 .ssh]# ssh 192.168.0.103]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[zabbix如何监控进程或端口]]></title>
      <url>%2F2016%2F05%2F31%2Fzabbix%E5%A6%82%E4%BD%95%E7%9B%91%E6%8E%A7%E8%BF%9B%E7%A8%8B%E6%88%96%E7%AB%AF%E5%8F%A3%2F</url>
      <content type="text"><![CDATA[前几天生产环境有个job没有起来，结果测试人员一直以为是个bug,后来开发人员觉得这块没有bug逻辑啊，怎么回事啊。。终于，经过很长时间的排查，发现有个job快一天没有启动了。。。原因是:前一天晚上新版本上线，然后忘记启动这个job了。。。。这个尴尬的事情，终于让人明白对于进程或者端口的监控是多么的重要~~~本来想写个python脚本来监控的，后来上网一看，哇塞，zabbix好牛逼哦，用zabbix简单的配置下就能搞定了，何必重复造轮子，我写的估计性能也赶不上zabbix,于是我果断选择了用zabbix来监控进程或端口。。。 首先，zabbix的安装部署我就不讲了，将来我会整理一份zabbix的安装部署出来好了。其次，无论监控进程还是端口，都需要在对应的机器上部署一个zabbix_agented。最后，我的前提是zabbix已经正常监听了进程/端口对应的host。接下来，就分别说说如何用zabbix监控进程以及监控端口。 本文只讲到如何触发trigger,至于trigger触发后如何报警，请参考：http://http://wing324.github.io/2016/05/28/zabbix%E4%BD%BF%E7%94%A8python%E8%84%9A%E6%9C%AC%E5%8F%91%E9%80%81%E6%8A%A5%E8%AD%A6%E9%82%AE%E4%BB%B6/ zabbix版本:3.0 zabbix监控进程使用zabbix的item中的key来做进程监控。 监控进程的key,是模糊匹配proc.num[,,,] key对应的释义The number of processes. Returns integer统计指定process的总数 示例中的key值配置proc.num[,mysql,all,/usr/sbin/mysqld –basedir=/usr –datadir=/data/mysql/mysqldata3307/mydata] zabbix中item的配置如下： 报警设置当然用zabbix的trigger相应的trigger对应的Expression如下:{test_agentd:proc.num[,mysql,all,/usr/sbin/mysqld –basedir=/usr –datadir=/data/mysql/mysqldata3307/mydata].last(,0)}=0 zabbix中trigger的配置如下： zabbix监控端口使用zabbix的item中的key来做端口 监控进程的keynet.tcp.listen[port] key对应的释义Checks if this TCP port is in LISTEN state. Returns 0 - it is not in LISTEN state; 1 - it is in LISTEN state检测tcp端口的状态，监听中返回1，未监听返回0 示例中的key值配置net.tcp.listen[3307] zabbix中item的配置如下： 报警设置当然也用zabbix的trigger相应的trigger对应的Expression如下:{test_agentd:net.tcp.listen[3307].last(0)}=0 zabbix中trigger的配置如下： Tips：加上zabbix邮件报警，你就可以停掉对应的进程或者端口来检测是否达到预期期望，即触发了trigger，而后发送报警邮件。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[zabbix使用python脚本发送报警邮件]]></title>
      <url>%2F2016%2F05%2F28%2Fzabbix%E4%BD%BF%E7%94%A8python%E8%84%9A%E6%9C%AC%E5%8F%91%E9%80%81%E6%8A%A5%E8%AD%A6%E9%82%AE%E4%BB%B6%2F</url>
      <content type="text"><![CDATA[此处是用外部脚本python实现zabbix的报警机制的。对于zabbix3.0此处存在一个小改动，让本宝宝忙活了一天才找到原因哒。。。。 1、编辑zabbix_server.conf文件，修改AlertScriptsPath参数,该参数用于指定外部脚本的绝对路径。vim /etc/zabbix/zabbix_server.confAlertScriptsPath=/usr/lib/zabbix/alertscripts 2、上传新增py脚本至AlertScriptsPath参数指定的绝对路径下,py文件如下：#! /usr/bin/env python# coding:utf-8'''[INFORMATION]Zabbix Send Email With PythonAUTHOR : WingGitHub : https://github.com/wing324Email : wing324@126.com'''from email import encodersfrom email.header import Headerfrom email.mime.text import MIMETextfrom email.utils import parseaddr, formataddrimport smtplibimport sysdef send_mail(_to_email,_subject,_message):# 定义邮件发送 smtp_host = 'smtp.xxx.xx' from_email = 'xxx@xxx.xx' passwd = 'xxxxxx' msg = MIMEText(_message,'plain','utf-8') msg['Subject'] = _subject smtp_server = smtplib.SMTP(smtp_host,25) smtp_server.login(from_email,passwd) smtp_server.sendmail(from_email,[_to_email],msg.as_string()) smtp_server.quit()if __name__ == '__main__': send_mail(sys.argv[1],sys.argv[2],sys.argv[3]) 3、修改python脚本的权限chown -R zabbix:zabbix zabbix_send_email.pychmod 755 zabbix_send_email.py 4、zabbix web端配置 Administration –&gt; Media types –&gt; Create media type 创建一个测试用户Administration –&gt; Users –&gt; Create user 为新创建的user指定media:Administration –&gt; Users –&gt; Create user –&gt; Media 创建action实现邮件报警Configuration –&gt; Actions –&gt; Create action 5、zabbix测试发送邮件找一个test的zabbix_agentd，kill掉，查看是否收到报警邮件。再将其恢复，查看是否收到恢复后的邮件。如果一切如预期所想，那么至此就完成了使用python脚本完成zabbix的报警邮件了。如果没有如预期所想。 TIPS：如果你用的zabbix3.0，请注意Administration –&gt; Media types –&gt; Create media type这一步的配置如下：（今天被这个坑了一天。。。。。。）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[我知道MySQL忘记了root密码咋办]]></title>
      <url>%2F2016%2F05%2F15%2F%E6%88%91%E7%9F%A5%E9%81%93MySQL%E5%BF%98%E8%AE%B0%E4%BA%86root%E5%AF%86%E7%A0%81%E5%92%8B%E5%8A%9E%2F</url>
      <content type="text"><![CDATA[环境越多，密码越多，没有合理的密码管理工具，很容易很容易忘记密码哒。。那么忘记数据库root密码了，你说咋办啊。。测试环境密码忘记事情比较小，你要是生产环境数据库密码都忘记了，说出去，别人家还敢让你去管理他们的数据库嘛。。别着急，我给你的秘密武器~~~ 方法一此方法是很常用的方法，是需要重启数据库的。就是使用skip-grant-tables来跳过授权表。# 首先，我假装忘记了数据库密码[root@localhost ~]# mysql -uroot -S /data/mysql/mysqldata3306/sock/mysql.sock -pEnter password: ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: YES)# 此时去my.cnf文件中添加skip-grant-tables参数，用来跳过授权表[root@localhost ~]# vim /etc/my.cnfskip-grant-tables# 重启MySQL[root@localhost ~]# kill -9 11366[root@localhost ~]# mysqld_multi start 3306# 无验证进入MySQL，此时可以看到成功进入[root@localhost ~]# mysql -uroot -S /data/mysql/mysqldata3306/sock/mysql.sockWelcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 1Server version: 5.6.26-log MySQL Community Server (GPL)Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.root@localhost : (none) 06:31:45&gt;# 使用UPDATE修改密码root@localhost : (none) 06:33:56&gt; update mysql.user set password=password('123456') where user='root';Query OK, 4 rows affected (2.73 sec)Rows matched: 4 Changed: 4 Warnings: 0root@localhost : (none) 06:34:16&gt; flush privileges;Query OK, 0 rows affected (0.16 sec)# 再次重启数据库啦，注释掉my.cnf文件中的skip-grant-tables[root@localhost ~]# vim /etc/my.cnf#skip-grant-tables[root@localhost ~]# mysqladmin shutdown -uroot -S /data/mysql/mysqldata3306/sock/mysql.sock -pEnter password:[root@localhost ~]# mysqld_multi start 3306# 使用新密码进入数据库，可以看到成功进入[root@localhost ~]# mysql -uroot -S /data/mysql/mysqldata3306/sock/mysql.sock -p123456Warning: Using a password on the command line interface can be insecure.Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 3Server version: 5.6.26-log MySQL Community Server (GPL)Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.root@localhost : (none) 06:38:41&gt; 方法二方法一呢需要重启数据库，可是咱是生产环境啊，重启数据库得损失多个亿啊，坚决不能重启数据库，那么此时，只能使用黑客的方法啦。。。具体思路是，可以将user表的frm/MYD/MYI文件拷贝到另外一个数据库中，然后对密码进行UPDATE，然后在将文件move回来，使用KILL -HUB pidof mysqld重新加载配置，然后就可以使用新密码啦~（今天想睡觉了，下次再来补上具体过程）# 首先将数据库（此处假设3306实例）中user相关的数据文件移到另一个库(此处假设3307实例)的test目录下mv /data/mysqldata3306/mydata/mysql/user.* /data/mysqldata3307/mysqldata/test# 进入3307数据库下root@localhost : (none) 06:33:56&gt; update test.user set password=password('123456') where user='root';Query OK, 4 rows affected (2.73 sec)Rows matched: 4 Changed: 4 Warnings: 0# 此时将3307的user相关文件再move回原来的数据库目录下mv /data/mysqldata3307/mysqldata/test /data/mysqldata3306/mydata/mysql/user.*# 使用KILL -HUB加载mysqld的权限，看到KILL不要怕，KILL -HUB是重新加载配置的意思啦KILL -HUB `pidof mysqld`# 此时你就可以使用新的密码登录到数据库里面去了，整个过程完全不需要启动数据库~[root@localhost ~]# mysql -uroot -S /data/mysql/mysqldata3306/sock/mysql.sock -p123456Warning: Using a password on the command line interface can be insecure.Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 3Server version: 5.6.26-log MySQL Community Server (GPL)Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.root@localhost : (none) 07:38:41&gt;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[你知道FLOAT类型不能用等值查询吗]]></title>
      <url>%2F2016%2F05%2F15%2F%E4%BD%A0%E7%9F%A5%E9%81%93%E6%B5%AE%E7%82%B9%E6%95%B0%E4%B8%8D%E8%83%BD%E7%94%A8%E7%AD%89%E5%80%BC%E6%9F%A5%E8%AF%A2%E5%90%97%2F</url>
      <content type="text"><![CDATA[今天听到一个说法，说float类型不能用等值查询，用等值查询你是查不到结果的。之前我可是完全不知道这么个结论啊。。作为DBA一枚，怎么能道听途说呢，万一别人胡说八道呢。我得先验证下这个小道消息对不对。。 场景重现root@127.0.0.1 : wing 11:57:45&gt; create table t(id float);Query OK, 0 rows affected (0.51 sec)root@127.0.0.1 : wing 12:01:29&gt; insert into t values(1.1);Query OK, 1 row affected (0.28 sec)root@127.0.0.1 : wing 12:01:38&gt; select * from t;+------+| id |+------+| 1.1 |+------+1 row in set (0.00 sec)# 哎呀呀，明明有1.1这个记录，为什么就是查不到哇root@127.0.0.1 : wing 12:01:41&gt; select * from t where id=1.1;Empty set (0.00 sec)# 试试这个方法root@127.0.0.1 : wing 12:01:47&gt; select * from t where id&gt;1;+------+| id |+------+| 1.1 |+------+1 row in set (0.00 sec)# 再试试这个方法root@127.0.0.1 : wing 12:02:18&gt; select * from t where id&lt;2;+------+| id |+------+| 1.1 |+------+1 row in set (0.00 sec)# 这个方法查不到倒是符合预期的root@127.0.0.1 : wing 12:07:09&gt; select * from t where id=1 -&gt; ;Empty set (0.00 sec) 原理通过实验验证，好像别人真的不是胡说八道呢。可是为什FLOAT类型不能等值查询呢，即浮点数为什么不能等号比较呢？。。人就怕认真（我在自夸，你感受到了咩。。）通过科学上网，我找到了原因。。float在存储时是以近似值的方式存储的，所以不能使用等值查询。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[如何在MySQL中阻止UPDATE语句没有添加WHERE条件的发生]]></title>
      <url>%2F2016%2F05%2F01%2F%E5%A6%82%E4%BD%95%E5%9C%A8MySQL%E4%B8%AD%E9%98%BB%E6%AD%A2UPDATE%E8%AF%AD%E5%8F%A5%E6%B2%A1%E6%9C%89%E6%B7%BB%E5%8A%A0WHERE%E6%9D%A1%E4%BB%B6%E7%9A%84%E5%8F%91%E7%94%9F%2F</url>
      <content type="text"><![CDATA[如果在生产环境中使用UPDATE语句更新表数据，此时如果忘记携带本应该添加的WHERE条件，那么。。Oh,no…后果可能不堪设想。。。之前就遇到一个同事在生产环境UPDATE忘记携带WHERE条件。。于是只能从binlog日志中找到相关数据，然后去恢复。。宝宝当时表示心好累。。。那么有没有什么办法可以阻止这样的事情发生，又不使用任何的审核工具呢。。。办法当然是有的，请听我说~ sql_safe_updatessql_safe_updates这个MySQL自带的参数就可以完美的解决我们的问题，并且该参数是可以在线变更的哦~当该参数开启的情况下，你必须要在UPDATE语句后携带WHERE条件，否则就会报出ERROR。。 举个栗子# sql_safe_updates=0,即未开启root@127.0.0.1 : test 07:58:34&gt; set sql_safe_updates=0;Query OK, 0 rows affected (0.00 sec)root@127.0.0.1 : test 07:58:43&gt; show variables like 'sql_safe_updates';+------------------+-------+| Variable_name | Value |+------------------+-------+| sql_safe_updates | OFF |+------------------+-------+1 row in set (0.00 sec)root@127.0.0.1 : test 07:58:55&gt; select * from t;+-------+| pd |+-------+| hello || mysql |+-------+2 rows in set (0.00 sec)root@127.0.0.1 : test 07:58:59&gt; begin;Query OK, 0 rows affected (0.00 sec)root@127.0.0.1 : test 07:59:04&gt; update t set pd='MySQL';Query OK, 2 rows affected (0.00 sec)Rows matched: 2 Changed: 2 Warnings: 0root@127.0.0.1 : test 07:59:12&gt; select * from t;+-------+| pd |+-------+| MySQL || MySQL |+-------+2 rows in set (0.00 sec)# sql_safe_updates=1,即开启root@127.0.0.1 : test 08:00:00&gt; set sql_safe_updates=1;Query OK, 0 rows affected (0.00 sec)root@127.0.0.1 : test 08:00:11&gt; show variables like 'sql_safe_updates';+------------------+-------+| Variable_name | Value |+------------------+-------+| sql_safe_updates | ON |+------------------+-------+1 row in set (0.00 sec)root@127.0.0.1 : test 08:00:16&gt; select * from t;+-------+| pd |+-------+| hello || mysql |+-------+2 rows in set (0.00 sec)root@127.0.0.1 : test 08:00:25&gt; begin;Query OK, 0 rows affected (0.00 sec)root@127.0.0.1 : test 08:00:27&gt; update t set pd='MySQL';ERROR 1175 (HY000): You are using safe update mode and you tried to update a table without a WHERE that uses a KEY column 如上属的栗子所示，当参数sql_safe_updates开启的时候，UPDATE语句不携带WHERE条件将会爆出一个错误。。所以小心使用UPDATE语句是真的很重要哇。。。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Oracle启动与关闭]]></title>
      <url>%2F2016%2F04%2F24%2FOracle%E5%90%AF%E5%8A%A8%E4%B8%8E%E5%85%B3%E9%97%AD%2F</url>
      <content type="text"><![CDATA[Oralce的启动与关闭。 Oracle启动oracle启动分为三个步骤： startup nomountOracle找到参数文件，根据参数文件设置将内存空间划分出来并打开相应的进程。SQL&gt; startup nomountORACLE instance started.Total System Global Area 814264320 bytesFixed Size 2257560 bytesVariable Size 415239528 bytesDatabase Buffers 394264576 bytesRedo Buffers 2502656 bytes alter database mount根据参数文件设置的控制文件位置，打开控制文件。SQL&gt; alter database mount;Database altered. alter database open开启Oracle数据库。SQL&gt; alter database open;Database altered. 注意startup直接全部执行”nomount”,”mount”,”open”步骤。 Oracle关闭abort模拟突然掉电 内存被清空、内存中的数据没有写入数据文件 事务被立即中断 没有提交、没有回滚 immediate（常用）强制中断当前正在运行的所有事务，回滚这些事务 回滚完毕，强制中断所有的连接 讲实例中的所有数据写入数据文件 transactional（常用）等待正在运行的事务，一直到他们提交或者回滚 所有事务主动结束以后（提交或者回滚），强行中断连接 将实例里面的数据写入数据文件 清空缓存 如果有事务一直没有提交或者回滚，实例无法关闭 normal(默认)等待事务的主动提交或者回滚 等待用户主动断开连接 如果有一个用户没有断开连接，那么数据库无法关闭 修改参数的scope含义scope=both参数立即生效并且永久生效 scope=spfile参数需要重启后生效 scope=memory参数立即生效，但重启后将不再生效]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Oracle的备份与恢复]]></title>
      <url>%2F2016%2F04%2F24%2FOracle%E7%9A%84%E5%A4%87%E4%BB%BD%E4%B8%8E%E6%81%A2%E5%A4%8D%2F</url>
      <content type="text"><![CDATA[数据库中最重要最关键的就是备份与恢复啦，这节我们就来谈一谈Oralce数据库的备份与恢复。 针对expdp,impdp的使用。参考链接：https://oracle-base.com/articles/10g/oracle-data-pump-10g 创建示例数据以scott用户为例[oracle@wing ~]$ sqlplus / as sysdbaSQL*Plus: Release 11.2.0.4.0 Production on Fri Feb 26 10:52:51 2016Copyright (c) 1982, 2013, Oracle. All rights reserved.Connected to:Oracle Database 11g Enterprise Edition Release 11.2.0.4.0 - 64bit ProductionWith the Partitioning, OLAP, Data Mining and Real Application Testing optionsSQL&gt;# 创建目录SQL&gt; create directory test_dir as '/u01/backup'; Directory created# 给scott用户解锁并设置密码SQL&gt; ALTER USER scott IDENTIFIED BY scott ACCOUNT UNLOCK;# 给scott用户赋权SQL&gt; grant read,write on directory test_dir to scott;Grant succeeded.# 以scott用户登录数据库中[oracle@wing ~]$ sqlplus scott/scott@WINGDBSQL*Plus: Release 11.2.0.4.0 Production on Fri Feb 26 11:09:15 2016Copyright (c) 1982, 2013, Oracle. All rights reserved.Connected to:Oracle Database 11g Enterprise Edition Release 11.2.0.4.0 - 64bit ProductionWith the Partitioning, OLAP, Data Mining and Real Application Testing optionsSQL&gt; # 创建表并且插入数据SQL&gt; create table test_dump (id int);Table createdSQL&gt; commit;Commit completeSQL&gt; desc test_dump;Name Type Nullable Default Comments ---- ------- -------- ------- -------- ID INTEGER Y SQL&gt; insert into test_dump values(1);1 row insertedSQL&gt; insert into test_dump values(2);1 row insertedSQL&gt; insert into test_dump values(3);1 row insertedSQL&gt; insert into test_dump values(4);1 row insertedSQL&gt; insert into test_dump values(5);1 row insertedSQL&gt; insert into test_dump values(6);1 row insertedSQL&gt; insert into test_dump values(7);1 row insertedSQL&gt; insert into test_dump values(8);1 row insertedSQL&gt; insert into test_dump values(9);1 row insertedSQL&gt; insert into test_dump values(10);1 row insertedSQL&gt; commit;Commit completeSQL&gt; select * from test_dump; ID--------------------------------------- 1 2 3 4 5 6 7 8 9 1010 rows selectedSQL&gt; create table test (name varchar(16));Table createdSQL&gt; desc test;Name Type Nullable Default Comments ---- ------------ -------- ------- -------- NAME VARCHAR2(16) Y SQL&gt; insert into test values('Oracle');1 row insertedSQL&gt; insert into test values('MySQL');1 row insertedSQL&gt; insert into test values('MS SQL');1 row insertedSQL&gt; insert into test values('SQL server');1 row insertedSQL&gt; insert into test values('PostgreSQL');1 row insertedSQL&gt; commit;Commit completeSQL&gt; select * from test;NAME----------------OracleMySQLMS SQLSQL serverPostgreSQLSQL&gt; create table scott_table (id int,name varchar(16));Table createdSQL&gt; insert into scott_table values(1,'Oracle');1 row insertedSQL&gt; insert into scott_table values(2,'MySQL');1 row insertedSQL&gt; commit;Commit complete# 至此，示例表创建完毕，共有test,test_dump,scott_table三张表 expdp表导出以test_dump,scott_table表导出为例子# tables指定需要备份的表# directory存放dumpfile和logfile的地方# dumpfile备份文件# logfile备份日志文件# 备份scott用户的test_dump,scott_table表[oracle@wing ~]$ expdp scott/scott@WINGDB tables=test_dump,scott_table directory=TEST_DIR dumpfile=scott_dump.dmp logfile=scott_exp.logExport: Release 11.2.0.4.0 - Production on Fri Feb 26 11:45:04 2016Copyright (c) 1982, 2011, Oracle and/or its affiliates. All rights reserved.Connected to: Oracle Database 11g Enterprise Edition Release 11.2.0.4.0 - 64bit ProductionWith the Partitioning, OLAP, Data Mining and Real Application Testing optionsStarting &quot;SCOTT&quot;.&quot;SYS_EXPORT_TABLE_01&quot;: scott/********@WINGDB tables=test_dump,scott_table directory=TEST_DIR dumpfile=scott_dump.dmp logfile=scott_exp.log Estimate in progress using BLOCKS method...Processing object type TABLE_EXPORT/TABLE/TABLE_DATATotal estimation using BLOCKS method: 128 KBProcessing object type TABLE_EXPORT/TABLE/TABLE. . exported &quot;SCOTT&quot;.&quot;SCOTT_TABLE&quot; 5.445 KB 2 rows. . exported &quot;SCOTT&quot;.&quot;TEST_DUMP&quot; 5.085 KB 10 rowsMaster table &quot;SCOTT&quot;.&quot;SYS_EXPORT_TABLE_01&quot; successfully loaded/unloaded******************************************************************************Dump file set for SCOTT.SYS_EXPORT_TABLE_01 is: /u01/backup/scott_dump.dmpJob &quot;SCOTT&quot;.&quot;SYS_EXPORT_TABLE_01&quot; successfully completed at Fri Feb 26 11:45:21 2016 elapsed 0 00:00:14# 进入directory目录下查看存在dumpfile和logfile[root@wing backup]# lltotal 112-rw-r-----. 1 oracle oinstall 110592 Feb 26 11:45 scott_dump.dmp-rw-r--r--. 1 oracle oinstall 1164 Feb 26 11:45 scott_exp.log schema导出以scottschema为例[oracle@wing ~]$ expdp scott/scott@WINGDB schemas=SCOTT directory=TEST_DIR dumpfile=SCOTT.dmp logfile=scott_exp.logExport: Release 11.2.0.4.0 - Production on Fri Feb 26 13:30:13 2016Copyright (c) 1982, 2011, Oracle and/or its affiliates. All rights reserved.Connected to: Oracle Database 11g Enterprise Edition Release 11.2.0.4.0 - 64bit ProductionWith the Partitioning, OLAP, Data Mining and Real Application Testing optionsStarting &quot;SCOTT&quot;.&quot;SYS_EXPORT_SCHEMA_01&quot;: scott/********@WINGDB schemas=SCOTT directory=TEST_DIR dumpfile=SCOTT.dmp logfile=scott_exp.log Estimate in progress using BLOCKS method...Processing object type SCHEMA_EXPORT/TABLE/TABLE_DATATotal estimation using BLOCKS method: 384 KBProcessing object type SCHEMA_EXPORT/PRE_SCHEMA/PROCACT_SCHEMAProcessing object type SCHEMA_EXPORT/TABLE/TABLEProcessing object type SCHEMA_EXPORT/TABLE/COMMENTProcessing object type SCHEMA_EXPORT/TABLE/INDEX/INDEXProcessing object type SCHEMA_EXPORT/TABLE/CONSTRAINT/CONSTRAINTProcessing object type SCHEMA_EXPORT/TABLE/INDEX/STATISTICS/INDEX_STATISTICSProcessing object type SCHEMA_EXPORT/TABLE/CONSTRAINT/REF_CONSTRAINTProcessing object type SCHEMA_EXPORT/TABLE/STATISTICS/TABLE_STATISTICS. . exported &quot;SCOTT&quot;.&quot;DEPT&quot; 5.929 KB 4 rows. . exported &quot;SCOTT&quot;.&quot;EMP&quot; 8.562 KB 14 rows. . exported &quot;SCOTT&quot;.&quot;SALGRADE&quot; 5.859 KB 5 rows. . exported &quot;SCOTT&quot;.&quot;SCOTT_TABLE&quot; 5.468 KB 4 rows. . exported &quot;SCOTT&quot;.&quot;TEST&quot; 5.062 KB 5 rows. . exported &quot;SCOTT&quot;.&quot;TEST_DUMP&quot; 5.156 KB 20 rows. . exported &quot;SCOTT&quot;.&quot;BONUS&quot; 0 KB 0 rowsMaster table &quot;SCOTT&quot;.&quot;SYS_EXPORT_SCHEMA_01&quot; successfully loaded/unloaded******************************************************************************Dump file set for SCOTT.SYS_EXPORT_SCHEMA_01 is: /u01/backup/SCOTT.dmpJob &quot;SCOTT&quot;.&quot;SYS_EXPORT_SCHEMA_01&quot; successfully completed at Fri Feb 26 13:30:48 2016 elapsed 0 00:00:34 database导出整个数据库的导出# full=Y代表全备整个数据库[oracle@wing ~]$ expdp system/Wing_database@WINGDB full=Y directory=TEST_DIR dumpfile=oracle11g.dmp logfile=oracle11g_exp.logExport: Release 11.2.0.4.0 - Production on Fri Feb 26 13:57:33 2016Copyright (c) 1982, 2011, Oracle and/or its affiliates. All rights reserved.Connected to: Oracle Database 11g Enterprise Edition Release 11.2.0.4.0 - 64bit ProductionWith the Partitioning, OLAP, Data Mining and Real Application Testing optionsStarting &quot;SYSTEM&quot;.&quot;SYS_EXPORT_FULL_01&quot;: system/********@WINGDB full=Y directory=TEST_DIR dumpfile=oracle11g.dmp logfile=oracle11g_exp.log Estimate in progress using BLOCKS method...Processing object type DATABASE_EXPORT/SCHEMA/TABLE/TABLE_DATATotal estimation using BLOCKS method: 384.6 MBProcessing object type DATABASE_EXPORT/TABLESPACEProcessing object type DATABASE_EXPORT/PROFILEProcessing object type DATABASE_EXPORT/SYS_USER/USERProcessing object type DATABASE_EXPORT/SCHEMA/USERProcessing object type DATABASE_EXPORT/ROLEProcessing object type DATABASE_EXPORT/GRANT/SYSTEM_GRANT/PROC_SYSTEM_GRANTProcessing object type DATABASE_EXPORT/SCHEMA/GRANT/SYSTEM_GRANTProcessing object type DATABASE_EXPORT/SCHEMA/ROLE_GRANTProcessing object type DATABASE_EXPORT/SCHEMA/DEFAULT_ROLEProcessing object type DATABASE_EXPORT/SCHEMA/TABLESPACE_QUOTAProcessing object type DATABASE_EXPORT/RESOURCE_COSTProcessing object type DATABASE_EXPORT/TRUSTED_DB_LINKProcessing object type DATABASE_EXPORT/SCHEMA/SEQUENCE/SEQUENCEProcessing object type DATABASE_EXPORT/SCHEMA/SEQUENCE/GRANT/OWNER_GRANT/OBJECT_GRANTProcessing object type DATABASE_EXPORT/DIRECTORY/DIRECTORYProcessing object type DATABASE_EXPORT/DIRECTORY/GRANT/OWNER_GRANT/OBJECT_GRANTProcessing object type DATABASE_EXPORT/CONTEXTProcessing object type DATABASE_EXPORT/SCHEMA/PUBLIC_SYNONYM/SYNONYMProcessing object type DATABASE_EXPORT/SCHEMA/SYNONYMProcessing object type DATABASE_EXPORT/SCHEMA/TYPE/INC_TYPEProcessing object type DATABASE_EXPORT/SCHEMA/TYPE/TYPE_SPECProcessing object type DATABASE_EXPORT/SCHEMA/TYPE/GRANT/OWNER_GRANT/OBJECT_GRANTProcessing object type DATABASE_EXPORT/SYSTEM_PROCOBJACT/PRE_SYSTEM_ACTIONS/PROCACT_SYSTEMProcessing object type DATABASE_EXPORT/SYSTEM_PROCOBJACT/PROCOBJProcessing object type DATABASE_EXPORT/SYSTEM_PROCOBJACT/POST_SYSTEM_ACTIONS/PROCACT_SYSTEMProcessing object type DATABASE_EXPORT/SCHEMA/PROCACT_SCHEMAProcessing object type DATABASE_EXPORT/SCHEMA/XMLSCHEMA/XMLSCHEMAProcessing object type DATABASE_EXPORT/SCHEMA/TABLE/TABLEProcessing object type DATABASE_EXPORT/SCHEMA/TABLE/PRE_TABLE_ACTIONProcessing object type DATABASE_EXPORT/SCHEMA/TABLE/GRANT/OWNER_GRANT/OBJECT_GRANTProcessing object type DATABASE_EXPORT/SCHEMA/TABLE/COMMENTProcessing object type DATABASE_EXPORT/SCHEMA/PACKAGE/PACKAGE_SPECProcessing object type DATABASE_EXPORT/SCHEMA/PACKAGE/GRANT/OWNER_GRANT/OBJECT_GRANTProcessing object type DATABASE_EXPORT/SCHEMA/FUNCTION/FUNCTIONProcessing object type DATABASE_EXPORT/SCHEMA/FUNCTION/GRANT/OWNER_GRANT/OBJECT_GRANTProcessing object type DATABASE_EXPORT/SCHEMA/PROCEDURE/PROCEDUREProcessing object type DATABASE_EXPORT/SCHEMA/PROCEDURE/GRANT/OWNER_GRANT/OBJECT_GRANTProcessing object type DATABASE_EXPORT/SCHEMA/PACKAGE/COMPILE_PACKAGE/PACKAGE_SPEC/ALTER_PACKAGE_SPECProcessing object type DATABASE_EXPORT/SCHEMA/FUNCTION/ALTER_FUNCTIONProcessing object type DATABASE_EXPORT/SCHEMA/PROCEDURE/ALTER_PROCEDUREProcessing object type DATABASE_EXPORT/SCHEMA/TABLE/INDEX/INDEXProcessing object type DATABASE_EXPORT/SCHEMA/TABLE/INDEX/FUNCTIONAL_INDEX/INDEXProcessing object type DATABASE_EXPORT/SCHEMA/TABLE/CONSTRAINT/CONSTRAINTProcessing object type DATABASE_EXPORT/SCHEMA/TABLE/INDEX/STATISTICS/INDEX_STATISTICSProcessing object type DATABASE_EXPORT/SCHEMA/TABLE/INDEX/STATISTICS/FUNCTIONAL_INDEX/INDEX_STATISTICSProcessing object type DATABASE_EXPORT/SCHEMA/VIEW/VIEWProcessing object type DATABASE_EXPORT/SCHEMA/VIEW/GRANT/OWNER_GRANT/OBJECT_GRANTProcessing object type DATABASE_EXPORT/SCHEMA/VIEW/COMMENTProcessing object type DATABASE_EXPORT/SCHEMA/PACKAGE_BODIES/PACKAGE/PACKAGE_BODYProcessing object type DATABASE_EXPORT/SCHEMA/TYPE/TYPE_BODYProcessing object type DATABASE_EXPORT/SCHEMA/TABLE/CONSTRAINT/REF_CONSTRAINTProcessing object type DATABASE_EXPORT/SCHEMA/TABLE/INDEX/BITMAP_INDEX/INDEXProcessing object type DATABASE_EXPORT/SCHEMA/TABLE/INDEX/STATISTICS/BITMAP_INDEX/INDEX_STATISTICSProcessing object type DATABASE_EXPORT/SCHEMA/TABLE/STATISTICS/TABLE_STATISTICSProcessing object type DATABASE_EXPORT/SCHEMA/TABLE/INDEX/DOMAIN_INDEX/INDEXProcessing object type DATABASE_EXPORT/SCHEMA/TABLE/POST_TABLE_ACTIONProcessing object type DATABASE_EXPORT/SCHEMA/TABLE/TRIGGERProcessing object type DATABASE_EXPORT/SCHEMA/VIEW/TRIGGERProcessing object type DATABASE_EXPORT/SCHEMA/EVENT/TRIGGERProcessing object type DATABASE_EXPORT/SCHEMA/MATERIALIZED_VIEWProcessing object type DATABASE_EXPORT/SCHEMA/JOBProcessing object type DATABASE_EXPORT/SCHEMA/DIMENSIONProcessing object type DATABASE_EXPORT/SCHEMA/TABLE/POST_INSTANCE/PROCACT_INSTANCEProcessing object type DATABASE_EXPORT/SCHEMA/TABLE/POST_INSTANCE/PROCDEPOBJProcessing object type DATABASE_EXPORT/SCHEMA/POST_SCHEMA/PROCOBJProcessing object type DATABASE_EXPORT/SCHEMA/POST_SCHEMA/PROCACT_SCHEMAProcessing object type DATABASE_EXPORT/AUDIT. . exported &quot;SH&quot;.&quot;CUSTOMERS&quot; 9.853 MB 55500 rows. . exported &quot;PM&quot;.&quot;ONLINE_MEDIA&quot; 7.854 MB 9 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_PAGE_PLUGS&quot; 5.205 MB 7416 rows. . exported &quot;SH&quot;.&quot;COSTS&quot;:&quot;COSTS_Q1_1998&quot; 139.5 KB 4411 rows. . exported &quot;SH&quot;.&quot;COSTS&quot;:&quot;COSTS_Q1_1999&quot; 183.5 KB 5884 rows. . exported &quot;SH&quot;.&quot;COSTS&quot;:&quot;COSTS_Q1_2000&quot; 120.6 KB 3772 rows. . exported &quot;SH&quot;.&quot;COSTS&quot;:&quot;COSTS_Q1_2001&quot; 227.8 KB 7328 rows. . exported &quot;SH&quot;.&quot;COSTS&quot;:&quot;COSTS_Q2_1998&quot; 79.52 KB 2397 rows. . exported &quot;SH&quot;.&quot;COSTS&quot;:&quot;COSTS_Q2_1999&quot; 132.5 KB 4179 rows. . exported &quot;SH&quot;.&quot;COSTS&quot;:&quot;COSTS_Q2_2000&quot; 119.0 KB 3715 rows. . exported &quot;SH&quot;.&quot;COSTS&quot;:&quot;COSTS_Q2_2001&quot; 184.5 KB 5882 rows. . exported &quot;SH&quot;.&quot;COSTS&quot;:&quot;COSTS_Q3_1998&quot; 131.1 KB 4129 rows. . exported &quot;SH&quot;.&quot;COSTS&quot;:&quot;COSTS_Q3_1999&quot; 137.3 KB 4336 rows. . exported &quot;SH&quot;.&quot;COSTS&quot;:&quot;COSTS_Q3_2000&quot; 151.4 KB 4798 rows. . exported &quot;SH&quot;.&quot;COSTS&quot;:&quot;COSTS_Q3_2001&quot; 234.4 KB 7545 rows. . exported &quot;SH&quot;.&quot;COSTS&quot;:&quot;COSTS_Q4_1998&quot; 144.7 KB 4577 rows. . exported &quot;SH&quot;.&quot;COSTS&quot;:&quot;COSTS_Q4_1999&quot; 159.0 KB 5060 rows. . exported &quot;SH&quot;.&quot;COSTS&quot;:&quot;COSTS_Q4_2000&quot; 160.2 KB 5088 rows. . exported &quot;SH&quot;.&quot;COSTS&quot;:&quot;COSTS_Q4_2001&quot; 278.4 KB 9011 rows. . exported &quot;SH&quot;.&quot;SALES&quot;:&quot;SALES_Q1_1998&quot; 1.412 MB 43687 rows. . exported &quot;SH&quot;.&quot;SALES&quot;:&quot;SALES_Q1_1999&quot; 2.071 MB 64186 rows. . exported &quot;SH&quot;.&quot;SALES&quot;:&quot;SALES_Q1_2000&quot; 2.012 MB 62197 rows. . exported &quot;SH&quot;.&quot;SALES&quot;:&quot;SALES_Q1_2001&quot; 1.965 MB 60608 rows. . exported &quot;SH&quot;.&quot;SALES&quot;:&quot;SALES_Q2_1998&quot; 1.160 MB 35758 rows. . exported &quot;SH&quot;.&quot;SALES&quot;:&quot;SALES_Q2_1999&quot; 1.754 MB 54233 rows. . exported &quot;SH&quot;.&quot;SALES&quot;:&quot;SALES_Q2_2000&quot; 1.802 MB 55515 rows. . exported &quot;SH&quot;.&quot;SALES&quot;:&quot;SALES_Q2_2001&quot; 2.051 MB 63292 rows. . exported &quot;SH&quot;.&quot;SALES&quot;:&quot;SALES_Q3_1998&quot; 1.633 MB 50515 rows. . exported &quot;SH&quot;.&quot;SALES&quot;:&quot;SALES_Q3_1999&quot; 2.166 MB 67138 rows. . exported &quot;SH&quot;.&quot;SALES&quot;:&quot;SALES_Q3_2000&quot; 1.909 MB 58950 rows. . exported &quot;SH&quot;.&quot;SALES&quot;:&quot;SALES_Q3_2001&quot; 2.130 MB 65769 rows. . exported &quot;SH&quot;.&quot;SALES&quot;:&quot;SALES_Q4_1998&quot; 1.581 MB 48874 rows. . exported &quot;SH&quot;.&quot;SALES&quot;:&quot;SALES_Q4_1999&quot; 2.014 MB 62388 rows. . exported &quot;SH&quot;.&quot;SALES&quot;:&quot;SALES_Q4_2000&quot; 1.814 MB 55984 rows. . exported &quot;SH&quot;.&quot;SALES&quot;:&quot;SALES_Q4_2001&quot; 2.257 MB 69749 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_STEP_ITEMS&quot; 3.505 MB 9671 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_STEP_PROCESSING&quot; 2.188 MB 2238 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_MESSAGES&quot; 4.156 MB 23311 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_DICTIONARY$&quot; 2.909 MB 70601 rows. . exported &quot;SH&quot;.&quot;SUPPLEMENTARY_DEMOGRAPHICS&quot; 697.3 KB 4500 rows. . exported &quot;OE&quot;.&quot;PRODUCT_DESCRIPTIONS&quot; 2.379 MB 8640 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ESA_REPORT&quot; 2.172 MB 31659 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_HC_OS_COMPONENTS&quot; 2.365 MB 23919 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_HC_VENDOR_SW_COMPONENTS&quot; 2.365 MB 23919 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_HC_VENDOR_SW_SUMMARY&quot; 1.844 MB 23919 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_REGION_REPORT_COLUMN&quot; 1.199 MB 7903 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_STEP_ITEM_HELP&quot; 1003. KB 6335 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_HC_OS_PROPERTIES&quot; 1.032 MB 15970 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_METRICS&quot; 3.230 MB 12637 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_IP_REPORT_ELEM_PARAMS&quot; 756.8 KB 1788 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_STEPS&quot; 787.3 KB 1754 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_DOCS&quot; 177.5 KB 9 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_METRICS_RAW&quot; 899.0 KB 15303 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_METRICS_1HOUR&quot; 791.4 KB 10451 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_INV_COMPONENT&quot; 701.0 KB 2856 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_LIST_ITEMS&quot; 590.3 KB 3048 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_STEP_VALIDATIONS&quot; 611.4 KB 1990 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_LIST_TEMPLATES&quot; 111.3 KB 105 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_STEP_BRANCHES&quot; 508.6 KB 3255 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_STEP_BUTTONS&quot; 472.4 KB 3513 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_TEMPLATES&quot; 296.3 KB 64 rows. . exported &quot;PM&quot;.&quot;PRINT_MEDIA&quot; 190.3 KB 4 rowsORA-39181: Only partial table data may be exported due to fine grain access control on &quot;OE&quot;.&quot;PURCHASEORDER&quot;. . exported &quot;OE&quot;.&quot;PURCHASEORDER&quot; 243.9 KB 132 rows. . exported &quot;SH&quot;.&quot;FWEEK_PSCAT_SALES_MV&quot; 419.8 KB 11266 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DB_INIT_PARAMS_ECM&quot; 470.2 KB 8454 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_STEP_PARAMS&quot; 378.8 KB 4032 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_POLICIES&quot; 714.3 KB 3260 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_LIST_OF_VALUES_DATA&quot; 392.0 KB 4184 rows. . exported &quot;SH&quot;.&quot;PROMOTIONS&quot; 58.89 KB 503 rows. . exported &quot;SH&quot;.&quot;TIMES&quot; 380.8 KB 1826 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_MESSAGES$&quot; 348.4 KB 3706 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_PAGE_PLUG_TEMPLATES&quot; 165.8 KB 166 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_WORKSHEETS&quot; 233.3 KB 30 rows. . exported &quot;SYSMAN&quot;.&quot;ESM_COLLECTION&quot; 310.7 KB 4175 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DB_RECSEGMENTSETTINGS_ECM&quot; 337.1 KB 4400 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_METRICS_1DAY&quot; 313.2 KB 3878 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_CUSTOM_AUTH_SETUPS&quot; 21.56 KB 11 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_ROW_TEMPLATES&quot; 81.09 KB 54 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_STEP_COMPUTATIONS&quot; 258.3 KB 984 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_CRED_PARAMS&quot; 30.90 KB 86 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_VIOLATIONS&quot; 271.4 KB 1114 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_LISTS_OF_VALUES$&quot; 207.8 KB 959 rows. . exported &quot;OE&quot;.&quot;WAREHOUSES&quot; 12.45 KB 9 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_IP_SQL_STATEMENTS&quot; 120.2 KB 31 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_SEC_INFO&quot; 9.078 KB 1 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_MENU_OPTIONS&quot; 170.1 KB 1452 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_WORKSHEET_COLUMNS&quot; 185.7 KB 721 rows. . exported &quot;OE&quot;.&quot;CUSTOMERS&quot; 77.97 KB 319 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_STD_ATTRS&quot; 188.8 KB 2415 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_INV_DEPENDENCY_RULE&quot; 195.9 KB 5124 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_EXECPLAN&quot; 199.4 KB 1448 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_PROP_PARAMS&quot; 10.82 KB 25 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_POLICY_ASSOC&quot; 97.54 KB 1208 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_POLICY_ASSOC_CFG&quot; 193.0 KB 1258 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_PROCESSING&quot; 73.33 KB 45 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_RANDOM_IMAGES&quot; 50.37 KB 42 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_CT_PRED_OPRD&quot; 13.84 KB 53 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_MAPPING_DOCS&quot; 7.890 KB 1 rows. . exported &quot;PM&quot;.&quot;TEXTDOCS_NESTEDTAB&quot; 87.73 KB 12 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_COLLECTIONS&quot; 34.78 KB 221 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_COLLECTION_TASKS&quot; 20.84 KB 28 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_MD_ALL_TBL_COLUMNS&quot; 110.2 KB 704 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_IP_ELEM_DEFAULT_PARAMS&quot; 42.48 KB 130 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_PARAM_SOURCE&quot; 79.73 KB 527 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_SCHEDULE&quot; 11.13 KB 2 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_SEC_INFO&quot; 9 KB 7 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_USER_PARAMS&quot; 7.515 KB 15 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_POLICY_ASSOC_CFG_PARAMS&quot; 67.34 KB 802 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_SYSTEM_PERFORMANCE_LOG&quot; 92.35 KB 1205 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_BANNER&quot; 12.64 KB 10 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_BUTTON_TEMPLATES&quot; 17.35 KB 12 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_FLASH_CHARTS&quot; 30.08 KB 5 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_FLASH_CHART_SERIES&quot; 16.32 KB 5 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_INSTALL&quot; 36.28 KB 2 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_LISTS&quot; 51.74 KB 601 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_PAGE_GENERIC_ATTR&quot; 8.632 KB 44 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_SHORTCUTS&quot; 38.79 KB 39 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_THEMES&quot; 20.89 KB 10 rows. . exported &quot;OE&quot;.&quot;PRODUCT_INFORMATION&quot; 72.77 KB 288 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_AGENT_SEC_INFO&quot; 7.960 KB 1 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ARU_PRODUCT_RELEASE_MAP&quot; 84.79 KB 5956 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CATEGORY_MAP&quot; 46.76 KB 637 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_COLL_ITEM_METRICS&quot; 20.57 KB 238 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CURRENT_METRICS&quot; 38.60 KB 498 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CURRENT_VIOLATION&quot; 46.64 KB 122 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_SNAPSHOT_MD_COLUMNS&quot; 87.19 KB 839 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_COMMAND_BLOCK_PROCS&quot; 5.914 KB 3 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_LARGE_PARAMS&quot; 5.929 KB 2 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_OUTPUT&quot; 12.39 KB 11 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_PARAMETER&quot; 8.859 KB 2 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_SQL_PARAMS&quot; 6.828 KB 7 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_SUBST_PARAMS&quot; 6.296 KB 13 rows. . exported &quot;SYSMAN&quot;.&quot;PARAM_VALUES_TAB&quot; 21.74 KB 240 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_LAST_VIOLATION&quot; 57.34 KB 795 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_LICENSE_DEFINITIONS&quot; 54.65 KB 59 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_LONG_TEXT&quot; 7.148 KB 19 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_METRIC_ERRORS&quot; 42.70 KB 279 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_POLICY_ASSOC_EVAL_DETAILS&quot; 59.38 KB 540 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_POLICY_VIOL_CTXT_DEF&quot; 68.09 KB 642 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_STORAGE_REPORT_DATA&quot; 53.82 KB 240 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TASK_QTABLE&quot; 21.75 KB 28 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_COLUMN_EXCEPTIONS&quot; 5.968 KB 3 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOWS&quot; 59.70 KB 10 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_ACTIVITY_LOG_NUMBER$&quot; 5.468 KB 1 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_ALT_CONFIG_PICK&quot; 8.203 KB 37 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_APPLICATION_GROUPS&quot; 7.960 KB 1 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_CALS&quot; 18.85 KB 11 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_CAL_TEMPLATES&quot; 38.88 KB 9 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_CHARSETS&quot; 7.781 KB 32 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_CLICKTHRU_LOG_NUMBER$&quot; 5.882 KB 1 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_COMPANIES&quot; 8.835 KB 3 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_COMPANY_SCHEMAS&quot; 6.273 KB 1 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_COMPANY_TYPES&quot; 6.593 KB 32 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_COMPUTATIONS&quot; 14.92 KB 14 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_COUNTRIES&quot; 9.820 KB 240 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_DB_AUTH&quot; 5.460 KB 1 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_DEVELOPER_ROLES&quot; 5.921 KB 9 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_DUAL100&quot; 5.703 KB 100 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_FIELD_TEMPLATES&quot; 19.31 KB 36 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_FND_USER&quot; 23.80 KB 1 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_HNT_COLUMN_INFO&quot; 23.54 KB 58 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_HNT_TABLE_INFO&quot; 9.375 KB 8 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_ICON_BAR&quot; 19.25 KB 12 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_ITEMS&quot; 15.33 KB 89 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_LANGUAGES&quot; 16.15 KB 132 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_LANGUAGE_MAP&quot; 15.28 KB 90 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_LOV_VALUES&quot; 8.859 KB 6 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_MENUS&quot; 7.718 KB 7 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_MENU_TEMPLATES&quot; 14.14 KB 8 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_PAGE_GROUPS&quot; 14.28 KB 105 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_PASSWORD_HISTORY&quot; 6.671 KB 1 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_PATCHES&quot; 10.68 KB 9 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_PICK_END_USERS&quot; 6.015 KB 4 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_PICK_PAGE_VIEWS&quot; 6.093 KB 5 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_PLATFORM_PREFS&quot; 9.007 KB 21 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_POPUP_LOV_TEMPLATE&quot; 32.22 KB 10 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_QUERY_COLUMN&quot; 9.921 KB 18 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_QUERY_CONDITION&quot; 9.601 KB 6 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_QUERY_DEFINITION&quot; 7.640 KB 6 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_QUERY_OBJECT&quot; 8.304 KB 6 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_REGION_UPD_RPT_COLS&quot; 36.80 KB 439 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_RESTRICTED_SCHEMAS&quot; 8.234 KB 46 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_SECURITY_SCHEMES&quot; 15.28 KB 19 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_STANDARD_CSS&quot; 9.742 KB 27 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_STANDARD_ICONS&quot; 16.92 KB 319 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_STANDARD_JS&quot; 7.203 KB 2 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_SW_CREATE_KEYWORDS&quot; 6.867 KB 10 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_SW_MAIN_KEYWORDS&quot; 10.85 KB 199 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_SW_SET_KEYWORDS&quot; 6.75 KB 4 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_SW_SQLPLUS_CMD&quot; 5.101 KB 8 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_TABS&quot; 13.52 KB 3 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_TOPLEVEL_TABS&quot; 13.17 KB 5 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_TRANSLATABLE_COLS$&quot; 29.54 KB 232 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_TREES&quot; 25.79 KB 3 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_UPGRADE_PROGRESS&quot; 21.89 KB 89 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_UPG_TAB_NAME_CHANGES&quot; 8.773 KB 42 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_UPG_TAB_OBSOLETE&quot; 6.468 KB 17 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_USER_ACCESS_LOG_NUM$&quot; 5.882 KB 1 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_WORKSHEET_RPTS&quot; 37.01 KB 30 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_WORKSPACE_REQ_SIZE&quot; 6.851 KB 14 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_EXPORTER&quot; 7.382 KB 5 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_OLB_XMLTAGTABLEMAP&quot; 11.28 KB 45 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_XMLTAGTABLEMAP&quot; 10.54 KB 36 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_MENU_XMLTAGTABLEMAP&quot; 8.703 KB 7 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_RESERVED_WORDS&quot; 5.890 KB 87 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_RPT_XMLTAGTABLEMAP&quot; 9.132 KB 15 rows. . exported &quot;HR&quot;.&quot;COUNTRIES&quot; 6.367 KB 25 rows. . exported &quot;HR&quot;.&quot;DEPARTMENTS&quot; 7.007 KB 27 rows. . exported &quot;HR&quot;.&quot;EMPLOYEES&quot; 16.80 KB 107 rows. . exported &quot;HR&quot;.&quot;JOBS&quot; 6.992 KB 19 rows. . exported &quot;HR&quot;.&quot;JOB_HISTORY&quot; 7.054 KB 10 rows. . exported &quot;HR&quot;.&quot;LOCATIONS&quot; 8.273 KB 23 rows. . exported &quot;HR&quot;.&quot;REGIONS&quot; 5.476 KB 4 rows. . exported &quot;IX&quot;.&quot;AQ$_ORDERS_QUEUETABLE_S&quot; 10.91 KB 4 rows. . exported &quot;IX&quot;.&quot;AQ$_STREAMS_QUEUE_TABLE_S&quot; 11.17 KB 1 rows. . exported &quot;OE&quot;.&quot;CATEGORIES_TAB&quot; 14.15 KB 22 rows. . exported &quot;OE&quot;.&quot;SUBCATEGORY_REF_LIST_NESTEDTAB&quot; 6.585 KB 21 rows. . exported &quot;OE&quot;.&quot;PRODUCT_REF_LIST_NESTEDTAB&quot; 12.50 KB 288 rows. . exported &quot;OE&quot;.&quot;INVENTORIES&quot; 21.67 KB 1112 rows. . exported &quot;OE&quot;.&quot;ORDERS&quot; 12.38 KB 105 rows. . exported &quot;OE&quot;.&quot;ORDER_ITEMS&quot; 20.87 KB 665 rows. . exported &quot;OE&quot;.&quot;PROMOTIONS&quot; 5.5 KB 2 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_ANON_ACTION_TYPES&quot; 5.484 KB 4 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_ANON_ATTRS&quot; 8.703 KB 37 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_ANON_RULES&quot; 6.289 KB 3 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_ANON_RULE_TYPES&quot; 5.546 KB 3 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_CT_ACTION&quot; 6.664 KB 7 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_CT_DAREFS&quot; 6.187 KB 72 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_CT_LOCATORPATHS&quot; 10.07 KB 95 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_CT_MACRO_DEP&quot; 5.460 KB 1 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_CT_MACRO_PAR&quot; 5.476 KB 2 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_CT_PRED&quot; 8.320 KB 61 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_CT_PRED_PAR&quot; 5.937 KB 3 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_CT_PRED_SET&quot; 8.906 KB 9 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_DATA_MODEL&quot; 5.859 KB 1 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_DICT_ATTRS&quot; 33.95 KB 2418 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_DOC_REFS&quot; 6.5 KB 7 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_DOC_TYPES&quot; 6.976 KB 8 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_INSTALL_DOCS&quot; 6.539 KB 9 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_INTERNAL_TAGS&quot; 6.210 KB 42 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_PREFS_VALID_VALUES_TAB&quot; 6.492 KB 33 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_PREFS_LOOKUP&quot; 9.789 KB 13 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_PREFS_DEF_VALUES_TAB&quot; 5.679 KB 7 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_PRV_ATTRS&quot; 9.804 KB 3 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_RT_PREF_PARAMS&quot; 8.375 KB 13 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_UID_DEFS&quot; 33.49 KB 245 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_VR_DT_MAP&quot; 6.507 KB 32 rows. . exported &quot;SCOTT&quot;.&quot;DEPT&quot; 5.929 KB 4 rows. . exported &quot;SCOTT&quot;.&quot;EMP&quot; 8.562 KB 14 rows. . exported &quot;SCOTT&quot;.&quot;SALGRADE&quot; 5.859 KB 5 rows. . exported &quot;SCOTT&quot;.&quot;SCOTT_TABLE&quot; 5.468 KB 4 rows. . exported &quot;SCOTT&quot;.&quot;TEST&quot; 5.062 KB 5 rows. . exported &quot;SCOTT&quot;.&quot;TEST_DUMP&quot; 5.156 KB 20 rows. . exported &quot;SH&quot;.&quot;CAL_MONTH_SALES_MV&quot; 6.312 KB 48 rows. . exported &quot;SH&quot;.&quot;CHANNELS&quot; 7.25 KB 5 rows. . exported &quot;SH&quot;.&quot;COUNTRIES&quot; 10.20 KB 23 rows. . exported &quot;SH&quot;.&quot;PRODUCTS&quot; 26.17 KB 72 rows. . exported &quot;SYSMAN&quot;.&quot;AQ$_MGMT_LOADER_QTABLE_S&quot; 10.81 KB 2 rows. . exported &quot;SYSMAN&quot;.&quot;AQ$_MGMT_NOTIFY_QTABLE_S&quot; 10.81 KB 2 rows. . exported &quot;SYSMAN&quot;.&quot;EMDW_TRACE_CONFIG&quot; 7.054 KB 9 rows. . exported &quot;SYSMAN&quot;.&quot;EM_PAGE_CONDITION_METADATA&quot; 5.640 KB 7 rows. . exported &quot;SYSMAN&quot;.&quot;EM_PAGE_CUST_METADATA&quot; 6.031 KB 4 rows. . exported &quot;SYSMAN&quot;.&quot;EUME2E_ASSOCS_LOOKUP&quot; 6.601 KB 9 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ADMIN_LICENSES&quot; 5.296 KB 19 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ALL_TARGET_PROPS&quot; 9.773 KB 5 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ARU_FAMILY_PRODUCT_MAP&quot; 25.85 KB 1660 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ARU_LANGUAGES&quot; 6.156 KB 40 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ARU_OUI_COMPONENTS&quot; 21.33 KB 393 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ARU_PLATFORMS&quot; 9.578 KB 76 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ARU_PRODUCTS&quot; 34.71 KB 744 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ARU_RELEASES&quot; 19.89 KB 863 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_AUDIT_DESTINATION&quot; 5.492 KB 1 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_AUDIT_MASTER&quot; 5.070 KB 1 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_AVAILABILITY&quot; 13.10 KB 138 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_AVAILABILITY_MARKER&quot; 6.015 KB 5 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_AVAILABLE_SEARCHES&quot; 7.023 KB 15 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BLACKOUT_PROXY_TARGETS&quot; 5.140 KB 5 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BLACKOUT_REASON&quot; 9.617 KB 60 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BSLN_METRICS&quot; 7.351 KB 6 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CALLBACKS&quot; 12.19 KB 87 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CATEGORIES&quot; 6.226 KB 12 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CATEGORY_CLASSES&quot; 5.460 KB 2 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_COLLECTION_METRIC_TASKS&quot; 20.5 KB 209 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_COLL_ITEMS&quot; 26.13 KB 264 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_COLL_ITEM_PROPERTIES&quot; 10.67 KB 48 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CORRECTIVE_ACTION&quot; 6.734 KB 2 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CREATED_USERS&quot; 5.976 KB 4 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CREDENTIALS2&quot; 7.742 KB 3 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CREDENTIAL_SETS&quot; 12.03 KB 35 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CREDENTIAL_SET_COLUMNS&quot; 15.15 KB 86 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CREDENTIAL_SET_COL_VALS&quot; 7.953 KB 8 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CREDENTIAL_TYPES&quot; 8.625 KB 19 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CREDENTIAL_TYPE_COLUMNS&quot; 12.24 KB 45 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CREDENTIAL_TYPE_COL_VALS&quot; 7.648 KB 3 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CREDENTIAL_TYPE_REF&quot; 8.476 KB 9 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CS_CONFIG_STANDARD&quot; 15.92 KB 3 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CS_HIERARCHY&quot; 11.29 KB 117 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CS_KEYWORD&quot; 5.554 KB 3 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CS_RULE&quot; 31.97 KB 94 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CS_RULEFOLDER&quot; 9.328 KB 23 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CURRENT_AVAILABILITY&quot; 6.437 KB 5 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CURRENT_METRIC_ERRORS&quot; 7.75 KB 2 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DB_CONTROLFILES_ECM&quot; 12.46 KB 44 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DB_DATAFILES_ECM&quot; 23.84 KB 154 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DB_DBNINSTANCEINFO_ECM&quot; 14.55 KB 24 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DB_FEATUREUSAGE&quot; 33.64 KB 161 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DB_HDM_METRIC_HELPER&quot; 6.687 KB 1 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DB_LICENSE_ECM&quot; 7.812 KB 24 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DB_OPTIONS_ECM&quot; 15.5 KB 264 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DB_RECTSSETTINGS_ECM&quot; 7.523 KB 22 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DB_RECUSERSETTINGS_ECM&quot; 7.101 KB 22 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DB_REDOLOGS_ECM&quot; 16.07 KB 72 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DB_ROLLBACK_SEGS_ECM&quot; 13.42 KB 24 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DB_SGA_ECM&quot; 14.60 KB 216 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DB_TABLESPACES_ECM&quot; 29.10 KB 154 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DELTA_ENTRY&quot; 16.14 KB 116 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DELTA_ENTRY_VALUES&quot; 33.67 KB 547 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DELTA_IDS&quot; 11.62 KB 45 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DELTA_ID_VALUES&quot; 10.14 KB 92 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DELTA_SNAP&quot; 15.20 KB 43 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DEPLOYMENT_SECTIONS&quot; 5.5 KB 1 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DM_ALITEMS&quot; 7.046 KB 48 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DM_RULEENTRY&quot; 12.51 KB 45 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DM_RULETEMPLATES&quot; 9.882 KB 19 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_ARU_MAP&quot; 8.164 KB 29 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_CSA_OUT_OF_BOX&quot; 5.023 KB 1 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_GEN_SNAPSHOT&quot; 27.94 KB 146 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_MD_HIST_TBLS&quot; 19.63 KB 39 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_RESOURCES&quot; 6.804 KB 14 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_SNAPSHOT&quot; 11.25 KB 22 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_SNAPSHOT_MD_TABLES&quot; 21.80 KB 122 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_SNAPSHOT_METADATA&quot; 17.28 KB 53 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_SNAP_COMPONENT_INFO&quot; 16.71 KB 84 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_EMD_PING&quot; 11.41 KB 1 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_EMD_PING_CHECK&quot; 5.460 KB 1 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ERROR_MASTER&quot; 6.453 KB 12 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_FAILOVER_CALLBACKS&quot; 5.179 KB 4 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_FAILOVER_TABLE&quot; 6.765 KB 1 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_FBP_PATCHING_GUIDS&quot; 5.609 KB 3 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_FLAT_TARGET_ASSOC&quot; 7.828 KB 3 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_GROUP_DEFAULT_CHART&quot; 10.25 KB 9 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_HA_INFO_ECM&quot; 8.796 KB 24 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_HA_MTTR&quot; 5.859 KB 1 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_HA_RMAN_CONFIG_ECM&quot; 6.859 KB 24 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_HC_CPU_DETAILS&quot; 9.218 KB 21 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_HC_HARDWARE_MASTER&quot; 11.14 KB 21 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_HC_NIC_DETAILS&quot; 15.12 KB 63 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_HC_OS_SUMMARY&quot; 10.57 KB 21 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_HC_SYSTEM_SUMMARY&quot; 8.007 KB 21 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_HTTP_SESSION_CALLBACKS&quot; 5.492 KB 1 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_INV_CONTAINER&quot; 10.90 KB 42 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_INV_CONTAINER_PROPERTY&quot; 8.531 KB 63 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_INV_SUMMARY&quot; 13.37 KB 42 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_IP_ELEM_PARAM_CLASSES&quot; 19.90 KB 116 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_IP_ELEM_TARGET_TYPES&quot; 9.25 KB 86 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_IP_REPORT_DEF&quot; 37.09 KB 105 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_IP_REPORT_DEF_ELEMENTS&quot; 38.97 KB 300 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_IP_REPORT_DEF_JIT_TYPES&quot; 8.406 KB 93 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_IP_REPORT_ELEM_DEF&quot; 20.96 KB 77 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB&quot; 14.44 KB 4 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_CALLBACKS&quot; 6.281 KB 6 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_COMMAND&quot; 17.67 KB 136 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_DISPLAY_ERROR_CODES&quot; 6.765 KB 8 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_EVENT&quot; 5.476 KB 1 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_EXECUTION&quot; 17.05 KB 8 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_EXEC_SUMMARY&quot; 16.80 KB 24 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_HISTORY&quot; 21.59 KB 41 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_LOCK_INFO&quot; 8.257 KB 5 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_LOCK_TARGETS&quot; 7.078 KB 5 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_PURGE_CRITERIA&quot; 6.453 KB 2 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_PURGE_POLICIES&quot; 5.523 KB 3 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_PURGE_VALUES&quot; 5.984 KB 2 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_SINGLE_TARGET_TYPES&quot; 9.093 KB 120 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_STATE_CHANGES&quot; 14.06 KB 64 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_TYPE_DISPLAY_INFO&quot; 11.78 KB 83 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_TYPE_INFO&quot; 34.12 KB 155 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_TYPE_MAX_VERSIONS&quot; 13.26 KB 150 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_TYPE_PARAM_DSPLY_INFO&quot; 31.05 KB 304 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_TYPE_PARAM_URI_INFO&quot; 8.148 KB 7 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_TYPE_URI_INFO&quot; 11.16 KB 79 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_VALUE_PARAMS&quot; 12.15 KB 140 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_LAST_SYNC_LOAD_DETAILS&quot; 5.875 KB 1 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_LICENSABLE_TARGET_TYPES&quot; 7.117 KB 49 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_LICENSED_TARGETS&quot; 6.296 KB 10 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_LOADER_DESIGNATORS&quot; 5.773 KB 40 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_LOGIN_ASSISTANTS&quot; 6.046 KB 2 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_MASTER_CHANGED_CALLBACK&quot; 5.898 KB 1 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_METADATA_SETS&quot; 9.984 KB 76 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_METRICS_COMPOSITE_KEYS&quot; 37.71 KB 367 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_METRICS_EXT&quot; 10.21 KB 20 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_METRIC_DEPENDENCY_DEF&quot; 8.062 KB 16 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_METRIC_VERSIONS&quot; 17.10 KB 273 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_MP_HOMEPAGE_REPORTS&quot; 7.789 KB 30 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_NESTED_JOB_TARGETS&quot; 12.01 KB 61 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_NOTIFY_FORMAT_HANDLERS&quot; 5.804 KB 6 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_NOTIFY_JOB_RULE_CONFIGS&quot; 10.22 KB 1 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_NOTIFY_PROFILES&quot; 6.718 KB 3 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_NOTIFY_QUEUES&quot; 6.226 KB 27 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_NOTIFY_RULES&quot; 8.320 KB 7 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_NOTIFY_RULE_CONFIGS&quot; 25.31 KB 41 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_OMS_PARAMETERS&quot; 10.64 KB 61 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_OPERATIONS_MASTER&quot; 13.85 KB 25 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_OUI_ARU_MAP&quot; 5.804 KB 27 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PAF_APPLICATIONS&quot; 11.32 KB 4 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PAF_JOBTYPES&quot; 10.17 KB 2 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PAF_JOBTYPE_PARAMS&quot; 14.94 KB 16 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PAF_PARAM_GROUPS&quot; 10.79 KB 6 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PAF_PROCEDURES&quot; 13.58 KB 2 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PAF_TEXTUAL_DATA&quot; 24.24 KB 2 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PARAMETERS&quot; 13.78 KB 84 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PDP_COLUMN_METADATA&quot; 5.968 KB 3 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PDP_METADATA&quot; 5.906 KB 2 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PDP_PARAM_METADATA&quot; 7.007 KB 9 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PDP_SETTING_METADATA&quot; 7.929 KB 3 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PERFORMANCE_NAMES&quot; 11.70 KB 106 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_POLICY_ASSOC_EVAL_SUMM&quot; 20.96 KB 167 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_POLICY_BIND_VARS&quot; 12.49 KB 169 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_POLICY_PARAMETERS&quot; 8.117 KB 27 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_POLICY_TYPE_VERSIONS&quot; 20.38 KB 585 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PRIVS&quot; 8.265 KB 24 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PRIV_GRANTS&quot; 12.69 KB 113 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PRIV_INCLUDES&quot; 5.812 KB 13 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PROV_COLLECTION&quot; 5.632 KB 21 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PROV_HARDWARE&quot; 11.20 KB 1 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PROV_TGT_STATUS&quot; 7.5 KB 1 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PURGE_POLICY&quot; 10.57 KB 16 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PURGE_POLICY_GROUP&quot; 6.140 KB 7 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PURGE_POLICY_TARGET_STATE&quot; 10.52 KB 36 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_REBUILD_INDEXES&quot; 5.859 KB 1 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ROLES&quot; 5.429 KB 1 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ROLE_GRANTS&quot; 6.343 KB 1 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ROWSET_HANDLERS&quot; 7.195 KB 14 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RT_BOOTSTRAP_TIMES&quot; 5.578 KB 5 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_SNAPSHOT_METRIC_MAP&quot; 19.13 KB 221 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_SPACE_METRICS&quot; 8.406 KB 1 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_STORAGE_REPORT_ALIAS&quot; 22.09 KB 240 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_STORAGE_REPORT_ISSUES&quot; 17.25 KB 100 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_STORAGE_REPORT_KEYS&quot; 21.83 KB 260 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_STRING_METRIC_HISTORY&quot; 12.94 KB 85 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_SWLIB_DIRECTORIES&quot; 7.398 KB 5 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_SYSTEM_ERROR_LOG&quot; 9.179 KB 3 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TARGETS&quot; 16.54 KB 5 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TARGET_ADD_CALLBACKS&quot; 5.976 KB 13 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TARGET_ASSOCS&quot; 7.539 KB 6 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TARGET_ASSOC_DEFS&quot; 11.03 KB 15 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TARGET_ASSOC_ERROR&quot; 6.484 KB 4 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TARGET_ASSOC_STATUS&quot; 5.953 KB 4 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TARGET_CREDENTIALS&quot; 6.710 KB 1 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TARGET_DELETE_EXCEPTIONS&quot; 5.945 KB 35 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TARGET_PROPERTIES&quot; 17.14 KB 204 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TARGET_PROP_DEFS&quot; 38.64 KB 288 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TARGET_ROLLUP_TIMES&quot; 12.26 KB 130 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TARGET_TYPES&quot; 9.718 KB 28 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TARGET_TYPE_COMPONENT_MAP&quot; 6.453 KB 5 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TARGET_TYPE_VERSIONS&quot; 11.92 KB 37 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TASK_WORKER_COUNTS&quot; 5.507 KB 2 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TYPE_PROPERTIES&quot; 8.578 KB 75 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_USER_CALLBACKS&quot; 6.5 KB 27 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_USER_CAS&quot; 5.460 KB 2 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_USER_CONTEXT&quot; 5.898 KB 4 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_USER_FOLDERS&quot; 6.835 KB 24 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_USER_SUBTAB_COL_PREFS&quot; 8.687 KB 41 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_VERSIONS&quot; 6.835 KB 5 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_VIEW_USER_CREDENTIALS&quot; 5.515 KB 1 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_VIOLATION_CONTEXT&quot; 32.64 KB 233 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_AUDIT_ATTRIBUTE&quot; 6.328 KB 2 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_OBJECT_TYPES&quot; 6.882 KB 28 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_RESOLUTION_METHOD&quot; 5.835 KB 19 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_TEMPLATE_STATUS&quot; 5.484 KB 3 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_TEMPLATE_TYPES&quot; 6.289 KB 2 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOWS_RESERVED&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_ACTIVITY_LOG1$&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_ACTIVITY_LOG2$&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_ALTERNATE_CONFIG&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_ALT_CONFIG_DETAIL&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_APP_BUILD_PREF&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_APP_COMMENTS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_BUILDER_AUDIT_TRAIL&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_CLICKTHRU_LOG$&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_CLICKTHRU_LOG2$&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_COLLECTIONS$&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_COLLECTION_MEMBERS$&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_COMPOUND_CONDITIONS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_CSS_REPOSITORY&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_CUSTOMIZED_TASKS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_DATA&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_DATA_LOAD_BAD_LOG&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_DATA_LOAD_UNLOAD&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_DEBUG&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_DEVELOPERS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_DYNAMIC_TRANSLATIONS$&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_EFFECTIVE_USERID_MAP&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_ENTRY_POINTS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_ENTRY_POINT_ARGS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_FILE_OBJECTS$PART&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_FND_GROUP_USERS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_FND_USER_GROUPS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_FOLDERS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_HNT_ARGUMENT_INFO&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_HNT_LOV_DATA&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_HNT_PROCEDURE_INFO&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_HTML_REPOSITORY&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_ICON_BAR_ATTRIBUTES&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_IMAGE_REPOSITORY&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_IMPORT_EXPORT&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_INSTALL_BUILD_OPT&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_INSTALL_CHECKS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_INSTALL_SCRIPTS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_JOBS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_JOB_BIND_VALUES&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_LOCK_PAGE&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_LOCK_PAGE_LOG&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_MAIL_ATTACHMENTS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_MAIL_LOG&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_MAIL_QUEUE&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_MODELS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_MODEL_PAGES&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_MODEL_PAGE_COLS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_MODEL_PAGE_REGIONS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_ONLINE_HELP&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_ONLINE_HELP_JA&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_PAGES_RESERVED&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_PAGE_CACHE&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_PAGE_SUBMISSIONS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_PICK_DATABASE_SIZE&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_PLATFORM_PREF&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_PREFERENCES$&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_PROVISION_COMPANY&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_PROVISION_SERICE_MOD&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_PURGED_SESSIONS$&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_QB_SAVED_COND&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_QB_SAVED_JOIN&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_QB_SAVED_QUERY&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_QB_SAVED_TABS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_REGION_CHART_SER_ATTR&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_REGION_REPORT_FILTER&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_REPORT_LAYOUTS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_REQUEST_VERIFICATIONS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_REQUIRED_ROLES&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_RSCHEMA_EXCEPTIONS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_SC_TRANS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_SESSIONS$&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_SHARED_QRY_SQL_STMTS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_SHARED_QUERIES&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_SHARED_WEB_SERVICES&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_SHORTCUT_USAGE_MAP&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_STEP_BRANCH_ARGS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_SW_BINDS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_SW_DETAIL_RESULTS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_SW_RESULTS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_SW_SQL_CMDS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_SW_STMTS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_TEMPLATES$&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_TEMPLATE_PREFERENCES&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_TEMPLATE_THEMES$&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_TRANSLATABLE_TEXT$&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_TREE_STATE&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_UPG_COL_NAME_CHANGES&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_USER_ACCESS_LOG1$&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_USER_ACCESS_LOG2$&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_VERSION$&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_WEB_PAGES&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_WEB_PG_LIST_ENTRIES&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_WEB_PG_REGIONS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_WORKSHEET_CATEGORIES&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_WORKSHEET_COL_GROUPS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_WORKSHEET_COMPUTATION&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_WORKSHEET_CONDITIONS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_WORKSHEET_DOCS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_WORKSHEET_GEOCACHE&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_WORKSHEET_HISTORY&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_WORKSHEET_LINKS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_WORKSHEET_LOVS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_WORKSHEET_LOV_ENTRIES&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_WORKSHEET_PRIVS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_WORKSHEET_ROWS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_WORKSHEET_STICK&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_WS_OPERATIONS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_WS_PARAMETERS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_FLOW_WS_PROCESS_PARMS_MAP&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_ACCESS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_ACC_COLUMNS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_ACC_FORMS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_ACC_FORMS_CONTROLS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_ACC_FORMS_MODULES&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_ACC_FORMS_PERM&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_ACC_GROUPS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_ACC_GRPSMEMBERS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_ACC_INDEXES&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_ACC_INDEXES_COLS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_ACC_MODULES&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_ACC_MODULES_PERM&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_ACC_PAGES&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_ACC_QUERIES&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_ACC_RELATIONS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_ACC_RELATION_COLS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_ACC_REPORTS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_ACC_RPTS_CONTROLS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_ACC_RPTS_MODULES&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_ACC_RPT_PERMS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_ACC_TABLES&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_ACC_TAB_PERM&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_ACC_USERS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FORMS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_ALERTS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_ATTACHEDLIBRARY&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_BLK_DSA&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_BLK_DSC&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_BLK_ITEMS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_BLK_ITEM_LIE&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_BLK_ITEM_RADIO&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_BLK_ITEM_TRIGGERS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_BLK_RELATIONS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_BLK_TRIGGERS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_BLOCKS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_CANVAS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_CNVG_COMPOUNDTEXT&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_CNVS_GRAPHICS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_CNVS_TABPAGE&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_COORDINATES&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_CPDTXT_TEXTSEGMENT&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_EDITOR&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_FMB_MENU&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_FMB_MENUITEM_ROLE&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_FMB_MENU_MENUITEM&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_FORMMODULES&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_LOV&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_LOVCOLUMNMAPPING&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_MENU&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_MENUITEM_ROLE&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_MENUS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_MENUSMODULEROLES&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_MENUS_MENUMODULES&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_MENUS_MODULES&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_MENUS_PROGRAMUNIT&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_MENU_MENUITEM&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_MODULEPARAMETER&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_MODULES&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_OBJECTGROUP&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_OBJECTGROUPCHILD&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_PROGRAMUNIT&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_PROPERTYCLASS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_RECORDGROUPCOLUMN&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_RECORDGROUPS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_REPORT&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_REV_APEX_APP&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_REV_BLK_ITEMS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_REV_BLOCKS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_REV_FORMMODULES&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_REV_LOV&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_REV_LOVCOLMAPS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_TRIGGERS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_VISUALATTRIBUTES&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_FRM_WINDOWS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_GENERATED_APPLICATIONS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_BLK_DATASOURCECOL&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_BLK_ITEM&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_BLK_ITEM_LIE&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_BLK_ITEM_TRIGGER&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_BLK_TRIGGER&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_BLOCK&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_CANVAS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_CG_COMPOUNDTEXT&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_CG_CT_TEXTSEGMENT&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_CNVS_GRAPHICS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_MODULES&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_OBJECTLIBRARY&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_OBJECTLIBRARYTAB&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_OLT_ALERT&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_OLT_BLK_ITEM&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_OLT_BLK_ITEM_TRIGR&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_OLT_BLOCK&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_OLT_CANVAS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_OLT_CNVS_GRAPHICS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_OLT_GRAPHICS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_OLT_ITEM&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_OLT_MENU&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_OLT_MENU_MENUITEM&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_OLT_OBJECTGROUP&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_OLT_OB_OBJGRPCHILD&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_OLT_REPORT&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_OLT_TABPAGE&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_OLT_TABPG_GRAPHICS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_OLT_VISUALATTRBUTE&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_OLT_WINDOW&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_PROGRAMUNIT&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_PROPERTYCLASS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_T_TP_GGGGG_CPDTXT&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_T_TP_GGGGG_CT_TXST&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_T_TP_GGGG_CPDTXT&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_T_TP_GGGG_CT_TXSGT&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_T_TP_GGGG_GRAPHICS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_T_TP_GGG_CPDTXT&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_T_TP_GGG_CT_TXTSGT&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_T_TP_GGG_GRAPHICS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_T_TP_GG_CPDTXT&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_T_TP_GG_CT_TXTSGT&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_T_TP_GG_GRAPHICS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_T_TP_G_GRAPHICS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_VISUALATTRIBUTE&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_OLB_WINDOW&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_PLSQL_LIBS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_PROJECTS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_PROJECT_COMPONENTS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_PROJECT_TRIGGERS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_REPORT&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_REV_APEXAPP&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_REV_FORMS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_REV_QUERIES&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_REV_REPORTS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_REV_TABLES&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_RPTS&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_RPT_DATA&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_RPT_DATASRC&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_RPT_DATASRC_GRP&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_RPT_DATASRC_SELECT&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_RPT_DATA_SUMMARY&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_RPT_GRP_DATAITEM&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_RPT_GRP_DATAITEM_DESC&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_RPT_GRP_DATAITEM_PRIV&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_RPT_GRP_FIELD&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_RPT_GRP_FILTER&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_RPT_GRP_FORMULA&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_RPT_GRP_ROWDELIM&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_RPT_GRP_SUMMARY&quot; 0 KB 0 rows. . exported &quot;APEX_030200&quot;.&quot;WWV_MIG_RPT_REPORTPRIVATE&quot; 0 KB 0 rows. . exported &quot;FLOWS_FILES&quot;.&quot;WWV_FLOW_FILE_OBJECTS$&quot; 0 KB 0 rows. . exported &quot;IX&quot;.&quot;AQ$_ORDERS_QUEUETABLE_G&quot; 0 KB 0 rows. . exported &quot;IX&quot;.&quot;AQ$_ORDERS_QUEUETABLE_H&quot; 0 KB 0 rows. . exported &quot;IX&quot;.&quot;AQ$_ORDERS_QUEUETABLE_I&quot; 0 KB 0 rows. . exported &quot;IX&quot;.&quot;AQ$_ORDERS_QUEUETABLE_L&quot; 0 KB 0 rows. . exported &quot;IX&quot;.&quot;AQ$_ORDERS_QUEUETABLE_T&quot; 0 KB 0 rows. . exported &quot;IX&quot;.&quot;AQ$_STREAMS_QUEUE_TABLE_C&quot; 0 KB 0 rows. . exported &quot;IX&quot;.&quot;AQ$_STREAMS_QUEUE_TABLE_G&quot; 0 KB 0 rows. . exported &quot;IX&quot;.&quot;AQ$_STREAMS_QUEUE_TABLE_H&quot; 0 KB 0 rows. . exported &quot;IX&quot;.&quot;AQ$_STREAMS_QUEUE_TABLE_I&quot; 0 KB 0 rows. . exported &quot;IX&quot;.&quot;AQ$_STREAMS_QUEUE_TABLE_L&quot; 0 KB 0 rows. . exported &quot;IX&quot;.&quot;AQ$_STREAMS_QUEUE_TABLE_T&quot; 0 KB 0 rows. . exported &quot;IX&quot;.&quot;ORDERS_QUEUETABLE&quot; 0 KB 0 rows. . exported &quot;IX&quot;.&quot;STREAMS_QUEUE_TABLE&quot; 0 KB 0 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_ANON_ATTRS_WRK&quot; 0 KB 0 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_ANON_RULES_WRK&quot; 0 KB 0 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_CT_ACTION_WRK&quot; 0 KB 0 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_CT_DAREFS_WRK&quot; 0 KB 0 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_CT_LOCATORPATHS_WRK&quot; 0 KB 0 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_CT_MACRO_DEP_WRK&quot; 0 KB 0 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_CT_MACRO_PAR_WRK&quot; 0 KB 0 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_CT_PRED_OPRD_WRK&quot; 0 KB 0 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_CT_PRED_PAR_WRK&quot; 0 KB 0 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_CT_PRED_SET_WRK&quot; 0 KB 0 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_CT_PRED_WRK&quot; 0 KB 0 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_CT_VLD_MSG&quot; 0 KB 0 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_DATA_MODEL_WRK&quot; 0 KB 0 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_DICT_ATTRS_WRK&quot; 0 KB 0 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_DOCS_WRK&quot; 0 KB 0 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_DOC_REFS_WRK&quot; 0 KB 0 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_MAPPED_PATHS&quot; 0 KB 0 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_MAPPED_PATHS_WRK&quot; 0 KB 0 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_MAPPING_DOCS_WRK&quot; 0 KB 0 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_PRV_ATTRS_WRK&quot; 0 KB 0 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_RT_PREF_PARAMS_WRK&quot; 0 KB 0 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_STD_ATTRS_WRK&quot; 0 KB 0 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_STORED_TAGS&quot; 0 KB 0 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_STORED_TAGS_WRK&quot; 0 KB 0 rows. . exported &quot;ORDDATA&quot;.&quot;ORDDCM_UID_DEFS_WRK&quot; 0 KB 0 rows. . exported &quot;OUTLN&quot;.&quot;OL$&quot; 0 KB 0 rows. . exported &quot;OUTLN&quot;.&quot;OL$HINTS&quot; 0 KB 0 rows. . exported &quot;OUTLN&quot;.&quot;OL$NODES&quot; 0 KB 0 rows. . exported &quot;OWBSYS&quot;.&quot;OWBRTPS&quot; 0 KB 0 rows. . exported &quot;SCOTT&quot;.&quot;BONUS&quot; 0 KB 0 rows. . exported &quot;SH&quot;.&quot;COSTS&quot;:&quot;COSTS_1995&quot; 0 KB 0 rows. . exported &quot;SH&quot;.&quot;COSTS&quot;:&quot;COSTS_1996&quot; 0 KB 0 rows. . exported &quot;SH&quot;.&quot;COSTS&quot;:&quot;COSTS_H1_1997&quot; 0 KB 0 rows. . exported &quot;SH&quot;.&quot;COSTS&quot;:&quot;COSTS_H2_1997&quot; 0 KB 0 rows. . exported &quot;SH&quot;.&quot;COSTS&quot;:&quot;COSTS_Q1_2002&quot; 0 KB 0 rows. . exported &quot;SH&quot;.&quot;COSTS&quot;:&quot;COSTS_Q1_2003&quot; 0 KB 0 rows. . exported &quot;SH&quot;.&quot;COSTS&quot;:&quot;COSTS_Q2_2002&quot; 0 KB 0 rows. . exported &quot;SH&quot;.&quot;COSTS&quot;:&quot;COSTS_Q2_2003&quot; 0 KB 0 rows. . exported &quot;SH&quot;.&quot;COSTS&quot;:&quot;COSTS_Q3_2002&quot; 0 KB 0 rows. . exported &quot;SH&quot;.&quot;COSTS&quot;:&quot;COSTS_Q3_2003&quot; 0 KB 0 rows. . exported &quot;SH&quot;.&quot;COSTS&quot;:&quot;COSTS_Q4_2002&quot; 0 KB 0 rows. . exported &quot;SH&quot;.&quot;COSTS&quot;:&quot;COSTS_Q4_2003&quot; 0 KB 0 rows. . exported &quot;SH&quot;.&quot;DIMENSION_EXCEPTIONS&quot; 0 KB 0 rows. . exported &quot;SH&quot;.&quot;SALES&quot;:&quot;SALES_1995&quot; 0 KB 0 rows. . exported &quot;SH&quot;.&quot;SALES&quot;:&quot;SALES_1996&quot; 0 KB 0 rows. . exported &quot;SH&quot;.&quot;SALES&quot;:&quot;SALES_H1_1997&quot; 0 KB 0 rows. . exported &quot;SH&quot;.&quot;SALES&quot;:&quot;SALES_H2_1997&quot; 0 KB 0 rows. . exported &quot;SH&quot;.&quot;SALES&quot;:&quot;SALES_Q1_2002&quot; 0 KB 0 rows. . exported &quot;SH&quot;.&quot;SALES&quot;:&quot;SALES_Q1_2003&quot; 0 KB 0 rows. . exported &quot;SH&quot;.&quot;SALES&quot;:&quot;SALES_Q2_2002&quot; 0 KB 0 rows. . exported &quot;SH&quot;.&quot;SALES&quot;:&quot;SALES_Q2_2003&quot; 0 KB 0 rows. . exported &quot;SH&quot;.&quot;SALES&quot;:&quot;SALES_Q3_2002&quot; 0 KB 0 rows. . exported &quot;SH&quot;.&quot;SALES&quot;:&quot;SALES_Q3_2003&quot; 0 KB 0 rows. . exported &quot;SH&quot;.&quot;SALES&quot;:&quot;SALES_Q4_2002&quot; 0 KB 0 rows. . exported &quot;SH&quot;.&quot;SALES&quot;:&quot;SALES_Q4_2003&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;AQ$_MGMT_LOADER_QTABLE_G&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;AQ$_MGMT_LOADER_QTABLE_H&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;AQ$_MGMT_LOADER_QTABLE_I&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;AQ$_MGMT_LOADER_QTABLE_L&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;AQ$_MGMT_LOADER_QTABLE_T&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;AQ$_MGMT_NOTIFY_QTABLE_G&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;AQ$_MGMT_NOTIFY_QTABLE_H&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;AQ$_MGMT_NOTIFY_QTABLE_I&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;AQ$_MGMT_NOTIFY_QTABLE_L&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;AQ$_MGMT_NOTIFY_QTABLE_T&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;DB_USER_PREFERENCES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;EMDW_TRACE_DATA&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;EM_COMPARISON_SUMMARY&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;EM_IPW_INFO&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;EM_PAGE_CUSTOMIZATIONS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;EM_PAGE_CUSTOM_CONDITIONS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ADMIN_METRIC_THRESHOLDS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ANNOTATION&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ARU_CREDENTIALS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_AUDIT_CUSTOM_ATTRIBS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_AUDIT_LOGS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_AVAILABILITY_RBK&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BACKUP_CONFIGURATION&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BAM_DATA_HUBS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BAM_DATA_ISESSIONS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BAM_DATA_OSESSIONS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BAM_ISESSION_DATASOURCE&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BAM_ISESSION_DIAG&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BAM_ISESSION_KPIS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BAM_OSESSION_ALERTS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BAM_OSESSION_DIAG&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BAM_OSESSION_METRICS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BAM_OSESSION_STATUS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BCN_AVAIL_DEF&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BCN_AVAIL_JOB&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BCN_AVAIL_LOG&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BCN_BCNSTEP_PROPS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BCN_BCNTXN_PROPS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BCN_STEPGROUP_DEFN&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BCN_STEPGROUP_STEPS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BCN_STEP_DEFN&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BCN_STEP_PROPS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BCN_TARGET&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BCN_TXN_AUDIT&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BCN_TXN_DEFN&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BCN_TXN_PROPS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BLACKOUTS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BLACKOUT_FLAT_TARGETS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BLACKOUT_HISTORY&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BLACKOUT_SCHEDULE&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BLACKOUT_STATE&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BLACKOUT_TARGET_DETAILS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BLACKOUT_WINDOWS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BSLN_BASELINES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BSLN_DATASOURCES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BSLN_INTERVALS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BSLN_RAWDATA&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BSLN_STATISTICS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BSLN_THRESHOLD_PARMS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BUG_ADVISORY&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BUG_ADVISORY_BUG&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BUG_ADV_HOME_PATCH&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BUG_AVAILABLE_PATCH&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BUG_FIX_APPLICABLE_COMP&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BUG_FIX_APPLIC_COMP_LIST&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BUG_PATCH_CERTIFICATE&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BUG_PATCH_FIXES_BUG&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_BUG_PATCH_PLATFORM&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CHANGE_AGENT_URL&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CM_BASELINES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CM_BASELINE_CONS_GROUPS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CM_BASELINE_DEPENDENCIES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CM_BASELINE_DEPENDENTS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CM_BASELINE_INIT_PARAMS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CM_BASELINE_OBJECTS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CM_BASELINE_OBJGRANTS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CM_BASELINE_PROXYGRANTS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CM_BASELINE_QUOTAGRANTS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CM_BASELINE_ROLEGRANTS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CM_BASELINE_SYSGRANTS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CM_BASELINE_VERSIONS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CM_COMPARISONS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CM_COMPARISON_INIT_PRMS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CM_COMPARISON_OBJECTS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CM_COMPARISON_VERSIONS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CM_SCHEMA_MAPS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CM_SCOPESPECS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CM_SCOPESPEC_NAMES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CM_SYNCHRONIZATIONS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CM_SYNCH_IMPACT_REPORTS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CM_SYNCH_OBJECTS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CM_SYNCH_SCRIPTS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CM_SYNCH_VERSIONS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_COLLECTION_CREDENTIALS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_COLLECTION_TASK_CONTEXT&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_COLLECTION_TEMPLATE_CREDS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_COLLECTION_WORKERS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_COMP_RESULT_TO_JOB_MAP&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_COMP_SNAPSHOT_TO_STEP_MAP&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CONFIG_ACTIVITIES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CONTAINER_CREDENTIALS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CPF_METRIC_SOURCE&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CREDENTIALS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CSTMZ_CHARTS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CSTMZ_CHART_SELTARGETS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CSTMZ_CUSTOM_COLUMNS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CSTMZ_DEFAULT_CHART&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CSTMZ_SUMMARY_CHART_DEF&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CS_EVAL_SUMM_RQS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CS_EVAL_SUMM_RULE&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CS_EVAL_SUMM_RULEFOLDER&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CS_INCLUSION&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CS_INCLUSION_PARAMETER&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CS_PARAMETER&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CS_PARAMETER_CHOICES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CS_REUSABLE_QUERY&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CS_RQS_HIERARCHY&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CS_RQS_INCLUSION&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CS_RULE_FIX_LINK&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CS_RULE_SIMPLE_TEST&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CS_RULE_VIOL_CTX&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_CS_SCHEDULED_EVAL&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DBNET_TNS_ADMINS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DB_INVOBJS_ECM&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DB_LATEST_HDM_FINDINGS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DELTA_COMPARISON_DELTAS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DELTA_COMP_DELTA_DETAILS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DELTA_COMP_KEY_COLS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DELTA_COMP_PROPERTIES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DELTA_SUMMARY_ERRORS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DELTA_COMP_SUMMARIES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DELTA_SAVED_COMPARISON&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DIROBJ_USERS_HOTLIST&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DM_COLUMN_RULES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DM_INFCONS_COLUMNS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DM_JOB_EXECUTIONS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DM_SCOPESPECS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DM_SS_COLUMNS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_DUPLICATE_TARGETS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_E2E_DETAILS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_E2E_DETAILS_1DAY&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_E2E_DETAILS_1HOUR&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_E2E_JDBC&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_E2E_JDBC_1DAY&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_E2E_JDBC_1HOUR&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_E2E_SQL&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_E2E_SQL_1DAY&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_E2E_SQL_1HOUR&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_E2E_SQL_CONN&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_E2E_SQL_STMT&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_E2E_SUMMARY&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_E2E_SUMMARY_1DAY&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_E2E_SUMMARY_1HOUR&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_CLUSTER_NODE_INFO&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_CSA&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_CSA_APPID_TARGET_MAP&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_CSA_COOKIES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_CSA_CUSTOM&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_CSA_FAILED&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_CSA_GENERAL_INFO&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_CSA_RULES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_CSA_SNAPSHOT_INFO&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_HOSTPATCH_COMPL_HIST&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_HOSTPATCH_GROUPS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_HOSTPATCH_GROUP_REPOS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_HOSTPATCH_HOSTS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_HOSTPATCH_HOST_COMPL&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_HOSTPATCH_REPOS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_HOSTPATCH_REPOS_PKGS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_HOST_CONFIGS_TO_DEL&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_HW&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_HW_CPU&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_HW_IOCARD&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_HW_NIC&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_LOADED_FILES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_OS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_OS_COMPONENT&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_OS_FILESYSTEM&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_OS_PROPERTY&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_OS_REGISTERED_SW&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_OS_REGISTERED_SW_COMP&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_PATCH_CACHE&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_SAVEDHOSTCONFIG&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_ULN_CHANNELS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_ULN_CHANNEL_PKGS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_ULN_CH_ADV&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_ULN_SS_CHANNELS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ECM_ULN_STAGE_SERVERS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_EMX_CELL_CD_CONFIG&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_EMX_CELL_C_CONFIG&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_EMX_CELL_GD_CONFIG&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_EMX_CELL_IORM_CONFIG&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_EMX_CELL_L_CONFIG&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_EMX_CELL_PD_CONFIG&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_ENTERPRISE_CREDENTIALS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_FAILED_CONFIG_ACTIVITIES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_FEATURES_MAPPING&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_FEATURE_PATCHES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_FLAT_ROLE_GRANTS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_GENSVC_AVAIL_BEACONS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_GENSVC_AVAIL_CONFIG&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_GENSVC_AVAIL_EVENTS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_GENSVC_AVAIL_JOB&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_GENSVC_AVAIL_TESTS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_GENSVC_JOBS_DETAILS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_GENSVC_TEST_AVAIL&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_GENSVC_TEST_AVAIL_MARKER&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_GENSVC_TEST_CUR_AVAIL&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_GENSVC_TMPL_VARS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_GENSVC_UPDBCN_JOB&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_GENSVC_UPDBCN_JOB_TESTS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_GROUP_CHART&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_GROUP_CHART_SELTARGETS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_GROUP_CUSTOM_COLUMNS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_GROUP_SUMMARY_CHART_DEF&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_HA_BACKUP&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_HA_CLS_INTR_CONN&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_HA_DG_TARGET_SUMMARY&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_HA_FILES_ECM&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_HA_INIT_PARAMS_ECM&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_HA_RAC_INTR_CONN&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_HC_FS_MOUNT_DETAILS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_HC_IOCARD_DETAILS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_HOST_CREDENTIALS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_HTTP_SESSION_OBJECTS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_INDEX_SIZES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_INV_COMPONENT_PATCH&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_INV_FILE&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_INV_PATCH&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_INV_PATCHED_FILE&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_INV_PATCHED_FILE_COMP&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_INV_PATCHSET&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_INV_PATCH_FIXED_BUG&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_INV_VERSIONED_PATCH&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_IP_EMAIL_REPORT&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_IP_PURGE_POLICY&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_IP_REPORT_ELEM_IMAGE&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_IP_REPORT_ELEM_TARGETS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_IP_STORED_REPORT&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_ASSOC_PARAMS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_BLACKOUT_ASSOC&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_CREDENTIALS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_EMD_STATUS_QUEUE&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_EXEC_CRED_INFO&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_EXEC_EVENT_PARAMS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_EXEC_LOCKS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_EXT_TARGETS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_FLAT_TARGETS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_NOTIFY_STATES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_PURGE_TARGETS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_QUEUES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_STEP_COMMAND_LOG&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_STEP_TARGETS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_TARGET&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_TYPE_DISPLAY_PARAM&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_JOB_TYPE_PARAM_DROPDOWNS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_LICENSES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_LICENSE_CONFIRMATION&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_LOADER_QTABLE&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_MANAGEMENT_PLUGINS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_MASTER_AGENT&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_METRIC_COLLECTIONS_REP&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_METRIC_DEPENDENCY&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_METRIC_DEPENDENCY_DETAILS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_MNTR_SET_COPIES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_MP_CONTRIBUTORS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_MP_CONTRIBUTOR_FILE&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_MP_DEPLOYMENTS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_MP_DEPLOYMENT_ERRORS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_MP_FILES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_MP_FILE_PROPS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_MP_GROUPS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_MP_GROUP_MEMBERS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_MP_MECHANISMS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_MP_NLS_SUBSTITUTIONS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_MP_PROPS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_NESTED_JOB_CRED_INFO&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_NET_EVENTS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_NOTIFICATION_LOG&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_NOTIFY_DEVICES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_NOTIFY_DEVICE_PARAMS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_NOTIFY_DEV_SCHEDULES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_NOTIFY_EMAIL_GATEWAY&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_NOTIFY_INPUT_QTABLE&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_NOTIFY_NOTIFYEES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_NOTIFY_QTABLE&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_NOTIFY_REQUEUE&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_NOTIFY_SCHEDULES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_OB_ADMIN_CLIENT_DB&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_OSM_DISK_GROUP_ECM&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PAF_COMP_JOBTYPE_MAPPINGS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PAF_ENCRYPTED_STRINGS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PAF_INSTANCES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PAF_MSG_QTABLE_1&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PAF_MSG_QTABLE_2&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PAF_NOTIFICATION_LOG&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PAF_OMS_STATUS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PAF_PAR_FILES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PAF_STATES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PDP_HOST_SETTING&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PDP_SETTINGS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PDP_SETTING_VALUES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PLANPROBLEM_FACTORS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PROV_ASN_DEPENDENCIES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PROV_ASN_TARGETS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PROV_ASSIGNMENT&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PROV_BOOTSERVER&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PROV_CLUSTER&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PROV_CLUSTER_NODES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PROV_DEFAULT_IMAGE&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PROV_HISTORY&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PROV_IP_RANGE&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PROV_IP_RESERVED&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PROV_NET_CONFIG&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PROV_OPERATION&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PROV_RPM_REP&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PROV_STAGED_COMPS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PROV_STAGING_DIRS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PROV_SUITE_INSTANCE&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_PROV_SUITE_INST_MEMBERS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RAC_SERVICES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RCA_EVENT&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RCA_EVENT_ASSOC&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RCA_METRIC_PROPS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RCA_METRIC_TEST&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RCA_RECOVERY&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RCA_RUN&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RCA_SUMMARY&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RCA_TARGET_PROPS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RCA_TEST_RESULT&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RCA_TRACE&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RCVCAT_CONFIG&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RCVCAT_REPOS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_REPOS_TIME_COEFFICIENT&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RT_COOKIE_DATA&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RT_DOMAIN_1DAY&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RT_DOMAIN_1HOUR&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RT_DOMAIN_BOOTSTRAP&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RT_DOMAIN_DIST_1DAY&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RT_DOMAIN_DIST_1HOUR&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RT_DOMAIN_DIST_BOOTSTRAP&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RT_INCOMPLETE_LOADS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RT_INCOMPLETE_LOADS_1DAY&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RT_INCOMPLETE_LOADS_1HOUR&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RT_IP_1DAY&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RT_IP_1HOUR&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RT_IP_BOOTSTRAP&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RT_IP_DIST_1DAY&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RT_IP_DIST_1HOUR&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RT_IP_DIST_BOOTSTRAP&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RT_METRICS_RAW&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RT_PR_MAPPING&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RT_PR_MAPPING_1DAY&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RT_PR_MAPPING_1HOUR&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RT_REGIONS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RT_REGION_ENTRIES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RT_REGION_MAPPING&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RT_TARGET_PROPERTIES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RT_URLS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RT_URL_1DAY&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RT_URL_1HOUR&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RT_URL_BOOTSTRAP&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RT_URL_DIST_1DAY&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RT_URL_DIST_1HOUR&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_RT_URL_DIST_BOOTSTRAP&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_SEVERITY_RBK&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_SL_METRICS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_SL_METRICS_HISTORY&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_SL_RULES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_SL_RULES_HISTORY&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_SQLPROBLEM_FACTORS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_SQL_BIND_VARS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_SQL_EVALUATION&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_SQL_METRIC_HELPER&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_SQL_PLAN&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_SQL_REUSE&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_SQL_SUMMARY&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_SWLIB_DATA_DIRECTORIES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_SWLIB_ENTITIES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_SWLIB_ENTITY_DATA&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_SWLIB_ENTITY_DOCUMENTS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_SWLIB_ENTITY_PARAMETERS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_SWLIB_ENTITY_PLATFORMS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_SWLIB_ENTITY_REFERENCES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_SWLIB_ENTITY_REVISIONS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_SWLIB_MATURITY_STATUS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_SWLIB_REVISION_PARAMETERS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_SYSTEM_CHANGES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TABLE_SIZES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TARGETS_DELETE&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TARGET_AGENT_ASSOC&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TARGET_ASSOC&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TARGET_ASSOC_INSTANCE&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TARGET_ASSOC_PROP&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TARGET_ASSOC_PROP_DEFS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TARGET_BASELINES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TARGET_BASELINES_DATA&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TARGET_DELETE_CALLBACKS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TARGET_PENDING_ASSOCS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TEMPLATES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TEMPLATE_COPIES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TEST&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TEST_DEFAULT_PROMOTION&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TEST_DEFAULT_THRESHOLDS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TEST_MCOLUMNS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TEST_METRICS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TEST_METRIC_PROPS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TEST_PROP&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TEST_PROP_CHOICES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TEST_PROP_LEVEL&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TEST_PROP_QUALIFIERS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TEST_PROP_UIGROUP&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TEST_QUALIFIERS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TEST_TARGET_MAP&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TEXTINDEX&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TEXTINDEX_LOGS_INFO&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TEXT_INDEX_STATS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TOPO_PAGE_BG_IMAGE&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TOPO_PAGE_OBJ_POS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_TOPO_PAGE_PREF&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_UPDATE_COLL_CREDS_DATA&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_UPDATE_CREDENTIALS_DATA&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_UPDATE_OPERATIONS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_UPDATE_OPERATIONS_DATA&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_UPDATE_OPERATIONS_DETAILS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_UPDATE_PDP_DATA&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_UPDATE_PDP_DATA_COPY&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_UPDATE_PDP_DATA_MAP&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_UPDATE_PROPERTIES_DATA&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_UPDATE_TEMPLATE_DATA_MAP&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_UPDATE_THRESHOLDS_DATA&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_URL_CACHE&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_URL_PROXY&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_USER_JOBS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_USER_PREFERENCES&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_USER_REPORT_DEFS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_USER_SESSION&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_USER_TARGETS&quot; 0 KB 0 rows. . exported &quot;SYSMAN&quot;.&quot;MGMT_USER_TEMPLATES&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;DEF$_AQCALL&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;DEF$_AQERROR&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;DEF$_CALLDEST&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;DEF$_DEFAULTDEST&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;DEF$_DESTINATION&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;DEF$_ERROR&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;DEF$_LOB&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;DEF$_ORIGIN&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;DEF$_PROPAGATOR&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;DEF$_PUSHED_TRANSACTIONS&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;MVIEW$_ADV_INDEX&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;MVIEW$_ADV_PARTITION&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_AUDIT_COLUMN&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_COLUMN_GROUP&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_CONFLICT&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_DDL&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_EXCEPTIONS&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_EXTENSION&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_FLAVORS&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_FLAVOR_OBJECTS&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_GENERATED&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_GROUPED_COLUMN&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_INSTANTIATION_DDL&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_KEY_COLUMNS&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_OBJECT_PARMS&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_PARAMETER_COLUMN&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_PRIORITY&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_PRIORITY_GROUP&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_REFRESH_TEMPLATES&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_REPCAT&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_REPCATLOG&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_REPCOLUMN&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_REPGROUP_PRIVS&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_REPOBJECT&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_REPPROP&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_REPSCHEMA&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_RESOLUTION&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_RESOLUTION_STATISTICS&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_RESOL_STATS_CONTROL&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_RUNTIME_PARMS&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_SITES_NEW&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_SITE_OBJECTS&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_SNAPGROUP&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_TEMPLATE_OBJECTS&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_TEMPLATE_PARMS&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_TEMPLATE_REFGROUPS&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_TEMPLATE_SITES&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_TEMPLATE_TARGETS&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_USER_AUTHORIZATIONS&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;REPCAT$_USER_PARM_VALUES&quot; 0 KB 0 rows. . exported &quot;SYSTEM&quot;.&quot;SQLPLUS_PRODUCT_PROFILE&quot; 0 KB 0 rowsMaster table &quot;SYSTEM&quot;.&quot;SYS_EXPORT_FULL_01&quot; successfully loaded/unloaded******************************************************************************Dump file set for SYSTEM.SYS_EXPORT_FULL_01 is: /u01/backup/oracle11g.dmpJob &quot;SYSTEM&quot;.&quot;SYS_EXPORT_FULL_01&quot; completed with 1 error(s) at Fri Feb 26 14:02:01 2016 elapsed 0 00:04:27 impdp表导入以test_dump,scott_table表导入为例子# 因为我目前只有一个oracle测试环境，所以首先我删除表test_dump,scott_table表，便于我导入相关的表SQL&gt; drop table test_dump;Table droppedSQL&gt; drop table scott_table;Table dropped# tables指定需要导入的表# directory存放dumpfile和logfile的地方# dumpfile需要导入的备份文件# logfile导入日志文件# 导入scott用户的test_dump,scott_table表[oracle@wing ~]$ impdp scott/scott@WINGDB tables=scott_table,test_dump directory=TEST_DIR dumpfile=scott_dump.dmp logfile=scott_imp.logImport: Release 11.2.0.4.0 - Production on Fri Feb 26 12:05:05 2016Copyright (c) 1982, 2011, Oracle and/or its affiliates. All rights reserved.Connected to: Oracle Database 11g Enterprise Edition Release 11.2.0.4.0 - 64bit ProductionWith the Partitioning, OLAP, Data Mining and Real Application Testing optionsMaster table &quot;SCOTT&quot;.&quot;SYS_IMPORT_TABLE_01&quot; successfully loaded/unloadedStarting &quot;SCOTT&quot;.&quot;SYS_IMPORT_TABLE_01&quot;: scott/********@WINGDB tables=scott_table,test_dump directory=TEST_DIR dumpfile=scott_dump.dmp logfile=scott_imp.log Processing object type TABLE_EXPORT/TABLE/TABLEProcessing object type TABLE_EXPORT/TABLE/TABLE_DATA. . imported &quot;SCOTT&quot;.&quot;SCOTT_TABLE&quot; 5.445 KB 2 rows. . imported &quot;SCOTT&quot;.&quot;TEST_DUMP&quot; 5.085 KB 10 rowsJob &quot;SCOTT&quot;.&quot;SYS_IMPORT_TABLE_01&quot; successfully completed at Fri Feb 26 12:05:08 2016 elapsed 0 00:00:02# 进入directory发现增加了impdp日志文件-rw-r--r--. 1 oracle oinstall 919 Feb 26 12:05 scott_imp.log# 使用scott用户查看表已经导入成功SQL&gt; select * from test_dump; ID--------------------------------------- 1 2 3 4 5 6 7 8 9 1010 rows selectedSQL&gt; select * from scott_table; ID NAME--------------------------------------- ---------------- 1 Oracle 2 MySQL# table_exists_action=append允许导入到已经存在的表中# 没有使用table_exists_action=append导入到已经存在的表结构导入失败[oracle@wing ~]$ impdp scott/scott@WINGDB tables=scott_table,test_dump directory=TEST_DIR dumpfile=scott_dump.dmp logfile=scott_imp.logImport: Release 11.2.0.4.0 - Production on Fri Feb 26 13:12:45 2016Copyright (c) 1982, 2011, Oracle and/or its affiliates. All rights reserved.Connected to: Oracle Database 11g Enterprise Edition Release 11.2.0.4.0 - 64bit ProductionWith the Partitioning, OLAP, Data Mining and Real Application Testing optionsMaster table &quot;SCOTT&quot;.&quot;SYS_IMPORT_TABLE_01&quot; successfully loaded/unloadedStarting &quot;SCOTT&quot;.&quot;SYS_IMPORT_TABLE_01&quot;: scott/********@WINGDB tables=scott_table,test_dump directory=TEST_DIR dumpfile=scott_dump.dmp logfile=scott_imp.log Processing object type TABLE_EXPORT/TABLE/TABLEORA-39151: Table &quot;SCOTT&quot;.&quot;TEST_DUMP&quot; exists. All dependent metadata and data will be skipped due to table_exists_action of skipORA-39151: Table &quot;SCOTT&quot;.&quot;SCOTT_TABLE&quot; exists. All dependent metadata and data will be skipped due to table_exists_action of skipProcessing object type TABLE_EXPORT/TABLE/TABLE_DATAJob &quot;SCOTT&quot;.&quot;SYS_IMPORT_TABLE_01&quot; completed with 2 error(s) at Fri Feb 26 13:12:46 2016 elapsed 0 00:00:01# 使用table_exists_action=append导入到已经存在的表结构导入成功[oracle@wing ~]$ impdp scott/scott@WINGDB tables=scott_table,test_dump directory=TEST_DIR dumpfile=scott_dump.dmp logfile=scott_imp.log table_exists_action=appendImport: Release 11.2.0.4.0 - Production on Fri Feb 26 13:12:35 2016Copyright (c) 1982, 2011, Oracle and/or its affiliates. All rights reserved.Connected to: Oracle Database 11g Enterprise Edition Release 11.2.0.4.0 - 64bit ProductionWith the Partitioning, OLAP, Data Mining and Real Application Testing optionsMaster table &quot;SCOTT&quot;.&quot;SYS_IMPORT_TABLE_01&quot; successfully loaded/unloadedStarting &quot;SCOTT&quot;.&quot;SYS_IMPORT_TABLE_01&quot;: scott/********@WINGDB tables=scott_table,test_dump directory=TEST_DIR dumpfile=scott_dump.dmp logfile=scott_imp.log table_exists_action=append Processing object type TABLE_EXPORT/TABLE/TABLETable &quot;SCOTT&quot;.&quot;TEST_DUMP&quot; exists. Data will be appended to existing table but all dependent metadata will be skipped due to table_exists_action of appendTable &quot;SCOTT&quot;.&quot;SCOTT_TABLE&quot; exists. Data will be appended to existing table but all dependent metadata will be skipped due to table_exists_action of appendProcessing object type TABLE_EXPORT/TABLE/TABLE_DATA. . imported &quot;SCOTT&quot;.&quot;SCOTT_TABLE&quot; 5.445 KB 2 rows. . imported &quot;SCOTT&quot;.&quot;TEST_DUMP&quot; 5.085 KB 10 rowsJob &quot;SCOTT&quot;.&quot;SYS_IMPORT_TABLE_01&quot; successfully completed at Fri Feb 26 13:12:39 2016 elapsed 0 00:00:03 schema导入以scott用户的schema为例[oracle@wing ~]$ impdp scott/scott@WINGDB schemas=SCOTT directory=TEST_DIR dumpfile=SCOTT.dmp logfile=scott_imp.logImport: Release 11.2.0.4.0 - Production on Fri Feb 26 13:39:33 2016Copyright (c) 1982, 2011, Oracle and/or its affiliates. All rights reserved.Connected to: Oracle Database 11g Enterprise Edition Release 11.2.0.4.0 - 64bit ProductionWith the Partitioning, OLAP, Data Mining and Real Application Testing optionsMaster table &quot;SCOTT&quot;.&quot;SYS_IMPORT_SCHEMA_01&quot; successfully loaded/unloadedStarting &quot;SCOTT&quot;.&quot;SYS_IMPORT_SCHEMA_01&quot;: scott/********@WINGDB schemas=SCOTT directory=TEST_DIR dumpfile=SCOTT.dmp logfile=scott_imp.log Processing object type SCHEMA_EXPORT/PRE_SCHEMA/PROCACT_SCHEMAProcessing object type SCHEMA_EXPORT/TABLE/TABLEProcessing object type SCHEMA_EXPORT/TABLE/TABLE_DATA. . imported &quot;SCOTT&quot;.&quot;DEPT&quot; 5.929 KB 4 rows. . imported &quot;SCOTT&quot;.&quot;EMP&quot; 8.562 KB 14 rows. . imported &quot;SCOTT&quot;.&quot;SALGRADE&quot; 5.859 KB 5 rows. . imported &quot;SCOTT&quot;.&quot;SCOTT_TABLE&quot; 5.468 KB 4 rows. . imported &quot;SCOTT&quot;.&quot;TEST&quot; 5.062 KB 5 rows. . imported &quot;SCOTT&quot;.&quot;TEST_DUMP&quot; 5.156 KB 20 rows. . imported &quot;SCOTT&quot;.&quot;BONUS&quot; 0 KB 0 rowsProcessing object type SCHEMA_EXPORT/TABLE/INDEX/INDEXProcessing object type SCHEMA_EXPORT/TABLE/CONSTRAINT/CONSTRAINTProcessing object type SCHEMA_EXPORT/TABLE/INDEX/STATISTICS/INDEX_STATISTICSProcessing object type SCHEMA_EXPORT/TABLE/CONSTRAINT/REF_CONSTRAINTProcessing object type SCHEMA_EXPORT/TABLE/STATISTICS/TABLE_STATISTICSJob &quot;SCOTT&quot;.&quot;SYS_IMPORT_SCHEMA_01&quot; successfully completed at Fri Feb 26 13:39:37 2016 elapsed 0 00:00:04# 以scott用户验证schema导入# 首先由于我只有一个oracle测试环境，所以需要先删除我的schema下的对象SQL&gt; select * from user_tables;TABLE_NAME TABLESPACE_NAME CLUSTER_NAME IOT_NAME STATUS PCT_FREE PCT_USED INI_TRANS MAX_TRANS INITIAL_EXTENT NEXT_EXTENT MIN_EXTENTS MAX_EXTENTS PCT_INCREASE FREELISTS FREELIST_GROUPS LOGGING BACKED_UP NUM_ROWS BLOCKS EMPTY_BLOCKS AVG_SPACE CHAIN_CNT AVG_ROW_LEN AVG_SPACE_FREELIST_BLOCKS NUM_FREELIST_BLOCKS DEGREE INSTANCES CACHE TABLE_LOCK SAMPLE_SIZE LAST_ANALYZED PARTITIONED IOT_TYPE TEMPORARY SECONDARY NESTED BUFFER_POOL FLASH_CACHE CELL_FLASH_CACHE ROW_MOVEMENT GLOBAL_STATS USER_STATS DURATION SKIP_CORRUPT MONITORING CLUSTER_OWNER DEPENDENCIES COMPRESSION COMPRESS_FOR DROPPED READ_ONLY SEGMENT_CREATED RESULT_CACHE------------------------------ ------------------------------ ------------------------------ ------------------------------ -------- ---------- ---------- ---------- ---------- -------------- ----------- ----------- ----------- ------------ ---------- --------------- ------- --------- ---------- ---------- ------------ ---------- ---------- ----------- ------------------------- ------------------- ---------------------------------------- ---------------------------------------- -------------------- ---------- ----------- ------------- ----------- ------------ --------- --------- ------ ----------- ----------- ---------------- ------------ ------------ ---------- --------------- ------------ ---------- ------------------------------ ------------ ----------- ------------ ------- --------- --------------- ------------DEPT USERS VALID 10 1 255 65536 1048576 1 2147483645 YES N 4 5 0 0 0 20 0 0 1 1 N ENABLED 4 2016/2/14 10: NO N N NO DEFAULT DEFAULT DEFAULT DISABLED YES NO DISABLED YES DISABLED DISABLED NO NO YES DEFAULTEMP USERS VALID 10 1 255 65536 1048576 1 2147483645 YES N 14 5 0 0 0 38 0 0 1 1 N ENABLED 14 2016/2/14 10: NO N N NO DEFAULT DEFAULT DEFAULT DISABLED YES NO DISABLED YES DISABLED DISABLED NO NO YES DEFAULTBONUS USERS VALID 10 1 255 YES N 0 0 0 0 0 0 0 0 1 1 N ENABLED 0 2016/2/14 10: NO N N NO DEFAULT DEFAULT DEFAULT DISABLED YES NO DISABLED YES DISABLED DISABLED NO NO NO DEFAULTSALGRADE USERS VALID 10 1 255 65536 1048576 1 2147483645 YES N 5 5 0 0 0 10 0 0 1 1 N ENABLED 5 2016/2/14 10: NO N N NO DEFAULT DEFAULT DEFAULT DISABLED YES NO DISABLED YES DISABLED DISABLED NO NO YES DEFAULTTEST USERS VALID 10 1 255 65536 1048576 1 2147483645 YES N 1 1 N ENABLED NO N N NO DEFAULT DEFAULT DEFAULT DISABLED NO NO DISABLED YES DISABLED DISABLED NO NO YES DEFAULTTEST_DUMP USERS VALID 10 1 255 65536 1048576 1 2147483645 YES N 1 1 N ENABLED NO N N NO DEFAULT DEFAULT DEFAULT DISABLED NO NO DISABLED YES DISABLED DISABLED NO NO YES DEFAULTSCOTT_TABLE USERS VALID 10 1 255 65536 1048576 1 2147483645 YES N 1 1 N ENABLED NO N N NO DEFAULT DEFAULT DEFAULT DISABLED NO NO DISABLED YES DISABLED DISABLED NO NO YES DEFAULT7 rows selectedSQL&gt; drop table test;Table droppedSQL&gt; drop table test_dump;Table droppedSQL&gt; drop table scott_table;Table droppedSQL&gt; drop table emp;Table droppedSQL&gt; drop table dept;Table droppedSQL&gt; drop table salgrade;Table droppedSQL&gt; drop table bonus;Table droppedSQL&gt; select table_name from user_tables;TABLE_NAME------------------------------SQL&gt; select * from user_tables;TABLE_NAME TABLESPACE_NAME CLUSTER_NAME IOT_NAME STATUS PCT_FREE PCT_USED INI_TRANS MAX_TRANS INITIAL_EXTENT NEXT_EXTENT MIN_EXTENTS MAX_EXTENTS PCT_INCREASE FREELISTS FREELIST_GROUPS LOGGING BACKED_UP NUM_ROWS BLOCKS EMPTY_BLOCKS AVG_SPACE CHAIN_CNT AVG_ROW_LEN AVG_SPACE_FREELIST_BLOCKS NUM_FREELIST_BLOCKS DEGREE INSTANCES CACHE TABLE_LOCK SAMPLE_SIZE LAST_ANALYZED PARTITIONED IOT_TYPE TEMPORARY SECONDARY NESTED BUFFER_POOL FLASH_CACHE CELL_FLASH_CACHE ROW_MOVEMENT GLOBAL_STATS USER_STATS DURATION SKIP_CORRUPT MONITORING CLUSTER_OWNER DEPENDENCIES COMPRESSION COMPRESS_FOR DROPPED READ_ONLY SEGMENT_CREATED RESULT_CACHE------------------------------ ------------------------------ ------------------------------ ------------------------------ -------- ---------- ---------- ---------- ---------- -------------- ----------- ----------- ----------- ------------ ---------- --------------- ------- --------- ---------- ---------- ------------ ---------- ---------- ----------- ------------------------- ------------------- ---------------------------------------- ---------------------------------------- -------------------- ---------- ----------- ------------- ----------- ------------ --------- --------- ------ ----------- ----------- ---------------- ------------ ------------ ---------- --------------- ------------ ---------- ------------------------------ ------------ ----------- ------------ ------- --------- --------------- ------------# impdp之后再查看是否有相应的对象SQL&gt; select * from user_tables;TABLE_NAME TABLESPACE_NAME CLUSTER_NAME IOT_NAME STATUS PCT_FREE PCT_USED INI_TRANS MAX_TRANS INITIAL_EXTENT NEXT_EXTENT MIN_EXTENTS MAX_EXTENTS PCT_INCREASE FREELISTS FREELIST_GROUPS LOGGING BACKED_UP NUM_ROWS BLOCKS EMPTY_BLOCKS AVG_SPACE CHAIN_CNT AVG_ROW_LEN AVG_SPACE_FREELIST_BLOCKS NUM_FREELIST_BLOCKS DEGREE INSTANCES CACHE TABLE_LOCK SAMPLE_SIZE LAST_ANALYZED PARTITIONED IOT_TYPE TEMPORARY SECONDARY NESTED BUFFER_POOL FLASH_CACHE CELL_FLASH_CACHE ROW_MOVEMENT GLOBAL_STATS USER_STATS DURATION SKIP_CORRUPT MONITORING CLUSTER_OWNER DEPENDENCIES COMPRESSION COMPRESS_FOR DROPPED READ_ONLY SEGMENT_CREATED RESULT_CACHE------------------------------ ------------------------------ ------------------------------ ------------------------------ -------- ---------- ---------- ---------- ---------- -------------- ----------- ----------- ----------- ------------ ---------- --------------- ------- --------- ---------- ---------- ------------ ---------- ---------- ----------- ------------------------- ------------------- ---------------------------------------- ---------------------------------------- -------------------- ---------- ----------- ------------- ----------- ------------ --------- --------- ------ ----------- ----------- ---------------- ------------ ------------ ---------- --------------- ------------ ---------- ------------------------------ ------------ ----------- ------------ ------- --------- --------------- ------------DEPT USERS VALID 10 1 255 65536 1048576 1 2147483645 YES N 4 5 0 0 0 20 0 0 1 1 N ENABLED 4 2016/2/14 10: NO N N NO DEFAULT DEFAULT DEFAULT DISABLED YES NO DISABLED YES DISABLED DISABLED NO NO YES DEFAULTEMP USERS VALID 10 1 255 65536 1048576 1 2147483645 YES N 14 5 0 0 0 38 0 0 1 1 N ENABLED 14 2016/2/14 10: NO N N NO DEFAULT DEFAULT DEFAULT DISABLED YES NO DISABLED YES DISABLED DISABLED NO NO YES DEFAULTBONUS USERS VALID 10 1 255 YES N 0 0 0 0 0 0 0 0 1 1 N ENABLED 0 2016/2/14 10: NO N N NO DEFAULT DEFAULT DEFAULT DISABLED YES NO DISABLED YES DISABLED DISABLED NO NO NO DEFAULTSALGRADE USERS VALID 10 1 255 65536 1048576 1 2147483645 YES N 5 5 0 0 0 10 0 0 1 1 N ENABLED 5 2016/2/14 10: NO N N NO DEFAULT DEFAULT DEFAULT DISABLED YES NO DISABLED YES DISABLED DISABLED NO NO YES DEFAULTTEST USERS VALID 10 1 255 65536 1048576 1 2147483645 YES N 1 1 N ENABLED NO N N NO DEFAULT DEFAULT DEFAULT DISABLED NO NO DISABLED YES DISABLED DISABLED NO NO YES DEFAULTTEST_DUMP USERS VALID 10 1 255 65536 1048576 1 2147483645 YES N 1 1 N ENABLED NO N N NO DEFAULT DEFAULT DEFAULT DISABLED NO NO DISABLED YES DISABLED DISABLED NO NO YES DEFAULTSCOTT_TABLE USERS VALID 10 1 255 65536 1048576 1 2147483645 YES N 1 1 N ENABLED NO N N NO DEFAULT DEFAULT DEFAULT DISABLED NO NO DISABLED YES DISABLED DISABLED NO NO YES DEFAULT7 rows selected database导入导入整个database[oracle@wing ~]$ impdp system/Wing_database@WINGDB full=Y directory=TEST_DIR dumpfile=oracle11g.dmp logfile=oracle11g_imp.log# 结果不再展示 include/exclude的使用include表示只备份/恢复该字段指定的对象exclude表示除了该字段指定的对象都备份/恢复对象为object type(具体待以后补充)# include导出[oracle@wing ~]$ expdp scott/Wing_database@WINGDB tables=test,scott_table include=TABLE_DATA directory=TEST_DIR dumpfile=scott.dmp logfile=scott_exp.logExport: Release 11.2.0.4.0 - Production on Fri Feb 26 14:56:58 2016Copyright (c) 1982, 2011, Oracle and/or its affiliates. All rights reserved.Connected to: Oracle Database 11g Enterprise Edition Release 11.2.0.4.0 - 64bit ProductionWith the Partitioning, OLAP, Data Mining and Real Application Testing optionsStarting &quot;SCOTT&quot;.&quot;SYS_EXPORT_TABLE_01&quot;: scott/********@WINGDB tables=test,scott_table include=TABLE_DATA directory=TEST_DIR dumpfile=scott.dmp logfile=scott_exp.log Estimate in progress using BLOCKS method...Processing object type TABLE_EXPORT/TABLE/TABLE_DATATotal estimation using BLOCKS method: 128 KB. . exported &quot;SCOTT&quot;.&quot;SCOTT_TABLE&quot; 5.468 KB 4 rows. . exported &quot;SCOTT&quot;.&quot;TEST&quot; 5.062 KB 5 rowsMaster table &quot;SCOTT&quot;.&quot;SYS_EXPORT_TABLE_01&quot; successfully loaded/unloaded******************************************************************************Dump file set for SCOTT.SYS_EXPORT_TABLE_01 is: /u01/backup/scott.dmpJob &quot;SCOTT&quot;.&quot;SYS_EXPORT_TABLE_01&quot; successfully completed at Fri Feb 26 14:57:00 2016 elapsed 0 00:00:02# include导入[oracle@wing ~]$ impdp scott/Wing_database@WINGDB tables=test,scott_table include=TABLE_DATA directory=TEST_DIR dumpfile=scott.dmp logfile=scott_imp.logImport: Release 11.2.0.4.0 - Production on Fri Feb 26 15:00:44 2016Copyright (c) 1982, 2011, Oracle and/or its affiliates. All rights reserved.Connected to: Oracle Database 11g Enterprise Edition Release 11.2.0.4.0 - 64bit ProductionWith the Partitioning, OLAP, Data Mining and Real Application Testing optionsMaster table &quot;SCOTT&quot;.&quot;SYS_IMPORT_TABLE_01&quot; successfully loaded/unloadedStarting &quot;SCOTT&quot;.&quot;SYS_IMPORT_TABLE_01&quot;: scott/********@WINGDB tables=test,scott_table include=TABLE_DATA directory=TEST_DIR dumpfile=scott.dmp logfile=scott_imp.log Processing object type TABLE_EXPORT/TABLE/TABLE_DATA. . imported &quot;SCOTT&quot;.&quot;SCOTT_TABLE&quot; 5.468 KB 4 rows. . imported &quot;SCOTT&quot;.&quot;TEST&quot; 5.062 KB 5 rowsJob &quot;SCOTT&quot;.&quot;SYS_IMPORT_TABLE_01&quot; successfully completed at Fri Feb 26 15:00:45 2016 elapsed 0 00:00:01# exclude导出用法同include# exclude导入用法通include expdp参数详解[oracle@wing ~]$ expdp help=YExport: Release 11.2.0.4.0 - Production on Fri Feb 26 15:17:53 2016Copyright (c) 1982, 2011, Oracle and/or its affiliates. All rights reserved.The Data Pump export utility provides a mechanism for transferring data objectsbetween Oracle databases. The utility is invoked with the following command: Example: expdp scott/tiger DIRECTORY=dmpdir DUMPFILE=scott.dmpYou can control how Export runs by entering the ‘expdp’ command followedby various parameters. To specify parameters, you use keywords: Format: expdp KEYWORD=value or KEYWORD=(value1,value2,…,valueN) Example: expdp scott/tiger DUMPFILE=scott.dmp DIRECTORY=dmpdir SCHEMAS=scottor TABLES=(T1:P1,T1:P2), if T1 is partitioned tableUSERID must be the first parameter on the command line. The available keywords and their descriptions follow. Default values are listed within square brackets. ATTACH Attach to an existing job. For example, ATTACH=job_name. CLUSTER Utilize cluster resources and distribute workers across the Oracle RAC. Valid keyword values are: [Y] and N. COMPRESSION Reduce the size of a dump file. Valid keyword values are: ALL, DATA_ONLY, [METADATA_ONLY] and NONE. CONTENT Specifies data to unload. Valid keyword values are: [ALL], DATA_ONLY and METADATA_ONLY. 指定需要导出的内容 DATA_OPTIONS Data layer option flags. Valid keyword values are: XML_CLOBS. DIRECTORY Directory object to be used for dump and log files. 指定dumpfile和logfile的存放位置 DUMPFILE Specify list of destination dump file names [expdat.dmp]. For example, DUMPFILE=scott1.dmp, scott2.dmp, dmpdir:scott3.dmp. expdp中指定dumpfile的文件位置 ENCRYPTION Encrypt part or all of a dump file. Valid keyword values are: ALL, DATA_ONLY, ENCRYPTED_COLUMNS_ONLY, METADATA_ONLY and NONE. 加密部分/全部的备份文件 ENCRYPTION_ALGORITHM Specify how encryption should be done. Valid keyword values are: [AES128], AES192 and AES256. 指定加密算法 ENCRYPTION_MODE Method of generating encryption key. Valid keyword values are: DUAL, PASSWORD and [TRANSPARENT]. 生成加密密钥的方法 ENCRYPTION_PASSWORD Password key for creating encrypted data within a dump file. 加密密码 ESTIMATE Calculate job estimates. Valid keyword values are: [BLOCKS] and STATISTICS. ESTIMATE_ONLY Calculate job estimates without performing the export. EXCLUDE Exclude specific object types. For example, EXCLUDE=SCHEMA:”=’HR’”. 指定备份中需要排除的对象 FILESIZE Specify the size of each dump file in units of bytes. 指定每个备份文件的大小（单位为byte） FLASHBACK_SCN SCN used to reset session snapshot. 指定导出特定SCN时刻的表数据 FLASHBACK_TIME Time used to find the closest corresponding SCN value. 指定导出特定时间点的表数据 FULL Export entire database [N]. 备份整个数据库 HELP Display Help messages [N]. 显示帮助信息 INCLUDE Include specific object types. For example, INCLUDE=TABLE_DATA. 指定需要备份的对象 JOB_NAME Name of export job to create. LOGFILE Specify log file name [export.log]. 备份的日志文件 NETWORK_LINK Name of remote database link to the source system. NOLOGFILE Do not write log file [N].不写备份日志文件 PARALLEL Change the number of active workers for current job. 指定并发线程数，注意一个并发对应一个dumpfile,所以指定的并发线程数应该与dumpfile数量相同，dumpfile可以使用通配符%U进行匹配 例如： expdp ananda/abc123 tables=CASES directory=DPDATA1 dumpfile=expCASES_%U.dmp parallel=4 job_name=Cases_Export PARFILE Specify parameter file name. 指定expdp参数文件，文件以.par结束 QUERY Predicate clause used to export a subset of a table. For example, QUERY=employees:”WHERE department_id &gt; 10”. 按SQL语句条件备份 REMAP_DATA Specify a data conversion function. For example, REMAP_DATA=EMP.EMPNO:REMAPPKG.EMPNO. 将源对象的数据备份到目标对象的数据中 REUSE_DUMPFILES Overwrite destination dump file if it exists [N]. SAMPLE Percentage of data to be exported. 要导出的数据的百分比。 SCHEMAS List of schemas to export [login schema]. 需要备份的schema的列表 SERVICE_NAME Name of an active Service and associated resource group to constrain Oracle RAC resources. SOURCE_EDITION Edition to be used for extracting metadata. STATUS Frequency (secs) job status is to be monitored where the default [0] will show new status when available. TABLES Identifies a list of tables to export. For example, TABLES=HR.EMPLOYEES,SH.SALES:SALES_1995. 需要备份的table的列表 TABLESPACES Identifies a list of tablespaces to export. 需要备份的tablespace列表 TRANSPORTABLE Specify whether transportable method can be used. Valid keyword values are: ALWAYS and [NEVER]. TRANSPORT_FULL_CHECK Verify storage segments of all tables [N]. TRANSPORT_TABLESPACES List of tablespaces from which metadata will be unloaded. VERSION Version of objects to export. Valid keyword values are: [COMPATIBLE], LATEST or any valid database version. The following commands are valid while in interactive mode.Note: abbreviations are allowed. ADD_FILE Add dumpfile to dumpfile set. CONTINUE_CLIENT Return to logging mode. Job will be restarted if idle. EXIT_CLIENT Quit client session and leave job running. FILESIZE Default filesize (bytes) for subsequent ADD_FILE commands. HELP Summarize interactive commands. KILL_JOB Detach and delete job. PARALLEL Change the number of active workers for current job. REUSE_DUMPFILES Overwrite destination dump file if it exists [N]. START_JOB Start or resume current job. Valid keyword values are: SKIP_CURRENT. STATUS Frequency (secs) job status is to be monitored where the default [0] will show new status when available. STOP_JOB Orderly shutdown of job execution and exits the client. Valid keyword values are: IMMEDIATE. impdp参数详解[oracle@wing backup]$ impdp -helpImport: Release 11.2.0.4.0 - Production on Fri Feb 26 16:26:24 2016Copyright (c) 1982, 2011, Oracle and/or its affiliates. All rights reserved.The Data Pump Import utility provides a mechanism for transferring data objectsbetween Oracle databases. The utility is invoked with the following command: Example: impdp scott/tiger DIRECTORY=dmpdir DUMPFILE=scott.dmpYou can control how Import runs by entering the ‘impdp’ command followedby various parameters. To specify parameters, you use keywords: Format: impdp KEYWORD=value or KEYWORD=(value1,value2,…,valueN) Example: impdp scott/tiger DIRECTORY=dmpdir DUMPFILE=scott.dmpUSERID must be the first parameter on the command line. The available keywords and their descriptions follow. Default values are listed within square brackets. ATTACH Attach to an existing job. For example, ATTACH=job_name. CLUSTER Utilize cluster resources and distribute workers across the Oracle RAC. Valid keyword values are: [Y] and N. CONTENT Specifies data to load. Valid keywords are: [ALL], DATA_ONLY and METADATA_ONLY. 指定需要导入的数据 DATA_OPTIONS Data layer option flags. Valid keywords are: SKIP_CONSTRAINT_ERRORS. DIRECTORY Directory object to be used for dump, log and SQL files. 指定dumpfile和logfile的存放位置 DUMPFILE List of dump files to import from [expdat.dmp]. For example, DUMPFILE=scott1.dmp, scott2.dmp, dmpdir:scott3.dmp. 备份文件的存放位置 ENCRYPTION_PASSWORD Password key for accessing encrypted data within a dump file. Not valid for network import jobs. ESTIMATE Calculate job estimates. Valid keywords are: [BLOCKS] and STATISTICS. EXCLUDE Exclude specific object types. For example, EXCLUDE=SCHEMA:”=’HR’”. FLASHBACK_SCN SCN used to reset session snapshot. 指定重新设置特定SCN FLASHBACK_TIME Time used to find the closest corresponding SCN value. 指定找出最近的特定时间点的SCN值 FULL Import everything from source [Y]. 恢复整个数据库 HELP Display help messages [N]. 显示帮助信息 INCLUDE Include specific object types. For example, INCLUDE=TABLE_DATA. 恢复指定的object type JOB_NAME Name of import job to create. LOGFILE Log file name [import.log]. 指定import日志文件 NETWORK_LINK Name of remote database link to the source system. NOLOGFILE Do not write log file [N]. 不写impdp的日志文件 PARALLEL Change the number of active workers for current job. 指定并发线程数 PARFILE Specify parameter file. 指定impdp PARTITION_OPTIONS Specify how partitions should be transformed. Valid keywords are: DEPARTITION, MERGE and [NONE]. QUERY Predicate clause used to import a subset of a table. For example, QUERY=employees:”WHERE department_id &gt; 10”. 按SQL条件恢复 REMAP_DATA Specify a data conversion function. For example, REMAP_DATA=EMP.EMPNO:REMAPPKG.EMPNO. REMAP_DATAFILE Redefine data file references in all DDL statements. REMAP_SCHEMA Objects from one schema are loaded into another schema. 将源schema导入到目标schema REMAP_TABLE Table names are remapped to another table. For example, REMAP_TABLE=HR.EMPLOYEES:EMPS. 将源表导入到目标表 REMAP_TABLESPACE Tablespace objects are remapped to another tablespace. 将源表空间对象导入到目标表空间中 REUSE_DATAFILES Tablespace will be initialized if it already exists [N]. SCHEMAS List of schemas to import. 指定恢复schema列表 SERVICE_NAME Name of an active Service and associated resource group to constrain Oracle RAC resources. SKIP_UNUSABLE_INDEXES Skip indexes that were set to the Index Unusable state. 跳过不可用状态的索引 SOURCE_EDITION Edition to be used for extracting metadata. SQLFILE Write all the SQL DDL to a specified file. 将所有的DDL写到一个指定的文件中 STATUS Frequency (secs) job status is to be monitored where the default [0] will show new status when available. STREAMS_CONFIGURATION Enable the loading of Streams metadata TABLE_EXISTS_ACTION Action to take if imported object already exists. Valid keywords are: APPEND, REPLACE, [SKIP] and TRUNCATE. 如果恢复数据时，对应的表存在，那么impdp才去的措施，append指将数据追加到已经存在的表中，replace指替换存在的表，skip指跳过已存在表的数据恢复，truncate指将已存在表的数据清空 TABLES Identifies a list of tables to import. For example, TABLES=HR.EMPLOYEES,SH.SALES:SALES_1995. 指定恢复table列表 TABLESPACES Identifies a list of tablespaces to import. 指定恢复的tablespace列表 TARGET_EDITION Edition to be used for loading metadata. TRANSFORM Metadata transform to apply to applicable objects. Valid keywords are: OID, PCTSPACE, SEGMENT_ATTRIBUTES and STORAGE. TRANSPORTABLE Options for choosing transportable data movement. Valid keywords are: ALWAYS and [NEVER]. Only valid in NETWORK_LINK mode import operations. TRANSPORT_DATAFILES List of data files to be imported by transportable mode. TRANSPORT_FULL_CHECK Verify storage segments of all tables [N]. TRANSPORT_TABLESPACES List of tablespaces from which metadata will be loaded. Only valid in NETWORK_LINK mode import operations. VERSION Version of objects to import. Valid keywords are: [COMPATIBLE], LATEST or any valid database version. Only valid for NETWORK_LINK and SQLFILE. The following commands are valid while in interactive mode.Note: abbreviations are allowed. CONTINUE_CLIENT Return to logging mode. Job will be restarted if idle. EXIT_CLIENT Quit client session and leave job running. HELP Summarize interactive commands. KILL_JOB Detach and delete job. PARALLEL Change the number of active workers for current job. START_JOB Start or resume current job. Valid keywords are: SKIP_CURRENT. STATUS Frequency (secs) job status is to be monitored where the default [0] will show new status when available. STOP_JOB Orderly shutdown of job execution and exits the client. Valid keywords are: IMMEDIATE.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Oracle的TIMESTAMP的几种类型]]></title>
      <url>%2F2016%2F04%2F24%2FOracle%E7%9A%84TIMESTAMP%E7%9A%84%E5%87%A0%E7%A7%8D%E7%B1%BB%E5%9E%8B%2F</url>
      <content type="text"><![CDATA[Oralce中TIMESTAMP的几种类型。 TIMESTAMP的几种类型比较TIMESTAMP时间戳类型，与date的区别在于，date不能精确到毫秒，而timestamp可以精确到毫秒，毫秒的位数为0-9位，默认为6位。SQL&gt; select tp from timestamp_test;TP--------------------------------------------------------------------------------01-3月 -16 09.22.33.000000 上午 TIMESTAMP WITH TIME ZONETIMESTAMP WITH TIME ZONE 与 TIMESTAMP的区别在于，前者输出显示携带存入该时间值的数据库时区，后者输出不携带时区。SQL&gt; select tp_tz from timestamp_test;TP_TZ--------------------------------------------------------------------------------01-3月 -16 09.22.33.000000 上午 +08:00 TIMESTAMP WITH LOCAL TIME ZONE与TIMESTAMP的区别在于，前者的输出受时区影响，会跟着时区的变化而变化，而后者存入数据库后将不受时区影响。即前者以数据库本地时区保存数据，输出时将转换成客户端时区输出。SQL&gt; select tp_l_tz from timestamp_test;TP_L_TZ--------------------------------------------------------------------------------01-3月 -16 09.22.33.000000 上午 实战演练# 创建timestamp_test测试表SQL&gt; create table timestamp_test(dt date,tp timestamp(6),tp_tz timestamp(6) with time zone,tp_l_tz timestamp(6) with local time zone);Table created# 在测试表中添加数据SQL&gt; insert into timestamp_test values(sysdate,sysdate,sysdate,sysdate);1 row insertedSQL&gt; commit;Commit complete# 查看数据库的时区和当前会话的时区SQL&gt; select dbtimezone,sessiontimezone from dual;DBTIMEZONE SESSIONTIMEZONE---------- ---------------------------------------------------------------------------+00:00 +08:00# 查看当前时间SQL&gt; select sysdate from dual;SYSDATE-----------2016/3/1 9:# 查看测试表的数据SQL&gt; select * from timestamp_test;DT TP TP_TZ TP_L_TZ----------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- --------------------------------------------------------------------------------2016/3/1 9: 01-3月 -16 09.22.33.000000 上午 01-3月 -16 09.22.33.000000 上午 +08:00 01-3月 -16 09.22.33.000000 上午# 修改当前会话的时区SQL&gt; alter session set time_zone='+10:00';Session altered# 查看当前会话时区修改后的测试表的数据SQL&gt; select dbtimezone,sessiontimezone from dual;DBTIMEZONE SESSIONTIMEZONE---------- ---------------------------------------------------------------------------+00:00 +10:00SQL&gt; select * from timestamp_test;DT TP TP_TZ TP_L_TZ----------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- --------------------------------------------------------------------------------2016/3/1 9: 01-3月 -16 09.22.33.000000 上午 01-3月 -16 09.22.33.000000 上午 +08:00 01-3月 -16 11.22.33.000000 上午]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Oralce中VARCHAR2()和NVARCHAR2()的区别]]></title>
      <url>%2F2016%2F04%2F19%2FOralce%E4%B8%ADVARCHAR2-%E5%92%8CNVARCHAR2-%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
      <content type="text"><![CDATA[Oralce中VARCHAR2()和NVARCHAR2()的区别。 VARCHAR2()和NVARCHAR2()的官方定义官方文档定义如下： VARCHAR2(size [BYTE | CHAR])Variable-length character string having maximum length size bytes or characters. Maximum size is 4000 bytes or characters, and minimum is 1 byte or 1 character. You must specify size for VARCHAR2.BYTE indicates that the column will have byte length semantics. CHAR indicates that the column will have character semantics. NVARCHAR2(size)Variable-length Unicode character string having maximum length size characters. The number of bytes can be up to two times size for AL16UTF16 encoding and three times size for UTF8 encoding. Maximum size is determined by the national character set definition, with an upper limit of 4000 bytes. You must specify size for NVARCHAR2. 中文翻译： VARCHAR2(size [BYTE | CHAR])具有最大长度的字节数（bytes）或字符数（char）的可变长度的字符类型。最大长度为4000字节/字符，最小长度是1字节/字符。你必须为VARCHAR2()类型指定大小。BYTE代表该列以字节计算长度，CHAR代表该列以字符计算长度。 NVARCHAR2(size)具有最大长度的带有字符集属性的可变长度的字符类型。它的长度是AL16UTF16字符集的2倍，UTF8字符集的三倍。它的最大长度取决于字符集，上限位4000字节。您必须为NVARCHAR2()类型指定大小。 总结：NVARCHAR2(size)与VARCHAR2(size CHAR)相似，唯一的区别是NVARCHAR2(size)的最大长度是4000字节（实验测试结果是，在utf8的字符集下，最大长度为2000字符），而VARCHAR2(size CHAR）的最大长度是4000字符。 实战演练使用字符集为UTF8。# 验证NVARCHAR2(size)与VARCHAR2(size CHAR)相似SQL&gt; create table t_varchar2(name varchar2(6 CHAR));Table createdSQL&gt; insert into t_varchar2 values('中国');1 row insertedSQL&gt; insert into t_varchar2 values('中华人民共和');1 row insertedSQL&gt; insert into t_varchar2 values('中华人民共和国');insert into t_varchar2 values('中华人民共和国')ORA-12899: 列 "SCOTT"."T_VARCHAR2"."NAME" 的值太大 (实际值: 7, 最大值: 6)SQL&gt; create table t_nvarchar2(name nvarchar2(6));Table createdSQL&gt; insert into t_nvarchar2 values('中国');1 row insertedSQL&gt; insert into t_nvarchar2 values('中华人民共和');1 row insertedSQL&gt; insert into t_nvarchar2 values('中华人民共和国');insert into t_nvarchar2 values('中华人民共和国')ORA-12899: 列 "SCOTT"."T_NVARCHAR2"."NAME" 的值太大 (实际值: 7, 最大值: 6)# 验证NVARCHAR2(sie)与VARCHAR(size CHAR)存在最大长度不同SQL&gt; create table t_varchar2(name varchar2(4000 CHAR));Table createdSQL&gt; create table t_nvarchar2(name nvarchar2(4000));create table t_nvarchar2(name nvarchar2(4000))ORA-00910: 指定的长度对于数据类型而言过长]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Oracle错误集锦]]></title>
      <url>%2F2016%2F04%2F10%2FOracle%E9%94%99%E8%AF%AF%E9%9B%86%E9%94%A6%2F</url>
      <content type="text"><![CDATA[收集Oracle的错误并记录，作为未来供自己参考。 Oracle-4031原因分析大量的硬解析出现，产生大量的free chunk，然后突然出现大的SQL语句，找不到合适的free chunk。 解决方案 将shared pool中的library cache和row cache全部清空(该方案治标不治本) alter system flush shared_pool; 共享SQL以减少硬解析开发时需要统一书写风格（要知道两个SQL语句多一个空格都是不一样的），其次使用绑定变量的方式实现SQL共享。如果无法再程序开发层面修改，可以修改如下参数达到共享SQL的功能： # 该参数可以解决绑定变量的问题，但不能用于解决书写风格不统一的问题alter system set cursor_sharing=&apos;force&apos;; 保留区（该区域只用来缓存大对象，即较大的SQL语句） # 执行如下语句，查看在保留区中是否有存在请求chunk失败的SQL语句select REQUEST_MISSES from v$shared_pool_reserved;# 如果上述语句结果大于0，则需要调大shared_pool_reserved_size的大小alter system shared_pool_reserved_size=？ 增加sharde pool的空间 select COMMPONENT,CURRENT_SIZE from V$SGA_DYNAMIC_COMPONENTS;show parameter sga_target;show parameter sga_max_size;alter system set shared_pool_size=150M scope=both;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[MySQL之procedure analyse()函数]]></title>
      <url>%2F2016%2F04%2F09%2FMySQL%E4%B9%8Bprocedure-analyse-%E5%87%BD%E6%95%B0%2F</url>
      <content type="text"><![CDATA[你还在为MySQL表结构该选择什么类型字段而郁闷？你还在为MySQL字段该选择多少长度而纠结？给你一个武林秘籍吧~也是我今天偷学来的=。= 简介procedure analyse()函数是MySQL内置的对MySQL字段值进行统计分析后给出建议的字段类型。 语法procesure analyse(max_elements,max_memory) max_elements指定每列非重复值的最大值，当超过这个值的时候，MySQL不会推荐enum类型。 max_memoryanalyse()为每列找出所有非重复值所采用的最大内存大小。 实战演练# 对t1表所有的列进行分析wing@3306&gt;show create table t1;+-------+---------------------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+-------+---------------------------------------------------------------------------------------------------------------------------------------------------------+| t1 | CREATE TABLE `t1` ( `id` int(11) DEFAULT NULL, `name` varchar(16) DEFAULT NULL, `score` int(11) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8 |+-------+---------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec)wing@3306&gt;select * from t1 procedure analyse(4);+---------------+-----------+-----------+------------+------------+------------------+-------+-------------------------+-------------+--------------------------------+| Field_name | Min_value | Max_value | Min_length | Max_length | Empties_or_zeros | Nulls | Avg_value_or_avg_length | Std | Optimal_fieldtype |+---------------+-----------+-----------+------------+------------+------------------+-------+-------------------------+-------------+--------------------------------+| wing.t1.id | 1 | 200000 | 1 | 6 | 0 | 0 | 100000.5000 | 116099.2790 | MEDIUMINT(6) UNSIGNED NOT NULL || wing.t1.name | 000jxc6V | zzznmkcX | 8 | 8 | 0 | 0 | 8.0000 | NULL | CHAR(8) NOT NULL || wing.t1.score | 1 | 100 | 1 | 3 | 0 | 0 | 50.4889 | 28.8768 | TINYINT(3) UNSIGNED NOT NULL |+---------------+-----------+-----------+------------+------------+------------------+-------+-------------------------+-------------+--------------------------------+3 rows in set (0.14 sec)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[MySQL之timeout参数]]></title>
      <url>%2F2016%2F04%2F09%2FMySQL%E4%B9%8Btimeout%E5%8F%82%E6%95%B0%2F</url>
      <content type="text"><![CDATA[总是记不住MySQL里面和timeout相关参数的含义，所以决定对其进行整理。 首先查看下与MySQL相关的timeout参数：mysql&gt; show variables like '%timeout%';+-----------------------------+----------+| Variable_name | Value |+-----------------------------+----------+| connect_timeout | 10 || delayed_insert_timeout | 300 || innodb_flush_log_at_timeout | 1 || innodb_lock_wait_timeout | 120 || innodb_rollback_on_timeout | ON || interactive_timeout | 172800 || lock_wait_timeout | 31536000 || net_read_timeout | 30 || net_write_timeout | 60 || rpl_stop_slave_timeout | 31536000 || slave_net_timeout | 3600 || wait_timeout | 172800 |+-----------------------------+----------+12 rows in set (0.00 sec) connect_timeout释义：client与MySQL建立连接时，MySQL允许client建立连接的秒数，如果client建立与MySQL的连接超过connect_timeout，MySQL将拒绝client此次的连接（例如client登录MySQL时，client超过connect_timeout依旧没有登录到MySQL,MySQL将拒接client的此次登录）。MySQL会报出错误：Lost connection to MySQL server at ‘XXX’, system error: errno. delayed_insert_timeoutINSERT DELAYED的超时时间。 innodb_flush_log_at_timeout每隔N秒刷新一次MySQL日志。 innodb_lock_wait_timeoutinnodb事务中行锁等待超时时间（只适用于innodb行锁），一旦innodb行锁超过该值，将会回滚事务，对于事务回滚方式由innodb_rollback_on_timeout决定。 innodb_rollback_on_timeout当innodb行锁等待超时后，如果innodb_rollback_on_timeout=0,则只回滚超时事务的最后一个语句(MySQL的默认处理方式)，如果innodb_rollback_on_timeout=1，则回滚整个超时事务（推荐）。 interactive_timeout对于交互式的连接，当该连接超过interactive_timeout时间没有活动时，MySQL将关闭该交互式连接。 lock_wait_timeout表锁等待超时的时间(官网上说的是metadata lock)，如LOCK TABLES,FLUSH TABLES WITH READ LOCK，HANDLER语句均会使用到metadata lock。 net_read_timeout服务器等待客户端发送数据包的超时时间。 （服务器读） net_write_timeout客户端等待服务器发送数据包的超时时间。（服务器写） rpl_stop_slave_timeout为了方式stop slave与其他的slave SQL线程发生死锁，所以通过rpl_stop_slave_timeout控制STOP SLAVE的等待时间。 slave_net_timeout当slave超过slave_net_timeout时间没有读到来自master的数据，就认为master宕机了，于是slave断开连接，尝试重连。 wait_timeout对于非交互式的连接，一个持续sleep状态的线程超时时间。即通过show processlist看到的sleep状态的线程，当它持续sleep时间超过interactive_timeout之后，将会倍MySQL主动KILL掉。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[MySQL实时监控sql工具之orztop]]></title>
      <url>%2F2016%2F04%2F04%2FMySQL%E5%AE%9E%E6%97%B6%E7%9B%91%E6%8E%A7sql%E5%B7%A5%E5%85%B7%E4%B9%8Borztop%2F</url>
      <content type="text"><![CDATA[orztop是一款实时show full processlist的工具，我们可以实时看到数据库有哪些线程，执行哪些语句等。工具使用方便简单。解决了我们需要手动刷新show full processlist的痛苦。 orztop结果图此处我正在对我的mysql使用tpcc-mysql工具进行压测，此时orztop得到的结果图如下： orztop命令行orztop命令行均为较简单的英文，这些英文的阅读能力是作为一个IT人员的必备专业技能。。[root@wing orztop]# ./orztop --help==========================================================================================Info : Created By zhuxu@taobao.comUsage :Command line options : -help Print Help Info. -h,--host Hostname/Ip to use for mysql connection. -u,--user User to use for mysql connection. -p,--pwd Password to use for mysql connection. -P,--port Port to use for mysql connection(default 3306). -S,--socket Socket to use for mysql connection. -t Time(second) Interval. ==========================================================================================]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux限流工具之pv]]></title>
      <url>%2F2016%2F04%2F03%2FLinux%E9%99%90%E6%B5%81%E5%B7%A5%E5%85%B7%E4%B9%8Bpv%2F</url>
      <content type="text"><![CDATA[pv是一款Liunx下的限流工具，可以使用该工具查看任务进度，传输速率，使用时间以及预估完成时间，还能限制传输速率 pv示例# 我的MySQL数据库中有张表t，数据量如下wing@3306&gt;select count(*) from t;+----------+| count(*) |+----------+| 25165824 |+----------+1 row in set (10.94 sec)[root@wing wing]# du -h t.*12K t.frm741M t.ibd# 此时我不使用pv工具完成该表的备份,在无任何输出的界面上等待一段时间[root@wing ~]# mysqldump -uroot -P3306 -h127.0.0.1 wing t &gt; dump.sql[root@wing ~]## 此时我使用pv工具完成该表的备份,我们可以看到文件已经传输的大小，使用时间，传输速率，已经进度条（感觉进度条不是特别准确的样子）[root@wing ~]# mysqldump -uroot -P3306 -h127.0.0.1 wing t | pv &gt; dump.sql 429MiB 0:00:32 [13.1MiB/s] [ &lt;=&gt; ] pv参数详解Usage: pv [OPTION] [FILE]…Concatenate FILE(s), or standard input, to standard output,with monitoring. -p, –progressshow progress bar显示进度条（目测不是很准的样子==）（默认） -t, –timershow elapsed time显示任务已经进行的时长（默认） -e, –eta show estimated time of arrival (completion)显示剩余多长时间完成（默认，但好像并不能显示） -r, –rateshow data transfer rate counter显示当前传输速率（默认） -a, –average-rateshow data transfer average rate counter显示平均传输速率 -b, –bytesshow number of bytes transferred -F, –format FORMATset output format to FORMAT -n, –numericoutput percentages, not visual information显示进度百分比 -q, –quietdo not output any transfer information at all不输出任何信息 -W, –wait display nothing until first byte transferred -s, –size SIZEset estimated data size to SIZE bytes -l, –line-modecount lines instead of bytes -i, –interval SECupdate every SEC seconds -w, –width WIDTHassume terminal is WIDTH characters wide -H, –height HEIGHTassume terminal is HEIGHT rows high -N, –name NAMEprefix visual information with NAME -f, –forceoutput even if standard error is not a terminal -c, –cursoruse cursor positioning escape sequences L, –rate-limit RATElimit transfer to RATE bytes per second限制每秒的传输速率，RATE可为n,nK,nM,nG -B, –buffer-size BYTESuse a buffer size of BYTES -E, –skip-errorsskip read errors in input -S, –stop-at-sizestop after –size bytes have been transferred -R, –remote PIDupdate settings of process PID -P, –pidfile FILEsave process ID in FILE -h, –helpshow this help and exit -V, –versionshow version information and exit Please report any bugs to Andrew Wood &#97;&#x6e;&#100;&#x72;&#x65;&#x77;&#x2e;&#119;&#111;&#111;&#x64;&#64;&#105;&#x76;&#x61;&#x72;&#x63;&#x68;&#46;&#x63;&#x6f;&#109;.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LGWR触发条件]]></title>
      <url>%2F2016%2F04%2F02%2FLGWR%E8%A7%A6%E5%8F%91%E6%9D%A1%E4%BB%B6%2F</url>
      <content type="text"><![CDATA[Oracle中LGWR进程和MySQL的redo log刷新方式很相似。所以记录下来。 用户commit； 有1/3的redolog buffer未被写入磁盘 有大于1M的redolog buffer未被写入磁盘 每隔3s触发一次LGWR DBWR需要写入的数据的SCN大于LGWR记录的SCN,DBWR触发LGWR写入 LGWR特点 写频繁 一次写的量较小 顺序写 redo log需要放在写性能比较好的磁盘上，如固态盘、raid10、raid01（raid5/6写性能较差）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[MySQL常用的备份工具]]></title>
      <url>%2F2016%2F04%2F02%2FMySQL%E5%B8%B8%E7%94%A8%E7%9A%84%E5%A4%87%E4%BB%BD%E5%B7%A5%E5%85%B7%2F</url>
      <content type="text"><![CDATA[最近在面试过程中，总是遇到面试官问：平时工作中经常是如何备份的，使用哪些备份工具。于是还是默默的寻找资料总结一下吧。 常用备份工具 mysql复制 逻辑备份(mysqldump，mydumper) 物理备份(copy,xtrabackup) 备份工具差异对比1.mysql复制相对于其他的备份来说，得到的备份数据比较实时。2.逻辑备份：分表比较容易。mysqldump备份数据时是将所有sql语句整合在同一个文件中；mydumper备份数据时是将SQL语句按照表拆分成单个的sql文件，每个sql文件对应一个完整的表。3.物理备份：拷贝即可用，速度快。copy:直接拷贝文件到数据目录下，可能引起表损坏或者数据不一致。xtrabackup对于innodb表是不需要锁表的，对于myisam表仍然需要锁表。 拓展xtrabackup的原理xtrabackup利用innodb的crash recovery的原理，备份innodb表时不需要锁表，xtrabackup一页一页复制data pages，并且xtrabackup还有另外一个线程监视redo log，一旦redo log发生变化，xtrabackup就会将log pages复制走（因为redo log是循环的，所以为了防止日志被覆盖，redo log一旦发生变化，就会立刻被xtrabackup复制走）【由于数据是复制出来的，所以备份出来的数据是不一致的，待恢复时，结合redo log利用crush recovery的原理恢复一致性的数据】。xtrabackup对于myisam表是需要锁表的。 mysqldump的原理参考链接：http://tencentdba.com/blog/mysqldump-backup-principle/首先呢，对于mysqldump来说是需要锁表的，除了–single-transaction单独使用的时候是不需要锁表的。以下分析均以innodb存储引擎为例子。 mysqldump不添加任何参数在mysqldump准备备份某个表的数据之前，mysqldump会对该表添加read表级锁，直到该表数据备份完毕，才会释放锁。 # 备份sqlmysqldump -uroot -h127.0.0.1 -P3306 wing t1 &gt; dump1.sql# general log160307 9:22:15 3 Connect root@127.0.0.1 on 3 Query /*!40100 SET @@SQL_MODE=&apos;&apos; */ 3 Query /*!40103 SET TIME_ZONE=&apos;+00:00&apos; */ 3 Query SHOW VARIABLES LIKE &apos;gtid\_mode&apos; 3 Query SELECT LOGFILE_GROUP_NAME, FILE_NAME, TOTAL_EXTENTS, INITIAL_SIZE, ENGINE, EXTRA FROM INFORMATION_SCHEMA.FILES WHERE FILE_TYPE = &apos;UNDO LOG&apos; AND FILE_NAME IS NOT NULL AND LOGFILE_GROUP_NAME IN (SELECT DISTINCT LOGFILE_GROUP_NAME FROM INFORMATION_SCHEMA.FILES WHERE FILE_TYPE = &apos;DATAFILE&apos; AND TABLESPACE_NAME IN (SELECT DISTINCT TABLESPACE_NAME FROM INFORMATION_SCHEMA.PARTITIONS WHERE TABLE_SCHEMA=&apos;wing&apos; AND TABLE_NAME IN (&apos;t1&apos;))) GROUP BY LOGFILE_GROUP_NAME, FILE_NAME, ENGINE ORDER BY LOGFILE_GROUP_NAME 3 Query SELECT DISTINCT TABLESPACE_NAME, FILE_NAME, LOGFILE_GROUP_NAME, EXTENT_SIZE, INITIAL_SIZE, ENGINE FROM INFORMATION_SCHEMA.FILES WHERE FILE_TYPE = &apos;DATAFILE&apos; AND TABLESPACE_NAME IN (SELECT DISTINCT TABLESPACE_NAME FROM INFORMATION_SCHEMA.PARTITIONS WHERE TABLE_SCHEMA=&apos;wing&apos; AND TABLE_NAME IN (&apos;t1&apos;)) ORDER BY TABLESPACE_NAME, LOGFILE_GROUP_NAME 3 Query SHOW VARIABLES LIKE &apos;ndbinfo\_version&apos; 3 Init DB wing 3 Query SHOW TABLES LIKE &apos;t1&apos; 3 Query LOCK TABLES `t1` READ /*!32311 LOCAL */ # 表备份开始时，锁表 3 Query show table status like &apos;t1&apos; 3 Query SET SQL_QUOTE_SHOW_CREATE=1 3 Query SET SESSION character_set_results = &apos;binary&apos; 3 Query show create table `t1` 3 Query SET SESSION character_set_results = &apos;utf8&apos; 3 Query show fields from `t1` 3 Query SELECT /*!40001 SQL_NO_CACHE */ * FROM `t1` 3 Query SET SESSION character_set_results = &apos;binary&apos; 3 Query use `wing` 3 Query select @@collation_database 3 Query SHOW TRIGGERS LIKE &apos;t1&apos; 3 Query SET SESSION character_set_results = &apos;utf8&apos; 3 Query UNLOCK TABLES # 表备份结束后u，释放表 3 Quit 添加–single-trasaction参数的mysqldump在备份数据之前，mysqldump会做两件事情，首先将数据库的隔离级别设置为RR模式，然后发出start trasaction语句，开启事务（在RR级别下，此时利用innodb的mvcc得到一份数据快照，保证了数据的一致性），最后开始备份数据，期间无需锁表。 # 备份sqlmysqldump -uroot -h127.0.0.1 -P3306 --single-transaction wing t1 &gt; dump1.sql# general log160307 9:30:56 5 Connect root@127.0.0.1 on 5 Query /*!40100 SET @@SQL_MODE=&apos;&apos; */ 5 Query /*!40103 SET TIME_ZONE=&apos;+00:00&apos; */ 5 Query SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ # 设置RR隔离级别 5 Query START TRANSACTION /*!40100 WITH CONSISTENT SNAPSHOT */ # 开启一致性读事务 5 Query SHOW VARIABLES LIKE &apos;gtid\_mode&apos; 5 Query UNLOCK TABLES 5 Query SELECT LOGFILE_GROUP_NAME, FILE_NAME, TOTAL_EXTENTS, INITIAL_SIZE, ENGINE, EXTRA FROM INFORMATION_SCHEMA.FILES WHERE FILE_TYPE = &apos;UNDO LOG&apos; AND FILE_NAME IS NOT NULL AND LOGFILE_GROUP_NAME IN (SELECT DISTINCT LOGFILE_GROUP_NAME FROM INFORMATION_SCHEMA.FILES WHERE FILE_TYPE = &apos;DATAFILE&apos; AND TABLESPACE_NAME IN (SELECT DISTINCT TABLESPACE_NAME FROM INFORMATION_SCHEMA.PARTITIONS WHERE TABLE_SCHEMA=&apos;wing&apos; AND TABLE_NAME IN (&apos;t1&apos;))) GROUP BY LOGFILE_GROUP_NAME, FILE_NAME, ENGINE ORDER BY LOGFILE_GROUP_NAME 5 Query SELECT DISTINCT TABLESPACE_NAME, FILE_NAME, LOGFILE_GROUP_NAME, EXTENT_SIZE, INITIAL_SIZE, ENGINE FROM INFORMATION_SCHEMA.FILES WHERE FILE_TYPE = &apos;DATAFILE&apos; AND TABLESPACE_NAME IN (SELECT DISTINCT TABLESPACE_NAME FROM INFORMATION_SCHEMA.PARTITIONS WHERE TABLE_SCHEMA=&apos;wing&apos; AND TABLE_NAME IN (&apos;t1&apos;)) ORDER BY TABLESPACE_NAME, LOGFILE_GROUP_NAME 5 Query SHOW VARIABLES LIKE &apos;ndbinfo\_version&apos; 5 Init DB wing 5 Query SHOW TABLES LIKE &apos;t1&apos; 5 Query SAVEPOINT sp 5 Query show table status like &apos;t1&apos; 5 Query SET SQL_QUOTE_SHOW_CREATE=1 5 Query SET SESSION character_set_results = &apos;binary&apos; 5 Query show create table `t1` 5 Query SET SESSION character_set_results = &apos;utf8&apos; 5 Query show fields from `t1` 5 Query SELECT /*!40001 SQL_NO_CACHE */ * FROM `t1` 5 Query SET SESSION character_set_results = &apos;binary&apos; 5 Query use `wing` 5 Query select @@collation_database 5 Query SHOW TRIGGERS LIKE &apos;t1&apos; 5 Query SET SESSION character_set_results = &apos;utf8&apos; 5 Query ROLLBACK TO SAVEPOINT sp 5 Query RELEASE SAVEPOINT sp 5 Quit 正常使用mysqldump备份必携带–single-transaction,–master-data=2参数这两个参数的使用可以保证数据的一致性以及数据的不丢失。但是mysqldump开始的时候会有短暂的锁。160307 10:30:58 3 Connect root@127.0.0.1 on 3 Query /*!40100 SET @@SQL_MODE=&apos;&apos; */ 3 Query /*!40103 SET TIME_ZONE=&apos;+00:00&apos; */ 3 Query FLUSH /*!40101 LOCAL */ TABLES 3 Query FLUSH TABLES WITH READ LOCK # flush tables 3 Query SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ # 开启RR隔离事务级别 3 Query START TRANSACTION /*!40100 WITH CONSISTENT SNAPSHOT */ # 开启一致性读事务 3 Query SHOW VARIABLES LIKE &apos;gtid\_mode&apos; 3 Query SHOW MASTER STATUS 3 Query UNLOCK TABLES 3 Query SELECT LOGFILE_GROUP_NAME, FILE_NAME, TOTAL_EXTENTS, INITIAL_SIZE, ENGINE, EXTRA FROM INFORMATION_SCHEMA.FILES WHERE FILE_TYPE = &apos;UNDO LOG&apos; AND FILE_NAME IS NOT NULL AND LOGFILE_GROUP_NAME IN (SELECT DISTINCT LOGFILE_GROUP_NAME FROM INFORMATION_SCHEMA.FILES WHERE FILE_TYPE = &apos;DATAFILE&apos; AND TABLESPACE_NAME IN (SELECT DISTINCT TABLESPACE_NAME FROM INFORMATION_SCHEMA.PARTITIONS WHERE TABLE_SCHEMA=&apos;wing&apos; AND TABLE_NAME IN (&apos;t1&apos;))) GROUP BY LOGFILE_GROUP_NAME, FILE_NAME, ENGINE ORDER BY LOGFILE_GROUP_NAME 3 Query SELECT DISTINCT TABLESPACE_NAME, FILE_NAME, LOGFILE_GROUP_NAME, EXTENT_SIZE, INITIAL_SIZE, ENGINE FROM INFORMATION_SCHEMA.FILES WHERE FILE_TYPE = &apos;DATAFILE&apos; AND TABLESPACE_NAME IN (SELECT DISTINCT TABLESPACE_NAME FROM INFORMATION_SCHEMA.PARTITIONS WHERE TABLE_SCHEMA=&apos;wing&apos; AND TABLE_NAME IN (&apos;t1&apos;)) ORDER BY TABLESPACE_NAME, LOGFILE_GROUP_NAME 3 Query SHOW VARIABLES LIKE &apos;ndbinfo\_version&apos; 3 Init DB wing 3 Query SHOW TABLES LIKE &apos;t1&apos; 3 Query SAVEPOINT sp 3 Query show table status like &apos;t1&apos; 3 Query SET SQL_QUOTE_SHOW_CREATE=1 3 Query SET SESSION character_set_results = &apos;binary&apos; 3 Query show create table `t1` 3 Query SET SESSION character_set_results = &apos;utf8&apos; 3 Query show fields from `t1` 3 Query SELECT /*!40001 SQL_NO_CACHE */ * FROM `t1` 3 Query SET SESSION character_set_results = &apos;binary&apos; 3 Query use `wing` 3 Query select @@collation_database 3 Query SHOW TRIGGERS LIKE &apos;t1&apos; 3 Query SET SESSION character_set_results = &apos;utf8&apos; 3 Query ROLLBACK TO SAVEPOINT sp 3 Query RELEASE SAVEPOINT sp 3 Quit Tips在早上4点备份开始备份，5点备份完毕，那么此时mysqldump –single-transaction –master-data=2到底备份的是几点数据呢？ 答案是4点的。对于innodb，使用RR级别以及一致性快照读，获取的是4点的数据快照。对于myisam，使用锁表的方式，等myisam备份完毕后，才会释放锁，得到的仍旧是4点的快照数据。# wing数据库下存在的表以及表结构# 注意每张表不同的存储引擎哦[wing]root@127.0.0.1 : wing 04:06:52&gt; show tables;+----------------+| Tables_in_wing |+----------------+| t1 || t2 || t3 || t4 |+----------------+4 rows in set (0.00 sec)[wing]root@127.0.0.1 : wing 04:06:54&gt; show create table t1;+-------+--------------------------------------------------------------------------------------+| Table | Create Table |+-------+--------------------------------------------------------------------------------------+| t1 | CREATE TABLE `t1` ( `id` int(11) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8 |+-------+--------------------------------------------------------------------------------------+1 row in set (0.00 sec)[wing]root@127.0.0.1 : wing 04:07:03&gt; show create table t2;+-------+--------------------------------------------------------------------------------------+| Table | Create Table |+-------+--------------------------------------------------------------------------------------+| t2 | CREATE TABLE `t2` ( `id` int(11) DEFAULT NULL) ENGINE=MyISAM DEFAULT CHARSET=utf8 |+-------+--------------------------------------------------------------------------------------+1 row in set (0.00 sec)[wing]root@127.0.0.1 : wing 04:07:06&gt; show create table t3;+-------+--------------------------------------------------------------------------------------+| Table | Create Table |+-------+--------------------------------------------------------------------------------------+| t3 | CREATE TABLE `t3` ( `id` int(11) DEFAULT NULL) ENGINE=MyISAM DEFAULT CHARSET=utf8 |+-------+--------------------------------------------------------------------------------------+1 row in set (0.00 sec)[wing]root@127.0.0.1 : wing 04:07:11&gt; show create table t4;+-------+--------------------------------------------------------------------------------------+| Table | Create Table |+-------+--------------------------------------------------------------------------------------+| t4 | CREATE TABLE `t4` ( `id` int(11) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8 |+-------+--------------------------------------------------------------------------------------+1 row in set (0.00 sec)# general log/usr/sbin/mysqld, Version: 5.6.28-log (MySQL Community Server (GPL)). started with:Tcp port: 3307 Unix socket: /data/mysql/mysqldata3307/sock/mysql.sockTime Id Command Argument160528 16:02:29 18 Query show variables like 'general_log'160528 16:02:53 19 Connect root@127.0.0.1 on 19 Query /*!40100 SET @@SQL_MODE='' */ 19 Query /*!40103 SET TIME_ZONE='+00:00' */ 19 Query FLUSH /*!40101 LOCAL */ TABLES 19 Query FLUSH TABLES WITH READ LOCK # flush tables 19 Query SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ # 设置RR事务隔离级别 19 Query START TRANSACTION /*!40100 WITH CONSISTENT SNAPSHOT */ # 开启一致性读事务 19 Query SHOW VARIABLES LIKE 'gtid\_mode' 19 Query SHOW MASTER STATUS 19 Query UNLOCK TABLES 19 Query SELECT LOGFILE_GROUP_NAME, FILE_NAME, TOTAL_EXTENTS, INITIAL_SIZE, ENGINE, EXTRA FROM INFORMATION_SCHEMA.FILES WHERE FILE_TYPE = 'UNDO LOG' AND FILE_NAME IS NOT NULL AND LOGFILE_GROUP_NAME IN (SELECT DISTINCT LOGFILE_GROUP_NAME FROM INFORMATION_SCHEMA.FILES WHERE FILE_TYPE = 'DATAFILE' AND TABLESPACE_NAME IN (SELECT DISTINCT TABLESPACE_NAME FROM INFORMATION_SCHEMA.PARTITIONS WHERE TABLE_SCHEMA IN ('wing'))) GROUP BY LOGFILE_GROUP_NAME, FILE_NAME, ENGINE ORDER BY LOGFILE_GROUP_NAME 19 Query SELECT DISTINCT TABLESPACE_NAME, FILE_NAME, LOGFILE_GROUP_NAME, EXTENT_SIZE, INITIAL_SIZE, ENGINE FROM INFORMATION_SCHEMA.FILES WHERE FILE_TYPE = 'DATAFILE' AND TABLESPACE_NAME IN (SELECT DISTINCT TABLESPACE_NAME FROM INFORMATION_SCHEMA.PARTITIONS WHERE TABLE_SCHEMA IN ('wing')) ORDER BY TABLESPACE_NAME, LOGFILE_GROUP_NAME 19 Query SHOW VARIABLES LIKE 'ndbinfo\_version' 19 Init DB wing 19 Query SHOW CREATE DATABASE IF NOT EXISTS `wing` 19 Query SAVEPOINT sp 19 Query show tables 19 Query show table status like 't1' # 开始备份t1表（innodb） 19 Query SET SQL_QUOTE_SHOW_CREATE=1 19 Query SET SESSION character_set_results = 'binary' 19 Query show create table `t1` 19 Query SET SESSION character_set_results = 'utf8' 19 Query show fields from `t1` 19 Query SELECT /*!40001 SQL_NO_CACHE */ * FROM `t1` 19 Query SET SESSION character_set_results = 'binary' 19 Query use `wing` 19 Query select @@collation_database 19 Query SHOW TRIGGERS LIKE 't1' 19 Query SET SESSION character_set_results = 'utf8' 19 Query ROLLBACK TO SAVEPOINT sp 19 Query show table status like 't2' # 开始备份t2表（myisam） 19 Query SET SQL_QUOTE_SHOW_CREATE=1 19 Query SET SESSION character_set_results = 'binary' 19 Query show create table `t2` 19 Query SET SESSION character_set_results = 'utf8' 19 Query show fields from `t2` 19 Query SELECT /*!40001 SQL_NO_CACHE */ * FROM `t2` 19 Query SET SESSION character_set_results = 'binary' 19 Query use `wing` 19 Query select @@collation_database 19 Query SHOW TRIGGERS LIKE 't2' 19 Query SET SESSION character_set_results = 'utf8' 19 Query ROLLBACK TO SAVEPOINT sp 19 Query show table status like 't3' # 开始备份t3表(myisam) 19 Query SET SQL_QUOTE_SHOW_CREATE=1 19 Query SET SESSION character_set_results = 'binary' 19 Query show create table `t3` 19 Query SET SESSION character_set_results = 'utf8' 19 Query show fields from `t3` 19 Query SELECT /*!40001 SQL_NO_CACHE */ * FROM `t3` 19 Query SET SESSION character_set_results = 'binary' 19 Query use `wing` 19 Query select @@collation_database 19 Query SHOW TRIGGERS LIKE 't3' 19 Query SET SESSION character_set_results = 'utf8' 19 Query ROLLBACK TO SAVEPOINT sp 19 Query show table status like 't4' # 开始备份t4表（innodb） 19 Query SET SQL_QUOTE_SHOW_CREATE=1 19 Query SET SESSION character_set_results = 'binary' 19 Query show create table `t4` 19 Query SET SESSION character_set_results = 'utf8' 19 Query show fields from `t4` 19 Query SELECT /*!40001 SQL_NO_CACHE */ * FROM `t4` 19 Query SET SESSION character_set_results = 'binary' 19 Query use `wing` 19 Query select @@collation_database 19 Query SHOW TRIGGERS LIKE 't4' 19 Query SET SESSION character_set_results = 'utf8' 19 Query ROLLBACK TO SAVEPOINT sp 19 Query RELEASE SAVEPOINT sp 19 Quit]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[数据库事物的特性]]></title>
      <url>%2F2016%2F03%2F29%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E7%89%A9%E7%9A%84%E7%89%B9%E6%80%A7%2F</url>
      <content type="text"><![CDATA[前段时间在疯狂的找工作，几乎每次面试都会要问下如题目一样的问题，第一次被问到有点恍惚，于是后来去总结了下，从此就轻松的搞定ACID特性啦~~ 数据库中事物的特性，简称ACID特性。最近在求职过程中，面试官几乎都会问事物的特性是什么。ACID具体代表的是什么呢，能不能举个栗子说说理解呢。。 原子性(Atomicity) 事务的原子性是指事务中包含的所有操作要么都做，要么都不做，保证数据库是一致的。 例如：A帐户向B帐户划账1000，则先将A减少1000，再将B增加1000，这两个动作要么都提交，要么都回退，不可能发生一个有效、 一个无效的情况。 一致性(Consistency) 一致性是指数据库在事务操作前和事务处理后，其中的数据必须都满足业务规则约束。 例如：A、B帐户的总金额在转账前和转帐后必须一致，其中的不一致必须是短暂的，在事务提交前才会出现的。 再如：约定B帐户不能多于1000元，则A转出1000成功，B转入1000失败，最终由原子性得到——整个事务回滚 隔离性(Isolation) 隔离性是数据库允许多个并发事务同时对数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。 例如：在A、B之间转帐时，C同时向A转帐，若同时进行则A、B之间的一致性不能得到满足。所以在A、B事务执行过程中，其他事务不能访问(修改)当前相关的数值。 持久性(Durability) 持久性表示为：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 在提交之前如果系统故障，则所有信息全部丢失。提交之后数据存放在磁盘中，是永久性的。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[四下让你明白MySQL函数之last_insert_id()]]></title>
      <url>%2F2016%2F03%2F29%2F%E5%9B%9B%E4%B8%8B%E8%AE%A9%E4%BD%A0%E6%98%8E%E7%99%BDMySQL%E5%87%BD%E6%95%B0%E4%B9%8Blast-insert-id%2F</url>
      <content type="text"><![CDATA[遇到一个小伙伴说，为什么last_insert_id()得到的结果与预期的不一样呢，于是，我认真的去研究的一下这个参数，研究结果请点击全文。 首先，举个栗子wing@3306&gt;show create table tt;+-------+-----------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+-------+-----------------------------------------------------------------------------------------------------------------------+| tt | CREATE TABLE `tt` ( `id` int(11) NOT NULL AUTO_INCREMENT, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 |+-------+-----------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec)# 没有指定值的时候，last_insert_id()符合预期希望wing@3306&gt;insert into tt values();Query OK, 1 row affected (0.00 sec)wing@3306&gt;select last_insert_id();+------------------+| last_insert_id() |+------------------+| 1 |+------------------+1 row in set (0.00 sec)wing@3306&gt;insert into tt values();Query OK, 1 row affected (0.00 sec)wing@3306&gt;select last_insert_id();+------------------+| last_insert_id() |+------------------+| 2 |+------------------+1 row in set (0.00 sec)# what?不是应该是5么，为什么是第一个插入的值3？last_insert_id开始有一点不符合预期了。。wing@3306&gt;insert into tt values(),(),();Query OK, 3 rows affected (0.01 sec)Records: 3 Duplicates: 0 Warnings: 0wing@3306&gt;select last_insert_id();+------------------+| last_insert_id() |+------------------+| 3 |+------------------+1 row in set (0.00 sec)wing@3306&gt;insert into tt values(),(),();Query OK, 3 rows affected (0.01 sec)Records: 3 Duplicates: 0 Warnings: 0wing@3306&gt;select last_insert_id();+------------------+| last_insert_id() |+------------------+| 6 |+------------------+1 row in set (0.00 sec)# 纳尼？按照预期不是10么？为什么还是之前的6？last_insert_id()我不懂你啊。。wing@3306&gt;insert into tt values(10);Query OK, 1 row affected (0.01 sec)wing@3306&gt;select last_insert_id();+------------------+| last_insert_id() |+------------------+| 6 |+------------------+1 row in set (0.00 sec) 其次，研究一下查阅MySQL官方文档，真的太重要了。。。官方出处：http://dev.mysql.com/doc/refman/5.6/en/information-functions.html#function_last-insert-id 官方文档原话：With no argument, LAST_INSERT_ID() returns a 64-bit value representing the first automatically generated value successfully inserted for an AUTO_INCREMENT column as a result of the most recently executed INSERT statement.翻译：没有参数的last_insert_id()返回的是最近一次针对autoincrement列执行的INSERT语句的第一个自动生成的值。 官方文档原话：If you insert multiple rows using a single INSERT statement, LAST_INSERT_ID() returns the value generated for the first inserted row only. The reason for this is to make it possible to reproduce easily the same INSERT statement against some other server.翻译：如果你在单条INSERT语句中插入多个值，那么last_insert_id()返回的是该INSERT语句第一个自动生成的值。 然后，剖析一下请认真阅读上述翻译中的黑色字体，牢记last_insert_id()的约束。 为什么插入指定的值，last_insert_id()就失效了呢？官方文档明明说了，是自动生成的值啊，不是你指定的值啊，是由autoincremnt计数器自己生成的才能被last_insert_id()追踪到哇。。 为什么多值插入的时候，显示的是第一条插入值啊，last不是最后一个值的意思么啊啊啊。。官方文档明明说了，是最近一次的INSERT语句**自动生成的第一个值**哇哇哇。。 最后，总结一下记住last_insert_id()的约束。。最近一次INSERT语句在autpincrement列上自动生成的第一个值。总结的这句话比翻译的那句话感觉顺口多了==]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[如何在Oracle关闭的情况下修改spfile里面的参数]]></title>
      <url>%2F2016%2F02%2F18%2F%E5%A6%82%E4%BD%95%E5%9C%A8Oracle%E5%85%B3%E9%97%AD%E7%9A%84%E6%83%85%E5%86%B5%E4%B8%8B%E4%BF%AE%E6%94%B9spfile%E9%87%8C%E9%9D%A2%E7%9A%84%E5%8F%82%E6%95%B0%2F</url>
      <content type="text"><![CDATA[在Oracle中pfile参数是可以手动更改的，但是spfile是二进制文件所以不可以手动更改，那么万一碰到了我遇到的情况（见下），修改参数错误，导致Oracle启动不了，一定要修改spfile该怎么办呢？ 问题我使用的Oracle11g，当我敲下如下一段命令后，就让我傻眼了。。alter system set sga_max_size=960M scope=spfile;shutdown immediatestartup 此时的startup报错了，错误为：SQL&gt; startupORA-00844: Parameter not taking MEMORY_TARGET into accountORA-00851: SGA_MAX_SIZE 985661440 cannot be set to more than MEMORY_TARGET 784334848. 原因分析原来在Oracle11g中增加了memory_target参数，sga_max_size必须必memory_target参数小。那么问题来了，此时我已经关闭Oracle了，spfile文件是二进制文件，又不能手动修改，那么我该怎么办呢。。好捉急好捉急。。。 思路通过pfile启动Oracle–&gt;在Oracle中通过create pfile=’’ from spfile=’’取出spfile的内容（pfile是可以手动修改的）–&gt;修改新建的pfile–&gt;以新的pfile启动Oracle–&gt;在Oracle中通过create spfile=’’ from pfile=’’获得修改后的spfile 实战[oracle@wing ~]$ sqlplus / as sysdbaSQL*Plus: Release 11.2.0.4.0 Production on Mon Feb 15 14:04:46 2016Copyright (c) 1982, 2013, Oracle. All rights reserved.Connected to an idle instance.SQL&gt; create pfile='/home/oracle/pfile.new' from spfile='/u01/app/oracle/product/11.2.0/db_1/dbs/spfilewingdb.ora';File created.SQL&gt; exitDisconnected from Oracle Database 11g Enterprise Edition Release 11.2.0.4.0 - 64bit ProductionWith the Partitioning, OLAP, Data Mining and Real Application Testing options通过vi修改pfile.new文件中相应的参数（本文档中是memory_target参数），修改后保存 [oracle@wing ~]$ sqlplus / as sysdbaSQL*Plus: Release 11.2.0.4.0 Production on Mon Feb 15 14:04:46 2016Copyright (c) 1982, 2013, Oracle. All rights reserved.Connected to an idle instance.SQL&gt; startup pfile='/home/oracle/pfile.new'ORACLE instance started.Total System Global Area 810090496 bytesFixed Size 2257520 bytesVariable Size 415239568 bytesDatabase Buffers 390070272 bytesRedo Buffers 2523136 bytesDatabase mounted.Database opened.SQL&gt; create spfile='/u01/app/oracle/product/11.2.0/db_1/dbsspfilewingdb.ora' from pfile='/home/oracle/pfile.new';File created.SQL&gt; shutdown immediateDatabase closed.Database dismounted.ORACLE instance shut down.SQL&gt; exitDisconnected from Oracle Database 11g Enterprise Edition Release 11.2.0.4.0 - 64bit ProductionWith the Partitioning, OLAP, Data Mining and Real Application Testing options[oracle@wing ~]$ sqlplus / as sysdbaSQL*Plus: Release 11.2.0.4.0 Production on Mon Feb 15 14:08:40 2016Copyright (c) 1982, 2013, Oracle. All rights reserved.Connected to an idle instance.SQL&gt; SQL&gt; startupORACLE instance started.Total System Global Area 810090496 bytesFixed Size 2257520 bytesVariable Size 415239568 bytesDatabase Buffers 390070272 bytesRedo Buffers 2523136 bytesDatabase mounted.Database opened.SQL&gt; show parameter memory NAME TYPE------------------------------------ --------------------------------VALUE------------------------------hi_shared_memory_address integer0memory_max_target big integer800Mmemory_target big integer800Mshared_memory_address integer0SQL&gt; show parameter sgaNAME TYPE------------------------------------ --------------------------------VALUE------------------------------lock_sga booleanFALSEpre_page_sga booleanFALSEsga_max_size big integer776Msga_target big integer740M# 至此Oracle使用新的spfile启动成功，参数也得到相应的修改]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[SQL语句在Oracle数据库中的漫游之路]]></title>
      <url>%2F2016%2F02%2F14%2FSQL%E8%AF%AD%E5%8F%A5%E5%9C%A8Oracle%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%AD%E7%9A%84%E6%BC%AB%E6%B8%B8%E4%B9%8B%E8%B7%AF%2F</url>
      <content type="text"><![CDATA[本文档为甲骨论视频笔记。讲述SQL语句在Oracle下的执行过程，由于初学Oracle,存在不完善的地方，以后会日渐完善。 SQL语句通过客户端与Oracle之间的连接（即网络），来到Oracle实例（后称oracle）面前，被Oracle的server process（server对process的进程）接收； server process拿着sql语句会首先进入shared pool中，检查该SQL语句有没有在shared pool中缓存，如果存在shared pool中，那么该sql语句将跳过解析过程，如果没有，则该sql语句将会被解析； Oracle需要将SQL语句解析成执行计划才能被Oracle执行； 解析过程：判断sql语法是否存在问题，判断用户是否有权限执行该SQL语句，判断sql语句涉及的表列等元数据信息是否存在，判断sql语句使用哪种执行方案等；（sql执行计划可以缓存到shared pool中，shared pool用来缓存sql的执行计划使用的） 解析完之后，server process首先到buffer cache中查找是否有该SQL语句存在的数据，如果存在，则通过网络返回给client，如果没有，则继续到dbf文件中获取数据至buffer pool中；（buffer cache用来缓存dbf的数据使用的） 如果需要对表进行修改，那么server process在buffer pool中修改表的数据，然后server process产生修改日志，这些修改日志会写到redolog buffer中，DBWR进程将server process修改后的数据写会磁盘上的dbf文件中，LGWR将redolog buffer中的redolog写到磁盘上的redolog文件中；（server process只负责读不负责写，由后台进程进行写。） 图解SQL语句在Oracle数据库中的漫游之路]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linunx下安装Oracle11gR2]]></title>
      <url>%2F2016%2F02%2F14%2FLinunx%E4%B8%8B%E5%AE%89%E8%A3%85Oracle11gR2%2F</url>
      <content type="text"><![CDATA[本文档参考Oracle官方文档以及老相Oracle视频。建议Oracle安装过程在Xmanager的图形界面以及终端下完成，因为后期oracle安装过程中的弹出框会使用到图形界面。 一、安装Xmanager参考文档：http://wenku.baidu.com/link?url=nDEe4RcMgd6-EVTgoyS9_eH13EdwIrskFzkjvQxprY6MPrgnTfeQ7RfhiPJZ4o6AOHz6Z3q7K4T03X1w9MgV3atm-cfFE7xoQEqYHsg-m2q 1.安装gdm包[root@wing ~]# yum install -y gdm 2.打开/etc/inittab ,到最后一行，查看是否为 id：5：initdefault[root@wing ~]# vi /etc/inittab 3.编辑/etc/gdm/customer.conf文件vi /etc/gdm/customer.conf我的/etc/gdm/customer.conf文件内容如下# GDM configuration storage[daemon][security]AllowRemoteRoot=true[xdmcp]Port=177Enable=1[greeter][chooser][debug] 4.在防火墙中开启177端口编辑/etc/sysconfig/iptables文件:vi /etc/sysconfig/iptables 在-A INPUT -m state –state NEW -m tcp -p tcp –dport 22 -j ACCEPT后面添加两行如下内容:-A INPUT -m state --state NEW -m tcp -p tcp --dport 6000:6010 -j ACCEPT-A INPUT -m state --state NEW -m udp -p udp --dport 177 -j ACCEPT 5.重新启动linuxreboot linux重启后，进入Xmanager里面的Xbrowser里面，即可看到你希望看到的那台linux。 二、安装Oracle之前的准备1.以root用户登录进入Linux中cmd: su - root 2.检查硬件要求内存最小内存1G推荐内存2Gcmd: grep MemTotal /proc/meminfo swap是物理内存的2倍cmd: grep SwapTotal /proc/meminfo /tmp生产环境中需要至少10Gcmd: df -k /tmp/ 操作系统位数必须64位cmd: uname -m 操作系统空间软件至少需要4.7G数据文件磁盘至少需要1.7Gcmd： df -h disk_name 3.检查软件要求操作系统内核# On Oracle Linux 4 and Red Hat Enterprise Linux 42.6.9 or later# On Oracle Linux 5 Update 2 with Red Hat Compatible Kernel2.6.18 or later# On Oracle Linux 5 Update 5 with Red Hat Compatible Kernel2.6.18 or later# On Oracle Linux 5 Update 5 with Unbreakable Enterprise Kernel2.6.32-100.0.19 or later# On Oracle Linux 62.6.32-100.28.5.el6.x86_64 or later# On Oracle Linux 6 with Red Hat Compatible Kernel# Note: Only the distributions and versions listed in the earlier list are# supported. Do not install the software on other versions of Linux.# 82.6.32-71.el6.x86_64 or later# On Oracle Linux 73.8.13-33.el7uek.x86_64 or later# On Oracle Linux 7 with Red Hat Compatible Kernel3.10.0-54.0.1.el7.x86_64 or later# On Red Hat Enterprise Linux 5 Update 22.6.18 or later# On Red Hat Enterprise Linux 5 Update 52.6.18 or later# On Red Hat Enterprise Linux 62.6.32-71.el6.x86_64 or later# On Red Hat Enterprise Linux 73.10.0-54.0.1.el7.x86_64 or later# On Asianux Server 32.6.18 or later# On Asianux Server 42.6.32-71.el6.x86_64 or later# On SUSE Linux Enterprise Server 102.6.16.21 or later# On SUSE Linux Enterprise Server 112.6.27.19 or later cmd: uname -r 安装包需求# The following or later version of packages for Oracle Linux 4 and Red Hat# Enterprise Linux 4 must be installed:binutils-2.15.92.0.2compat-libstdc++-33-3.2.3compat-libstdc++-33-3.2.3 (32 bit)elfutils-libelf-0.97elfutils-libelf-devel-0.97expat-1.95.7gcc-3.4.6gcc-c++-3.4.6glibc-2.3.4-2.41glibc-2.3.4-2.41 (32 bit)glibc-common-2.3.4glibc-devel-2.3.4glibc-headers-2.3.4libaio-0.3.105libaio-0.3.105 (32 bit)libaio-devel-0.3.105libaio-devel-0.3.105 (32 bit)libgcc-3.4.6libgcc-3.4.6 (32-bit)libstdc++-3.4.6libstdc++-3.4.6 (32 bit)libstdc++-devel 3.4.6make-3.80numactl-0.6.4.x86_64pdksh-5.2.14sysstat-5.0.5# The following or later version of packages for Oracle Linux 5, Red Hat Enterprise# Linux 5, and Asianux Server 3 must be installed:binutils-2.17.50.0.6compat-libstdc++-33-3.2.3compat-libstdc++-33-3.2.3 (32 bit)elfutils-libelf-0.125elfutils-libelf-devel-0.125gcc-4.1.2gcc-c++-4.1.2glibc-2.5-24glibc-2.5-24 (32 bit)glibc-common-2.5glibc-devel-2.5glibc-devel-2.5 (32 bit)glibc-headers-2.5ksh-20060214libaio-0.3.106libaio-0.3.106 (32 bit)libaio-devel-0.3.106libaio-devel-0.3.106 (32 bit)libgcc-4.1.2libgcc-4.1.2 (32 bit)libstdc++-4.1.2libstdc++-4.1.2 (32 bit)libstdc++-devel 4.1.2make-3.81sysstat-7.0.2# The following or later version of packages for Oracle Linux 6, Red Hat Enterprise# Linux 6, and Asianux Server 4 must be installed:binutils-2.20.51.0.2-5.11.el6 (x86_64)compat-libcap1-1.10-1 (x86_64)compat-libstdc++-33-3.2.3-69.el6 (x86_64)compat-libstdc++-33-3.2.3-69.el6.i686gcc-4.4.4-13.el6 (x86_64)gcc-c++-4.4.4-13.el6 (x86_64)glibc-2.12-1.7.el6 (i686)glibc-2.12-1.7.el6 (x86_64)glibc-devel-2.12-1.7.el6 (x86_64)glibc-devel-2.12-1.7.el6.i686kshlibgcc-4.4.4-13.el6 (i686)libgcc-4.4.4-13.el6 (x86_64)libstdc++-4.4.4-13.el6 (x86_64)libstdc++-4.4.4-13.el6.i686libstdc++-devel-4.4.4-13.el6 (x86_64)libstdc++-devel-4.4.4-13.el6.i686libaio-0.3.107-10.el6 (x86_64)libaio-0.3.107-10.el6.i686libaio-devel-0.3.107-10.el6 (x86_64)libaio-devel-0.3.107-10.el6.i686make-3.81-19.el6sysstat-9.0.4-11.el6 (x86_64)# The following or later version of packages for Oracle Linux 7, and Red Hat# Enterprise Linux 7 must be installed:binutils-2.23.52.0.1-12.el7.x86_64compat-libcap1-1.10-3.el7.x86_64gcc-4.8.2-3.el7.x86_64gcc-c++-4.8.2-3.el7.x86_64glibc-2.17-36.el7.i686glibc-2.17-36.el7.x86_64glibc-devel-2.17-36.el7.i686glibc-devel-2.17-36.el7.x86_64kshlibaio-0.3.109-9.el7.i686libaio-0.3.109-9.el7.x86_64libaio-devel-0.3.109-9.el7.i686libaio-devel-0.3.109-9.el7.x86_64libgcc-4.8.2-3.el7.i686libgcc-4.8.2-3.el7.x86_64libstdc++-4.8.2-3.el7.i686libstdc++-4.8.2-3.el7.x86_64libstdc++-devel-4.8.2-3.el7.i686libstdc++-devel-4.8.2-3.el7.x86_64libXi-1.7.2-1.el7.i686libXi-1.7.2-1.el7.x86_64libXtst-1.2.2-1.el7.i686libXtst-1.2.2-1.el7.x86_64make-3.82-19.el7.x86_64sysstat-10.1.5-1.el7.x86_64# The following or later version of packages for SUSE Linux Enterprise Server 10# must be installed:binutils-2.16.91.0.5compat-libstdc++-5.0.7gcc-4.1.0gcc-c++-4.1.2glibc-2.4-31.63glibc-devel-2.4-31.63glibc-devel-32bit-2.4-31.63ksh-93r-12.9libaio-0.3.104libaio-32bit-0.3.104libaio-devel-0.3.104libaio-devel-32bit-0.3.104libelf-0.8.5libgcc-4.1.2libstdc++-4.1.2libstdc++-devel-4.1.2make-3.80numactl-0.9.6.x86_64sysstat-8.0.4# The following or later version of packages for SUSE Linux Enterprise Server 11# must be installed:binutils-2.19gcc-4.3gcc-32bit-4.3gcc-c++-4.3glibc-2.9glibc-32bit-2.9glibc-devel-2.9glibc-devel-32bit-2.9ksh-93tlibaio-0.3.104libaio-32bit-0.3.104libaio-devel-0.3.104libaio-devel-32bit-0.3.104libstdc++33-3.3.3libstdc++33-32bit-3.3.3libstdc++43-4.3.3_20081022libstdc++43-32bit-4.3.3_20081022libstdc++43-devel-4.3.3_20081022libstdc++43-devel-32bit-4.3.3_20081022libgcc43-4.3.3_20081022libstdc++-devel-4.3make-3.81sysstat-8.1.5 cmd: rpm -q package_name安装所有包cmd: yum install -y binutils compat-libcap1-1.10-1 compat-libstdc++-33-3.2.3-69.el6 compat-libstdc++-33-3.2.3-69.el6.i686 gcc gcc-c++ glibc glibc-devel ksh libgcc-4.4.4-13.el6 libgcc libstdc++ libstdc++-devel libaio-0.3.107-10.el6 libaio-0.3.107-10.el6.i686 libaio-devel-0.3.107-10.el6 libaio-devel-0.3.107-10.el6.i686 make sysstat elfutils-libelf-devel 4.配置网络编辑/etc/sysconfig/network-scripts/ifcfg-eth0文件cmd: vi /etc/sysconfig/network-scripts/ifcfg-eth0# 我的配置文件DEVICE=eth0HWADDR=00:0C:29:F9:44:18TYPE=EthernetUUID=e89b278a-4312-4581-8d80-b3587da19008ONBOOT=yesBOOTPROTO=staticBROADCAST=192.168.98.255IPADDR=192.168.98.128NETMASK=255.255.255.0NETWORK=192.168.98.0# 修改完配置后，记得重启网络哦service network restart 5.配置主机名一旦Oracle安装之后，主机名不可变动哦。cmd: vi /etc/hosts# 我的配置文件127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.98.128 wing 6.创建操作系统的用户和组1) 确认oinstall用户组是否存在：`more /etc/oraInst.loc`，如果oinstall用户组存在，则存在该文件，如果oinstall用户组不存在，则不存在该文件。 [root@wing ~]# more /etc/oraInst.loc inventory_loc=/u01/app/oraInventoryinst_group=oinstall 2) 确认dba用户组存在：`grep dba /etc/group` 3) 确认oracle用户存在，并且属于正确的用户组：`id oracle` # 正确输出如下：[root@wing ~]# id oracleuid=500(oracle) gid=500(oinstall) groups=500(oinstall),501(dba) 创建cmd:# 添加oinstall用户组groupadd oinstall# 添加dba用户组groupadd dba# 添加oracle用户，并归属于正确的用户组useradd -g oinstall -G dba oracle# 如果oracle用户已存在，修改其用户组usermod -g oinstall -G dba oracle# 为oracle用户加密passwd oracle 7.调整Linux参数调整系统参数查看参数值cmd: sysctl -a | grep 参数名称建议参数调整cmd: vi /etc/sysctl.conf# 添加如下参数fs.aio-max-nr = 1048576fs.file-max = 6815744kernel.shmall = 2097152kernel.shmmax = 980271104kernel.shmmni = 4096kernel.sem = 250 32000 100 128net.ipv4.ip_local_port_range = 9000 65500net.core.rmem_default = 262144net.core.rmem_max = 4194304net.core.wmem_default = 262144net.core.wmem_max = 1048576# 使参数生效sysctl -p 调整limits参数cmd: vi /etc/security/limits.conf# 添加如下参数oracle soft nproc 2047oracle hard nproc 16384oracle soft nofile 1024oracle hard nofile 65536 调整login参数cmd: vi /etc/pam.d/login# 添加如下参数session required pam_limits.so 调整/etc/profile参数cmd: vi /etc/profile# 添加如下参数if [ $USER = &quot;oracle&quot; ]; then if [ $SHELL = &quot;/bin/ksh&quot; ]; then ulimit -p 16384 ulimit -n 65536 else ulimit -u 16384 -n 65536 fifi 8.为Oracle软件建立目录此处假设Oracle软件目录为 /u01/app/oraclecmd：# 建立目录mkdir -p /u01/app/oracle# 修改目录用户和组chown -R oracle:oinstall /u01/app/# 修改目录权限chmod -R 755 /u01/app/ 9.配置Oracle用户环境变量cmd:# 切换至oracle用户下su - oracle# 编辑环境变量文件vi .bash_profile# 添加如下环境变量export ORACLE_BASE=/u01/app/oracleexport ORACLE_HOME=/u01/app/oracle/product/11.2.0/db_1export ORACLE_SID=wingorclexport NLS_LANG=american_america.AL32UTF8export PATH=$PATH:$ORACLE_HOME/bin:.# 此刻需要重新切换至oracle用户才能生效 10.解压oracle11gR2软件至相应目录中cmd：unzip linux.x64_11gR2_database_1of2.zip -d /oraapp/unzip linux.x64_11gR2_database_2of2.zip -d /oraapp/ 此时进入/oraapp目录下，可以看到database目录，即为我们所需要的Oracle11gR2安装文件 11.安装前需要在界面操作的terminal中做如下操作[root@wing database]# w 11:32:46 up 2:29, 2 users, load average: 0.05, 0.11, 0.06USER TTY FROM LOGIN@ IDLE JCPU PCPU WHATroot tty1 :0 09:05 2:29m 3.38s 3.38s /usr/bin/Xorg :0 -nr -verbose -audit 4 -auth /var/run/gdm/auth-for-gdm-tBSNYD/database -nolisten tcp vt1root pts/1 192.168.98.1 10:27 0.00s 0.26s 0.01s w[root@wing database]# su - oracle[oracle@wing database]# export DISPLAY=192.168.98.1:0.0# 执行xhost +，显示如下命令即可[oracle@wing ~]# xhost +access control disabled, clients can connect from any host 三、安装Oracle1.执行Oracle安装脚本[root@wing database]# su - oracle[oracle@wing ~]$ cd /oraapp/database/# 使用oracle用户执行安装脚本[oracle@wing database]$ ./runInstaller 2.使用Xmanager连接主机接收图形界面（图片省略）3.按照图形界面提示执，最后需要以root用户手动执行两个脚本sh /u01/app/oraInventory/orainstRoot.shsh /u01/app/oracle/product/11.2.0/db_1/root.sh 四、在Oracle下建库1.使用netca配置监听[oracle@wing ~]$ xhost +access control disabled, clients can connect from any host[oracle@wing ~]$ dbca[oracle@wing ~]$ netca# 此处出现图形界面进行配置，配置成功后，输出结果如下Oracle Net Services Configuration:Configuring Listener:WING_LISTENERListener configuration complete.Oracle Net Listener Startup: Running Listener Control: /u01/app/oracle/product/11.2.0/db_1/bin/lsnrctl start WING_LISTENER Listener Control complete. Listener started successfully.Oracle Net Services configuration successful. The exit code is 0 2.使用dbca建立数据库[root@wing ~]# su - oracle[oracle@wing ~]$ dbca# 此时出现图形界面，根据界面提示完成oracle数据库建立 五、Oracle常用软件的启动与关闭Oracle启动listener–&gt;Oracle–&gt;EM##### listener# listener启动[oracle@wing ~]$ lsnrctl startLSNRCTL for Linux: Version 11.2.0.4.0 - Production on 11-FEB-2016 19:25:02Copyright (c) 1991, 2013, Oracle. All rights reserved.Starting /u01/app/oracle/product/11.2.0/db_1/bin/tnslsnr: please wait...TNSLSNR for Linux: Version 11.2.0.4.0 - ProductionSystem parameter file is /u01/app/oracle/product/11.2.0/db_1/network/admin/listener.oraLog messages written to /u01/app/oracle/diag/tnslsnr/wing/listener/alert/log.xmlListening on: (DESCRIPTION=(ADDRESS=(PROTOCOL=tcp)(HOST=wing)(PORT=1521)))Connecting to (ADDRESS=(PROTOCOL=tcp)(HOST=)(PORT=1521))STATUS of the LISTENER------------------------Alias LISTENERVersion TNSLSNR for Linux: Version 11.2.0.4.0 - ProductionStart Date 11-FEB-2016 19:25:03Uptime 0 days 0 hr. 0 min. 20 secTrace Level offSecurity ON: Local OS AuthenticationSNMP OFFListener Parameter File /u01/app/oracle/product/11.2.0/db_1/network/admin/listener.oraListener Log File /u01/app/oracle/diag/tnslsnr/wing/listener/alert/log.xmlListening Endpoints Summary... (DESCRIPTION=(ADDRESS=(PROTOCOL=tcp)(HOST=wing)(PORT=1521)))The listener supports no servicesThe command completed successfully# listener状态查看[oracle@wing ~]$ lsnrctl statusLSNRCTL for Linux: Version 11.2.0.4.0 - Production on 11-FEB-2016 19:26:44Copyright (c) 1991, 2013, Oracle. All rights reserved.Connecting to (ADDRESS=(PROTOCOL=tcp)(HOST=)(PORT=1521))STATUS of the LISTENER------------------------Alias LISTENERVersion TNSLSNR for Linux: Version 11.2.0.4.0 - ProductionStart Date 11-FEB-2016 19:25:03Uptime 0 days 0 hr. 1 min. 41 secTrace Level offSecurity ON: Local OS AuthenticationSNMP OFFListener Parameter File /u01/app/oracle/product/11.2.0/db_1/network/admin/listener.oraListener Log File /u01/app/oracle/diag/tnslsnr/wing/listener/alert/log.xmlListening Endpoints Summary... (DESCRIPTION=(ADDRESS=(PROTOCOL=tcp)(HOST=wing)(PORT=1521)))The listener supports no servicesThe command completed successfully# listener端口监听查看[oracle@wing ~]$ netstat -npl | grep 1521(Not all processes could be identified, non-owned process info will not be shown, you would have to be root to see it all.)tcp 0 0 :::1521 :::* LISTEN 17985/tnslsnr##### Oracle# Oracle启动[oracle@wing ~]$ sqlplus / as sysdbaSQL*Plus: Release 11.2.0.4.0 Production on Thu Feb 11 19:28:21 2016Copyright (c) 1982, 2013, Oracle. All rights reserved.Connected to an idle instance.SQL&gt; startupORACLE instance started.Total System Global Area 780824576 bytesFixed Size 2257312 bytesVariable Size 511708768 bytesDatabase Buffers 264241152 bytesRedo Buffers 2617344 bytesDatabase mounted.Database opened.# Oracle状态查看[root@wing ~]# ps -ef | grep oracleroot 13275 13254 0 18:46 pts/1 00:00:00 su - oracleoracle 13276 13275 0 18:46 pts/1 00:00:00 -bashoracle 17985 1 0 19:25 ? 00:00:00 /u01/app/oracle/product/11.2.0/db_1/bin/tnslsnr LISTENER -inheritoracle 18099 1 0 19:29 ? 00:00:00 ora_pmon_wingdboracle 18101 1 0 19:29 ? 00:00:00 ora_psp0_wingdboracle 18103 1 7 19:29 ? 00:00:02 ora_vktm_wingdboracle 18107 1 0 19:29 ? 00:00:00 ora_gen0_wingdboracle 18109 1 0 19:29 ? 00:00:00 ora_diag_wingdboracle 18111 1 0 19:29 ? 00:00:00 ora_dbrm_wingdboracle 18113 1 0 19:29 ? 00:00:00 ora_dia0_wingdboracle 18115 1 0 19:29 ? 00:00:00 ora_mman_wingdboracle 18117 1 0 19:29 ? 00:00:00 ora_dbw0_wingdboracle 18119 1 0 19:29 ? 00:00:00 ora_lgwr_wingdboracle 18121 1 0 19:29 ? 00:00:00 ora_ckpt_wingdboracle 18123 1 0 19:29 ? 00:00:00 ora_smon_wingdboracle 18125 1 0 19:29 ? 00:00:00 ora_reco_wingdboracle 18127 1 0 19:29 ? 00:00:00 ora_mmon_wingdboracle 18129 1 0 19:29 ? 00:00:00 ora_mmnl_wingdboracle 18131 1 0 19:29 ? 00:00:00 ora_d000_wingdboracle 18133 1 0 19:29 ? 00:00:00 ora_s000_wingdboracle 18154 1 0 19:30 ? 00:00:00 ora_qmnc_wingdboracle 18170 1 1 19:30 ? 00:00:00 ora_cjq0_wingdboracle 18172 1 2 19:30 ? 00:00:00 ora_j000_wingdboracle 18174 1 0 19:30 ? 00:00:00 ora_j001_wingdboracle 18175 13276 0 19:30 pts/1 00:00:00 sqlplus as sysdbaoracle 18176 18175 0 19:30 ? 00:00:00 oraclewingdb (DESCRIPTION=(LOCAL=YES)(ADDRESS=(PROTOCOL=beq)))oracle 18178 1 0 19:30 ? 00:00:00 ora_q000_wingdboracle 18180 1 0 19:30 ? 00:00:00 ora_q001_wingdbroot 18183 13222 0 19:30 pts/0 00:00:00 grep oracle##### EM# EM启动[oracle@wing ~]$ emctl start dbconsoleOracle Enterprise Manager 11g Database Control Release 11.2.0.4.0 Copyright (c) 1996, 2013 Oracle Corporation. All rights reserved.https://wing:1158/em/console/aboutApplicationStarting Oracle Enterprise Manager 11g Database Control ........... started. ------------------------------------------------------------------Logs are generated in directory /u01/app/oracle/product/11.2.0/db_1/wing_wingdb/sysman/log # EM状态[oracle@wing ~]$ emctl status dbconsoleOracle Enterprise Manager 11g Database Control Release 11.2.0.4.0 Copyright (c) 1996, 2013 Oracle Corporation. All rights reserved.https://wing:1158/em/console/aboutApplicationOracle Enterprise Manager 11g is running. ------------------------------------------------------------------Logs are generated in directory /u01/app/oracle/product/11.2.0/db_1/wing_wingdb/sysman/log# EM端口查看[oracle@wing ~]$ netstat -npl | grep 1158(Not all processes could be identified, non-owned process info will not be shown, you would have to be root to see it all.)tcp 0 0 :::1158 :::* LISTEN 20715/java Oracle关闭EM–&gt;listener–&gt;Oracle##### EM# EM端口监听查看[oracle@wing ~]$ netstat -npl | grep 1158(Not all processes could be identified, non-owned process info will not be shown, you would have to be root to see it all.)tcp 0 0 :::1158 :::* LISTEN 15338/java# EM关闭操作[oracle@wing ~]$ emctl stop dbconsoleOracle Enterprise Manager 11g Database Control Release 11.2.0.4.0 Copyright (c) 1996, 2013 Oracle Corporation. All rights reserved.https://wing:1158/em/console/aboutApplicationStopping Oracle Enterprise Manager 11g Database Control ... ... Stopped. # EM状态查看[oracle@wing ~]$ emctl status dbconsoleOracle Enterprise Manager 11g Database Control Release 11.2.0.4.0 Copyright (c) 1996, 2013 Oracle Corporation. All rights reserved.https://wing:1158/em/console/aboutApplicationOracle Enterprise Manager 11g is not running.##### listener# listener端口监听查看[oracle@wing ~]$ netstat -npl | grep 1521# listener关闭操作[oracle@wing ~]$ lsnrctl stopLSNRCTL for Linux: Version 11.2.0.4.0 - Production on 11-FEB-2016 19:13:55Copyright (c) 1991, 2013, Oracle. All rights reserved.Connecting to (ADDRESS=(PROTOCOL=tcp)(HOST=)(PORT=1521))The command completed successfully# listener状态查看[oracle@wing ~]$ lsnrctl statusLSNRCTL for Linux: Version 11.2.0.4.0 - Production on 11-FEB-2016 19:14:03Copyright (c) 1991, 2013, Oracle. All rights reserved.Connecting to (ADDRESS=(PROTOCOL=tcp)(HOST=)(PORT=1521))TNS-12541: TNS:no listener TNS-12560: TNS:protocol adapter error TNS-00511: No listener Linux Error: 111: Connection refused##### Oracle# Oracle关闭操作[oracle@wing ~]$ sqlplus / as sysdbaSQL*Plus: Release 11.2.0.4.0 Production on Thu Feb 11 19:14:37 2016Copyright (c) 1982, 2013, Oracle. All rights reserved.Connected to:Oracle Database 11g Enterprise Edition Release 11.2.0.4.0 - 64bit ProductionWith the Partitioning, OLAP, Data Mining and Real Application Testing optionsSQL&gt; SQL&gt; shutdown immediateDatabase closed.Database dismounted.ORACLE instance shut down.SQL&gt; # 检查Oracle是否关闭，重新连接，检查是否有数据库连接SQL&gt; exitDisconnected from Oracle Database 11g Enterprise Edition Release 11.2.0.4.0 - 64bit ProductionWith the Partitioning, OLAP, Data Mining and Real Application Testing options[oracle@wing ~]$ sqlplus / as sysdbaSQL*Plus: Release 11.2.0.4.0 Production on Thu Feb 11 19:16:37 2016Copyright (c) 1982, 2013, Oracle. All rights reserved.Connected to an idle instance.SQL&gt;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[与INNODB thread有关的三个参数]]></title>
      <url>%2F2016%2F02%2F03%2F%E4%B8%8EINNODB-thread%E6%9C%89%E5%85%B3%E7%9A%84%E4%B8%89%E4%B8%AA%E5%8F%82%E6%95%B0%2F</url>
      <content type="text"><![CDATA[对于INNODB并发线程数的见解。。 三个参数innodb_thread_concurrencyINNODB存储引擎中允许的最大的线程并发数。 innodb_thread_sleep_delay单位为毫秒；thread未能进入INNODB存储引擎后，需要等待innodb_thread_sleep_delay毫秒再次尝试进入。 innodb_concurrency_ticketsthread进入INNODB中，会获得innodb_concurrency_tickets次数通行证，该线程在接下来的innodb_concurrency_tickets次进入到INNODB中不需要再进行检查，可直接进入。 三个参数关系当一个thread需要进入到INNODB存储引擎层（以下简称INNODB），INNODB会检查已经进入到INNODB存储引擎层的thread总数是否超过innodb_thread_concurrency，如果超过了，那么该thread需要等待innodb_thread_sleep_delay（单位：毫秒）毫秒再次进行尝试，如果这次尝试失败后，该thread将会进入到FIFO的队列中进行等待唤醒（此时状态为sleeping）。一旦该thread进入到INNODB中，该thread将会获得innodb_concurrency_tickets的通行证，即该thread将会在接下来的innodb_concurrency_tickets次进入到INNODB中都不需要再进行检查，可直接进入。 注意thread尝试两次进入INNODB存储引擎层的目的是，减少等待线程的数量以及减少上下文切换。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[MySQL的SHOW PROFILE详解]]></title>
      <url>%2F2016%2F02%2F02%2FMySQL%E7%9A%84SHOW-PROFILE%E8%AF%A6%E8%A7%A3%2F</url>
      <content type="text"><![CDATA[MySQL中的profiling功能为MySQL的优化提供了另一条路径，可以根据profiling功能查看一条SQL到底是在哪里损失了性能。 SHOW PROFILE语法# 语法SHOW PROFILE [type [, type] ... ] [FOR QUERY n] [LIMIT row_count [OFFSET offset]]type: ALL | BLOCK IO | CONTEXT SWITCHES | CPU | IPC | MEMORY | PAGE FAULTS | SOURCE | SWAPS# 举个栗子mysql&gt; show profiles;+----------+------------+---------------------------------------+| Query_ID | Duration | Query |+----------+------------+---------------------------------------+| 1 | 0.01588975 | show variables like "profiling" || 2 | 0.00014025 | SELECT DATABASE() || 3 | 0.00026475 | show databases || 4 | 0.00014150 | show tables || 5 | 0.00046300 | show variables like "profiling_hist%" || 6 | 0.00909950 | set profiling_history_size=100 || 7 | 0.00045875 | show variables like "profiling_hist%" |+----------+------------+---------------------------------------+7 rows in set, 1 warning (0.03 sec)mysql&gt; show profile for query 7;+----------------------+----------+| Status | Duration |+----------------------+----------+| starting | 0.000040 || checking permissions | 0.000007 || Opening tables | 0.000039 || init | 0.000010 || System lock | 0.000007 || optimizing | 0.000005 || statistics | 0.000012 || preparing | 0.000010 || executing | 0.000247 || Sending data | 0.000016 || end | 0.000005 || query end | 0.000004 || closing tables | 0.000003 || removing tmp table | 0.000007 || closing tables | 0.000003 || freeing items | 0.000031 || cleaning up | 0.000016 |+----------------------+----------+17 rows in set, 1 warning (0.07 sec)mysql&gt; show profile for query 7 limit 3;+----------------------+----------+| Status | Duration |+----------------------+----------+| starting | 0.000040 || checking permissions | 0.000007 || Opening tables | 0.000039 |+----------------------+----------+3 rows in set, 1 warning (0.00 sec)mysql&gt; show profile for query 7 limit 3 offset 5;+------------+----------+| Status | Duration |+------------+----------+| optimizing | 0.000005 || statistics | 0.000012 || preparing | 0.000010 |+------------+----------+3 rows in set, 1 warning (0.00 sec)mysql&gt; show profile all for query 7;+----------------------+----------+----------+------------+-------------------+---------------------+--------------+---------------+---------------+-------------------+-------------------+-------------------+-------+-----------------------+------------------+-------------+| Status | Duration | CPU_user | CPU_system | Context_voluntary | Context_involuntary | Block_ops_in | Block_ops_out | Messages_sent | Messages_received | Page_faults_major | Page_faults_minor | Swaps | Source_function | Source_file | Source_line |+----------------------+----------+----------+------------+-------------------+---------------------+--------------+---------------+---------------+-------------------+-------------------+-------------------+-------+-----------------------+------------------+-------------+| starting | 0.000040 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | NULL | NULL | NULL || checking permissions | 0.000007 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | check_access | sql_parse.cc | 5386 || Opening tables | 0.000039 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | open_tables | sql_base.cc | 5011 || init | 0.000010 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | mysql_prepare_select | sql_select.cc | 1050 || System lock | 0.000007 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | mysql_lock_tables | lock.cc | 304 || optimizing | 0.000005 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | optimize | sql_optimizer.cc | 138 || statistics | 0.000012 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | optimize | sql_optimizer.cc | 362 || preparing | 0.000010 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | optimize | sql_optimizer.cc | 485 || executing | 0.000247 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | exec | sql_executor.cc | 110 || Sending data | 0.000016 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | exec | sql_executor.cc | 190 || end | 0.000005 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | mysql_execute_select | sql_select.cc | 1105 || query end | 0.000004 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | mysql_execute_command | sql_parse.cc | 5085 || closing tables | 0.000003 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | mysql_execute_command | sql_parse.cc | 5133 || removing tmp table | 0.000007 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | free_tmp_table | sql_tmp_table.cc | 1868 || closing tables | 0.000003 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | free_tmp_table | sql_tmp_table.cc | 1897 || freeing items | 0.000031 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | mysql_parse | sql_parse.cc | 6564 || cleaning up | 0.000016 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | dispatch_command | sql_parse.cc | 1776 |+----------------------+----------+----------+------------+-------------------+---------------------+--------------+---------------+---------------+-------------------+-------------------+-------------------+-------+-----------------------+------------------+-------------+17 rows in set, 1 warning (0.00 sec) type之ALL显示所有性能信息。wing@3303&gt;select count(*) from customer;+----------+| count(*) |+----------+| 1000000 |+----------+1 row in set (0.26 sec)wing@3303&gt;select * from customer order by telephone limit 2;+---------+-------+-------------+------------+----------+--------+----------------+| id | name | telephone | provinceid | province | city | address |+---------+-------+-------------+------------+----------+--------+----------------+| 1000000 | TVLJC | 13198765432 | 8 | Guangxi | City X | Street Y No. Z || 2 | FLKZS | 13198765432 | 2 | Shandong | City X | Street Y No. Z |+---------+-------+-------------+------------+----------+--------+----------------+2 rows in set (0.65 sec)wing@3303&gt;show profiles;+----------+------------+---------------------------------------------------+| Query_ID | Duration | Query |+----------+------------+---------------------------------------------------+| 1 | 0.00132425 | show variables like 'profiling_%' || 2 | 0.26232200 | select count(*) from customer || 3 | 0.00023575 | show create tabel customer || 4 | 0.00021000 | show create table customer || 5 | 0.64640025 | select * from customer order by telephone limit 2 |+----------+------------+---------------------------------------------------+5 rows in set, 1 warning (0.01 sec)wing@3303&gt;show profile all for query 5;+----------------------+----------+----------+------------+-------------------+---------------------+--------------+---------------+---------------+-------------------+-------------------+-------------------+-------+-----------------------+------------------+-------------+| Status | Duration | CPU_user | CPU_system | Context_voluntary | Context_involuntary | Block_ops_in | Block_ops_out | Messages_sent | Messages_received | Page_faults_major | Page_faults_minor | Swaps | Source_function | Source_file | Source_line |+----------------------+----------+----------+------------+-------------------+---------------------+--------------+---------------+---------------+-------------------+-------------------+-------------------+-------+-----------------------+------------------+-------------+| starting | 0.000088 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | NULL | NULL | NULL || checking permissions | 0.000015 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | check_access | sql_parse.cc | 5297 || Opening tables | 0.000027 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | open_tables | sql_base.cc | 5025 || init | 0.000031 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | mysql_prepare_select | sql_select.cc | 1050 || System lock | 0.000015 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | mysql_lock_tables | lock.cc | 304 || optimizing | 0.000013 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | optimize | sql_optimizer.cc | 138 || statistics | 0.000022 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | optimize | sql_optimizer.cc | 362 || preparing | 0.000018 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | optimize | sql_optimizer.cc | 485 || Sorting result | 0.000023 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | make_tmp_tables_info | sql_select.cc | 5307 || executing | 0.000009 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | exec | sql_executor.cc | 110 || Sending data | 0.000017 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | exec | sql_executor.cc | 190 || Creating sort index | 0.645990 | 0.661899 | 0.001999 | 106 | 0 | 0 | 272 | 0 | 0 | 0 | 0 | 0 | sort_table | sql_executor.cc | 2504 || end | 0.000035 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | mysql_execute_select | sql_select.cc | 1105 || query end | 0.000014 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | mysql_execute_command | sql_parse.cc | 4996 || closing tables | 0.000020 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | mysql_execute_command | sql_parse.cc | 5044 || freeing items | 0.000032 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | mysql_parse | sql_parse.cc | 6433 || cleaning up | 0.000034 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | dispatch_command | sql_parse.cc | 1778 |+----------------------+----------+----------+------------+-------------------+---------------------+--------------+---------------+---------------+-------------------+-------------------+-------------------+-------+-----------------------+------------------+-------------+17 rows in set, 1 warning (0.00 sec) type之BLOCK IO显示块IO（块的输入输出）的次数。wing@3303&gt;show profile block io for query 5;+----------------------+----------+--------------+---------------+| Status | Duration | Block_ops_in | Block_ops_out |+----------------------+----------+--------------+---------------+| starting | 0.000088 | 0 | 0 || checking permissions | 0.000015 | 0 | 0 || Opening tables | 0.000027 | 0 | 0 || init | 0.000031 | 0 | 0 || System lock | 0.000015 | 0 | 0 || optimizing | 0.000013 | 0 | 0 || statistics | 0.000022 | 0 | 0 || preparing | 0.000018 | 0 | 0 || Sorting result | 0.000023 | 0 | 0 || executing | 0.000009 | 0 | 0 || Sending data | 0.000017 | 0 | 0 || Creating sort index | 0.645990 | 0 | 272 || end | 0.000035 | 0 | 0 || query end | 0.000014 | 0 | 0 || closing tables | 0.000020 | 0 | 0 || freeing items | 0.000032 | 0 | 0 || cleaning up | 0.000034 | 0 | 0 |+----------------------+----------+--------------+---------------+17 rows in set, 1 warning (0.00 sec) type之CONTEXT SWITCHES显示自动和被动的上下文切换数量。wing@3303&gt;show profile context switches for query 5;+----------------------+----------+-------------------+---------------------+| Status | Duration | Context_voluntary | Context_involuntary |+----------------------+----------+-------------------+---------------------+| starting | 0.000088 | 0 | 0 || checking permissions | 0.000015 | 0 | 0 || Opening tables | 0.000027 | 0 | 0 || init | 0.000031 | 0 | 0 || System lock | 0.000015 | 0 | 0 || optimizing | 0.000013 | 0 | 0 || statistics | 0.000022 | 0 | 0 || preparing | 0.000018 | 0 | 0 || Sorting result | 0.000023 | 0 | 0 || executing | 0.000009 | 0 | 0 || Sending data | 0.000017 | 0 | 0 || Creating sort index | 0.645990 | 106 | 0 || end | 0.000035 | 0 | 0 || query end | 0.000014 | 0 | 0 || closing tables | 0.000020 | 0 | 0 || freeing items | 0.000032 | 0 | 0 || cleaning up | 0.000034 | 0 | 0 |+----------------------+----------+-------------------+---------------------+17 rows in set, 1 warning (0.00 sec) type之CPU显示用户和系统的CPU使用情况。wing@3303&gt;show profile cpu for query 5;+----------------------+----------+----------+------------+| Status | Duration | CPU_user | CPU_system |+----------------------+----------+----------+------------+| starting | 0.000088 | 0.000000 | 0.000000 || checking permissions | 0.000015 | 0.000000 | 0.000000 || Opening tables | 0.000027 | 0.000000 | 0.000000 || init | 0.000031 | 0.000000 | 0.000000 || System lock | 0.000015 | 0.000000 | 0.000000 || optimizing | 0.000013 | 0.000000 | 0.000000 || statistics | 0.000022 | 0.000000 | 0.000000 || preparing | 0.000018 | 0.000000 | 0.000000 || Sorting result | 0.000023 | 0.000000 | 0.000000 || executing | 0.000009 | 0.000000 | 0.000000 || Sending data | 0.000017 | 0.000000 | 0.000000 || Creating sort index | 0.645990 | 0.661899 | 0.001999 || end | 0.000035 | 0.000000 | 0.000000 || query end | 0.000014 | 0.000000 | 0.000000 || closing tables | 0.000020 | 0.000000 | 0.000000 || freeing items | 0.000032 | 0.000000 | 0.000000 || cleaning up | 0.000034 | 0.000000 | 0.000000 |+----------------------+----------+----------+------------+17 rows in set, 1 warning (0.00 sec) type之IPC显示发送和接收的消息数量。wing@3303&gt;show profile ipc for query 5;+----------------------+----------+---------------+-------------------+| Status | Duration | Messages_sent | Messages_received |+----------------------+----------+---------------+-------------------+| starting | 0.000088 | 0 | 0 || checking permissions | 0.000015 | 0 | 0 || Opening tables | 0.000027 | 0 | 0 || init | 0.000031 | 0 | 0 || System lock | 0.000015 | 0 | 0 || optimizing | 0.000013 | 0 | 0 || statistics | 0.000022 | 0 | 0 || preparing | 0.000018 | 0 | 0 || Sorting result | 0.000023 | 0 | 0 || executing | 0.000009 | 0 | 0 || Sending data | 0.000017 | 0 | 0 || Creating sort index | 0.645990 | 0 | 0 || end | 0.000035 | 0 | 0 || query end | 0.000014 | 0 | 0 || closing tables | 0.000020 | 0 | 0 || freeing items | 0.000032 | 0 | 0 || cleaning up | 0.000034 | 0 | 0 |+----------------------+----------+---------------+-------------------+17 rows in set, 1 warning (0.00 sec) type之MEMORYMySQL5.6中还未实现，只是计划实现。 type之PAGE FAULTS显示主要的和次要的页面故障。wing@3303&gt;show profile page faults for query 5;+----------------------+----------+-------------------+-------------------+| Status | Duration | Page_faults_major | Page_faults_minor |+----------------------+----------+-------------------+-------------------+| starting | 0.000088 | 0 | 0 || checking permissions | 0.000015 | 0 | 0 || Opening tables | 0.000027 | 0 | 0 || init | 0.000031 | 0 | 0 || System lock | 0.000015 | 0 | 0 || optimizing | 0.000013 | 0 | 0 || statistics | 0.000022 | 0 | 0 || preparing | 0.000018 | 0 | 0 || Sorting result | 0.000023 | 0 | 0 || executing | 0.000009 | 0 | 0 || Sending data | 0.000017 | 0 | 0 || Creating sort index | 0.645990 | 0 | 0 || end | 0.000035 | 0 | 0 || query end | 0.000014 | 0 | 0 || closing tables | 0.000020 | 0 | 0 || freeing items | 0.000032 | 0 | 0 || cleaning up | 0.000034 | 0 | 0 |+----------------------+----------+-------------------+-------------------+17 rows in set, 1 warning (0.00 sec) type之SOURCE显示源代码的函数名称，以及在源码文件名称与行数（即源码中的位置）。wing@3303&gt;show profile source for query 5;+----------------------+----------+-----------------------+------------------+-------------+| Status | Duration | Source_function | Source_file | Source_line |+----------------------+----------+-----------------------+------------------+-------------+| starting | 0.000088 | NULL | NULL | NULL || checking permissions | 0.000015 | check_access | sql_parse.cc | 5297 || Opening tables | 0.000027 | open_tables | sql_base.cc | 5025 || init | 0.000031 | mysql_prepare_select | sql_select.cc | 1050 || System lock | 0.000015 | mysql_lock_tables | lock.cc | 304 || optimizing | 0.000013 | optimize | sql_optimizer.cc | 138 || statistics | 0.000022 | optimize | sql_optimizer.cc | 362 || preparing | 0.000018 | optimize | sql_optimizer.cc | 485 || Sorting result | 0.000023 | make_tmp_tables_info | sql_select.cc | 5307 || executing | 0.000009 | exec | sql_executor.cc | 110 || Sending data | 0.000017 | exec | sql_executor.cc | 190 || Creating sort index | 0.645990 | sort_table | sql_executor.cc | 2504 || end | 0.000035 | mysql_execute_select | sql_select.cc | 1105 || query end | 0.000014 | mysql_execute_command | sql_parse.cc | 4996 || closing tables | 0.000020 | mysql_execute_command | sql_parse.cc | 5044 || freeing items | 0.000032 | mysql_parse | sql_parse.cc | 6433 || cleaning up | 0.000034 | dispatch_command | sql_parse.cc | 1778 |+----------------------+----------+-----------------------+------------------+-------------+17 rows in set, 1 warning (0.00 sec) type之SWAPS显示swap的次数。wing@3303&gt;show profile swaps for query 5;+----------------------+----------+-------+| Status | Duration | Swaps |+----------------------+----------+-------+| starting | 0.000088 | 0 || checking permissions | 0.000015 | 0 || Opening tables | 0.000027 | 0 || init | 0.000031 | 0 || System lock | 0.000015 | 0 || optimizing | 0.000013 | 0 || statistics | 0.000022 | 0 || preparing | 0.000018 | 0 || Sorting result | 0.000023 | 0 || executing | 0.000009 | 0 || Sending data | 0.000017 | 0 || Creating sort index | 0.645990 | 0 || end | 0.000035 | 0 || query end | 0.000014 | 0 || closing tables | 0.000020 | 0 || freeing items | 0.000032 | 0 || cleaning up | 0.000034 | 0 |+----------------------+----------+-------+17 rows in set, 1 warning (0.00 sec) SHOW PROFILE详解通过设置profiling参数，开启MySQL的profiling功能；通过设置profiling_history_size参数，设置保留的SQL语句数量；如果将profiling_history_size参数设置为0，同样具有关闭MySQL的profiling效果。mysql&gt; show variables like "profiling"; +---------------+-------+| Variable_name | Value |+---------------+-------+| profiling | OFF |+---------------+-------+1 row in set (0.00 sec)mysql&gt; set profiling=1;Query OK, 0 rows affected, 1 warning (0.00 sec)mysql&gt; show variables like "profiling";+---------------+-------+| Variable_name | Value |+---------------+-------+| profiling | ON |+---------------+-------+1 row in set (0.02 sec)mysql&gt; show variables like "profiling_hist%";+------------------------+-------+| Variable_name | Value |+------------------------+-------+| profiling_history_size | 15 |+------------------------+-------+1 row in set (0.00 sec)mysql&gt; set profiling_history_size=100;Query OK, 0 rows affected, 1 warning (0.01 sec)mysql&gt; show variables like "profiling_hist%";+------------------------+-------+| Variable_name | Value |+------------------------+-------+| profiling_history_size | 100 |+------------------------+-------+1 row in set (0.00 sec) SHOW PROFILE各个Status详解详见show processlist。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[解决UnicodeEncodeError: 'ascii' codec can't encode characters in position问题]]></title>
      <url>%2F2016%2F01%2F29%2F%E8%A7%A3%E5%86%B3UnicodeEncodeError-ascii-codec-can-t-encode-characters-in-position%E9%97%AE%E9%A2%98%2F</url>
      <content type="text"><![CDATA[在文件中读取select * form table_name语句中的’*’,总是读取不出来，报出error为：UnicodeEncodeError: ‘ascii’ codec can’t encode characters in position，然后通过google,找到了解决方法啦~ 解决方法在程序的开头添加如下代码：import sysreload(sys)sys.setdefaultencoding( "utf-8" )]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[MySQL:Got fatal error 1236]]></title>
      <url>%2F2016%2F01%2F18%2FMySQL-Got-fatal-error-1236%2F</url>
      <content type="text"><![CDATA[记一次MySQL replication error:Got fatal error 1236 from master when reading data from binary log: ‘Could not find first log file name in binary log index file’的解决方案 问题从库出现的问题如下： root@localhost : (none) 11:05:47&gt; show slave status\G*************************** 1. row *************************** Slave_IO_State: Master_Host: 192.168.200.157 Master_User: repl Master_Port: 3307 Connect_Retry: 60 Master_Log_File: mysql-bin.000136 Read_Master_Log_Pos: 41648 Relay_Log_File: mysql-relay-bin.000001 Relay_Log_Pos: 4 Relay_Master_Log_File: mysql-bin.000136 Slave_IO_Running: No Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 41648 Relay_Log_Space: 120 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: NULLMaster_SSL_Verify_Server_Cert: No Last_IO_Errno: 1236 Last_IO_Error: Got fatal error 1236 from master when reading data from binary log: 'Could not find first log file name in binary log index file' Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 330757 Master_UUID: d4772941-70e1-11e5-ad9c-fa163e7860dd Master_Info_File: /data/mysql/mysqldata3307/mydata/master.info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for the slave I/O thread to update it Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: 160117 23:01:49 Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: Auto_Position: 01 row in set (0.00 sec) 原因分析查看show slave status\G,发现Master_Log_File: mysql-bin.000136和Read_Master_Log_Pos: 41648，于是去主库查看mysql-bin.index发现唯独缺少了mysql-bin.000136，所以原因就在这，但是我还是没明白我的mysql-bin.000136为什么会缺失，之前存在磁盘空间不够，添加更多的磁盘空间，我想跟这个可能性有关吧。 解决方法master: 在mysql-bin.index中添加mysql-bin.000136; flush logs;slave： stop slave; change master to master_log_file=’mysql-bin.000136’,master_log_pos=41648; start slave;此时show slave status\G即可看到主从复制正常。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux下使用安装pip]]></title>
      <url>%2F2016%2F01%2F17%2FLinux%E4%B8%8B%E4%BD%BF%E7%94%A8%E5%AE%89%E8%A3%85pip%2F</url>
      <content type="text"><![CDATA[Linux下安装pip，python2.7.9之前的版本，python是没有内置pip的，所以需要我们自己安装哦~ 1、首先得有安装工具此处推荐使用pip，python2.7.9版本之后已经内置该模块管理工具。安装pip模块管理工具之前得有setuptoolsSetuptools最新版本：https://pypi.python.org/pypi/setuptoolsPip最新版本：https://pypi.python.org/pypi/pip得到Setuptools和Pip最新版本，并在Linux下解压。 2、安装setuptools:编译：python setup.py build安装：python setup.py install 3、安装pip:编译：python setup.py build安装：python setup.py install 4、利用pip安装python第三方模块包：pip install 模块包 # pip安装模块包实例pip install prettytable]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[MySQL for Python]]></title>
      <url>%2F2016%2F01%2F13%2FMySQL-for-Python%2F</url>
      <content type="text"><![CDATA[作为爱上python的MySQL DBA来说，可以使用python操作MySQL是在是一件太过瘾的事情，但是一段时间不用MySQLdb的库，就会忘记怎么使用python操作MySQL了，所以记忆力太差，我就只能把我的MySQLdb操作记录记载下来，便于以后的快速使用和学习啦 # MySQL中的数据root@localhost : menu 06:16:42&gt; select * from fish;+----+---------------+-------+| ID | NAME | PRICE |+----+---------------+-------+| 1 | catfish | 8.50 || 2 | catfish | 8.50 || 3 | tuna | 8.00 || 4 | catfish | 5.00 || 5 | bass | 6.75 || 6 | haddock | 6.50 || 7 | salmon | 9.50 || 8 | trout | 6.00 || 9 | tuna | 7.50 || 10 | yellowfintuna | 12.00 || 11 | yellowfintuna | 13.00 || 12 | tuna | 7.50 |+----+---------------+-------+12 rows in set (0.26 sec) # python中的操作[root@localhost python]# pythonPython 2.7.5 (default, Jun 17 2014, 18:11:42) [GCC 4.8.2 20140120 (Red Hat 4.8.2-16)] on linux2Type "help", "copyright", "credits" or "license" for more information.&gt;&gt;&gt; import tab&gt;&gt;&gt; import MySQLdb# python与MySQL建立连接&gt;&gt;&gt; mydb = MySQLdb.connect(user='root',host='127.0.0.1',port=3306,db='menu')&gt;&gt;&gt; cur=mydb.cursor()# 在python中执行sql语句&gt;&gt;&gt; command = cur.execute('select * from fish')# 获取sql语句执行的结果&gt;&gt;&gt; results = cur.fetchall()&gt;&gt;&gt; print results((1L, 'catfish', Decimal('8.50')), (2L, 'catfish', Decimal('8.50')), (3L, 'tuna', Decimal('8.00')), (4L, 'catfish', Decimal('5.00')), (5L, 'bass', Decimal('6.75')), (6L, 'haddock', Decimal('6.50')), (7L, 'salmon', Decimal('9.50')), (8L, 'trout', Decimal('6.00')), (9L, 'tuna', Decimal('7.50')), (10L, 'yellowfintuna', Decimal('12.00')), (11L, 'yellowfintuna', Decimal('13.00')), (12L, 'tuna', Decimal('7.50')))# 对sql语句的结果进行友好式的处理&gt;&gt;&gt; for record in results: ... print record[0],'--&gt;',record[1],'@',record[2],'each'... 1 --&gt; catfish @ 8.50 each2 --&gt; catfish @ 8.50 each3 --&gt; tuna @ 8.00 each4 --&gt; catfish @ 5.00 each5 --&gt; bass @ 6.75 each6 --&gt; haddock @ 6.50 each7 --&gt; salmon @ 9.50 each8 --&gt; trout @ 6.00 each9 --&gt; tuna @ 7.50 each10 --&gt; yellowfintuna @ 12.00 each11 --&gt; yellowfintuna @ 13.00 each12 --&gt; tuna @ 7.50 each# 在sql语句中添加变量&gt;&gt;&gt; values=7.50 &gt;&gt;&gt; command = cur.execute('select * from fish where price = %f'%(values))&gt;&gt;&gt; results = cur.fetchall()&gt;&gt;&gt; print results((9L, 'tuna', Decimal('7.50')), (12L, 'tuna', Decimal('7.50')))&gt;&gt;&gt; for record in results: ... print record[0],'--&gt;',record[1],'(',record[2],')'... 9 --&gt; tuna ( 7.50 )12 --&gt; tuna ( 7.50 )# 交互式执行sql语句&gt;&gt;&gt; operation = raw_input('operation:')operation:=&gt;&gt;&gt; values = float(raw_input('values:'))values:7.50&gt;&gt;&gt; command = cur.execute('select * from fish where price %s %f'%(operation,values))&gt;&gt;&gt; results = cur.fetchall()&gt;&gt;&gt; print results((9L, 'tuna', Decimal('7.50')), (12L, 'tuna', Decimal('7.50')))&gt;&gt;&gt; for record in results:... print record[0],'--&gt;',record[1],'(',record[2],')'... 9 --&gt; tuna ( 7.50 )12 --&gt; tuna ( 7.50 )# 关闭游标&gt;&gt;&gt; cur.close()# 提交sql操作&gt;&gt;&gt; mydb.commit()# 关闭MySQL连接&gt;&gt;&gt; mydb.close() # 在python建立连接后，在MySQL执行show processlist可以看到的连接变化，多增加了id=6的连接root@localhost : (none) 06:28:09&gt; show processlist;+----+-----------------+-----------------+------+---------+------+------------------------+------------------+| Id | User | Host | db | Command | Time | State | Info |+----+-----------------+-----------------+------+---------+------+------------------------+------------------+| 1 | event_scheduler | localhost | NULL | Daemon | 762 | Waiting on empty queue | NULL || 3 | root | localhost | menu | Sleep | 721 | | NULL || 5 | root | localhost | NULL | Query | 0 | init | show processlist || 6 | root | 127.0.0.1:33085 | menu | Sleep | 1 | | NULL |+----+-----------------+-----------------+------+---------+------+------------------------+------------------+4 rows in set (0.00 sec)# 在python中执行mydb.close()后，在MySQL执行show processlist可以看到的连接变化，id=6的连接消失了root@localhost : (none) 06:28:47&gt; show processlist;+----+-----------------+-----------+------+---------+------+------------------------+------------------+| Id | User | Host | db | Command | Time | State | Info |+----+-----------------+-----------+------+---------+------+------------------------+------------------+| 1 | event_scheduler | localhost | NULL | Daemon | 2544 | Waiting on empty queue | NULL || 3 | root | localhost | menu | Sleep | 2503 | | NULL || 5 | root | localhost | NULL | Query | 0 | init | show processlist |+----+-----------------+-----------+------+---------+------+------------------------+------------------+3 rows in set (0.00 sec)# 在python中获取数据库的所有表&gt;&gt;&gt; cur.close()&gt;&gt;&gt; mydb = MySQLdb.connect(user='root',host='127.0.0.1',port=3306,db='menu')&gt;&gt;&gt; cur=mydb.cursor()&gt;&gt;&gt; command=cur.execute('show tables')&gt;&gt;&gt; results = cur.fetchall()&gt;&gt;&gt; print results(('fish',), ('meat',))&gt;&gt;&gt; table_list=[]&gt;&gt;&gt; for record in results:... table_list.append(record[0])... #此处注意使用record[0],因为这里的每个reocrd原本都是一个元组&gt;&gt;&gt; print table_list['fish', 'meat']## 或者 当前操作是从1开始计数，因为做了特殊处理，如果不做特殊处理，字典都是从0开始计数的&gt;&gt;&gt; item_dict=&#123;&#125;&gt;&gt;&gt; for item in xrange(1,len(table_list)+1):... item_dict[item]=table_list[item-1]... &gt;&gt;&gt; print item_dict&#123;1: 'fish', 2: 'meat'&#125;# 在python中交互式友好的让用户选择需要操作的表&gt;&gt;&gt; for key in item_dict: ... print '%s ==&gt; %s'%(key,item_dict[key])... 1 ==&gt; fish2 ==&gt; meat&gt;&gt;&gt; choice = input('please enter your choice of table to be query:')please enter your choice of table to be query:1# 验证表确实存在与数据库中&gt;&gt;&gt; choice = input('please enter your choice of table to be query:')please enter your choice of table to be query:1&gt;&gt;&gt; try:... table_choice=item_dict[choice]... except:... "error"... &gt;&gt;&gt; choice = input('please enter your choice of table to be query:')please enter your choice of table to be query:5&gt;&gt;&gt; try: ... table_choice=item_dict[choice]... except:... "error"... 'error'# 显示表中列的信息&gt;&gt;&gt; sql1 = 'desc %s'%(item_dict[chioce])&gt;&gt;&gt; command = cur.execute(sql1)&gt;&gt;&gt; results = cur.fetchall()&gt;&gt;&gt; print results(('ID', 'int(11)', 'NO', 'PRI', None, 'auto_increment'), ('NAME', 'varchar(30)', 'NO', '', '', ''), ('PRICE', 'decimal(5,2)', 'NO', '', '0.00', ''))&gt;&gt;&gt; column_list=[]&gt;&gt;&gt; for record in results:... column_list.append(record[0])... &gt;&gt;&gt; print column_list['ID', 'NAME', 'PRICE'] 注意1、 当数据库中的数据更新后，原来的连接时感知不到数据库的变化，数据更新后的连接才能接受到数据更新后的数据。2、 此处的record中的每个元素实际上都是一个元组，所以取值的时候需要用record[0],而不是用record&gt;&gt;&gt; for record in results:... table_list.append(record[0])&gt;&gt;&gt; for record in results:... print type(record)... &lt;type &apos;tuple&apos;&gt;&lt;type &apos;tuple&apos;&gt;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux修改时区]]></title>
      <url>%2F2016%2F01%2F13%2FLinux%E4%BF%AE%E6%94%B9%E6%97%B6%E9%97%B4%2F</url>
      <content type="text"><![CDATA[之前一直修改时区都没有效果，今天总算让我可以成功的修改了，抓紧记录下。。。 修改时区# 以中国为例[root@ip-172-31-1-8 ~]# dateWed Jan 13 04:28:50 UTC 2016[root@ip-172-31-1-8 ~]# cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime cp: overwrite ‘/etc/localtime’? y[root@ip-172-31-1-8 ~]# dateWed Jan 13 12:31:48 CST 2016]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[金额为什么使用定点数而不是浮点数]]></title>
      <url>%2F2016%2F01%2F12%2F%E9%87%91%E9%A2%9D%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8%E5%AE%9A%E7%82%B9%E6%95%B0%E8%80%8C%E4%B8%8D%E6%98%AF%E6%B5%AE%E7%82%B9%E6%95%B0%2F</url>
      <content type="text"><![CDATA[以前一直不会区分浮点数和定点数，金额为什么使用定点数而不是浮点数呢。。。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[TIMESTAMP pk DATETIME]]></title>
      <url>%2F2016%2F01%2F11%2FTIMESTAMP-pk-DATETIME%2F</url>
      <content type="text"><![CDATA[本文的比较是基于MySQL5.6的版本，日常使用中比较建议使用TIMESTAMP类型，为什么呢？请允许我娓娓道来~1. TIMESTAMP受时区影响，DATETIME不受时区影响root@localhost : menu 06:39:51&gt; set session time_zone='+8:00';Query OK, 0 rows affected (0.00 sec)root@localhost : menu 06:40:32&gt; show variables like '%zone%';+------------------+--------+| Variable_name | Value |+------------------+--------+| system_time_zone | CST || time_zone | +08:00 |+------------------+--------+2 rows in set (0.00 sec)root@localhost : menu 06:40:34&gt; select * from t;Empty set (0.00 sec)root@localhost : menu 06:40:43&gt; insert into t values();Query OK, 1 row affected (0.00 sec)root@localhost : menu 06:40:54&gt; select * from t;+---------------------+---------------------+| id1 | id2 |+---------------------+---------------------+| 2016-01-12 06:40:54 | 2016-01-12 06:40:54 |+---------------------+---------------------+1 row in set (0.00 sec)root@localhost : menu 06:40:55&gt; set session time_zone='+9:00'; Query OK, 0 rows affected (0.01 sec)root@localhost : menu 06:41:07&gt; show variables like '%zone%';+------------------+--------+| Variable_name | Value |+------------------+--------+| system_time_zone | CST || time_zone | +09:00 |+------------------+--------+root@localhost : menu 06:41:04&gt; select * from t;+---------------------+---------------------+| id1 | id2 |+---------------------+---------------------+| 2016-01-12 07:40:54 | 2016-01-12 06:40:54 |+---------------------+---------------------+1 row in set (0.00 sec) 2. TIMESTAMP与DATETIME取值范围不同DATETIME的范围为‘1000-01-01 00：00：00.000000’–‘9999-12-31 23：59：59.999999’（不受时区影响），在UTC时区下，TIMESTAMP的范围为‘1970-01-01 00：00：01.000000’–‘2038-01-19 03：14：07.999999’ 3. TIMESTAMP和DATETIME存储所需的空间不同在没有微秒的情况下，MySQL5.6.4之后，DATETIME为5个字节，TIMESTAMP为4个字节。 注意：从MySQL5.6开始，允许同一个表中存在两种DEFAULT CURRENT_TIMESTAMP的TIMESTAMP类型。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[关于FLUSH PRIVILEGES的使用]]></title>
      <url>%2F2016%2F01%2F11%2F%E5%85%B3%E4%BA%8EFLUSH-PRIVILEGES%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
      <content type="text"><![CDATA[很多人在MySQL里面乱用FLUSH PRIVILEGES，你知道吗？包括从前的我，但是FLUSH PRIVILEGES你真的知道应该在什么场合使用么？ 根据官方文档的如下链接内容可知：https://dev.mysql.com/doc/refman/5.6/en/privilege-changes.html 当mysqld启动时，所有的权限都会被加载到内存中。如果使用GRANT/REVOKE/SET PASSWORD/RENAME USER命令来更改数据库中的权限表，mysqld服务器将会注意到这些变化并立即加载更新后的权限表至内存中，即权限生效；如果使用INSERT/UPDATE/DELETE语句更新权限表，则内存中的权限表不会感知到数据库中权限的更新，必须重启服务器或者使用FLUSH PRIVILEGES命令使更新的权限表加载到内存中，即权限需在重启服务器或者FLUSH PRIVILEGES之后方可生效。 权限生效的含义：1、表级别/列级别的权限，当更新后的权限加载至内存表中，已存在的会话下一次请求时可使用该权限，在修改权限后的建立的会话则立即生效；2、数据库级别的权限，当更新后的权限加载至内存表中，已存在会话下一次使用USE db_name后，可使用该权限，在修改权限后的建立的会话则立即生效；3、全局权限或者修改密码，当更新后的权限加载至内存表中，需要在下一次登录mysqld后，可使用该权限或密码，对已存在会话不起作用。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[getpass交互式密码模块]]></title>
      <url>%2F2016%2F01%2F02%2Fgetpass%E4%BA%A4%E4%BA%92%E5%BC%8F%E5%AF%86%E7%A0%81%E6%A8%A1%E5%9D%97%2F</url>
      <content type="text"><![CDATA[如今人们的安全意识真是越来越大了，密码泄露可是个大大的头疼事件，还好，python有个强大的库叫getpass，提供交互式输入密码，完全看不见密码长神马样子~ getpass模块提供了两个函数，分别为getpass.getpass([prompt[,stream]])和getpass.getuser()。 getpass.getpass([prompt[,stream]])提示用户输入密码。&gt;&gt;&gt; import getpass&gt;&gt;&gt; pwd1=getpass.getpass()Password: &gt;&gt;&gt; print pwd1wing1# 友好交互式输入&gt;&gt;&gt; pwd2=getpass.getpass(&apos;pls input ur password:&apos;)pls input ur password:&gt;&gt;&gt; print pwd2wing2 getpass.getuser()获得登陆的用户名&gt;&gt;&gt; user=getpass.getuser()&gt;&gt;&gt; print userroot]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[10条命令检查Linux性能]]></title>
      <url>%2F2015%2F12%2F29%2F10%E6%9D%A1%E5%91%BD%E4%BB%A4%E6%A3%80%E6%9F%A5Linux%E6%80%A7%E8%83%BD%2F</url>
      <content type="text"><![CDATA[最近看微博发现一片在Linux圈很流行的文章叫”10条命令检查Linux性能”，于是我对这10条命令相当感兴趣，顺便对各个命令做下详细的介绍好了。文章链接：http://wkee.net/post/linux-performance.html uptime解释load average（平均负载情况）后的三个数字：1分钟，5分钟，15分钟1分钟负载较高，15分钟负载较低，则说明服务器正在面临高负载的情况，CPU资源正在紧张，需要排查CPU的资源消耗在哪里#执行压测过程中的uptime[root@VM_159_22_centos logs]# uptime 16:50:11 up 23:02, 2 users, load average: 66.48, 24.53, 36.78[root@VM_159_22_centos logs]# uptime 16:51:03 up 23:02, 2 users, load average: 91.41, 37.97, 40.71[root@VM_159_22_centos logs]# uptime 16:52:16 up 23:04, 2 users, load average: 116.89, 55.67, 46.57 1分钟负载较低，15分钟负载较高，则说明服务器的高负载情况过去，CPU资源正在缓解#停止压测后的uptime[root@VM_159_22_centos logs]# uptime 16:57:35 up 23:09, 2 users, load average: 28.17, 58.97, 52.17[root@VM_159_22_centos logs]# uptime 16:57:37 up 23:09, 2 users, load average: 25.92, 57.99, 51.89[root@VM_159_22_centos logs]# uptime 16:57:41 up 23:09, 2 users, load average: 23.84, 57.03, 51.61[root@VM_159_22_centos logs]# uptime 16:58:38 up 23:10, 2 users, load average: 9.52, 47.43, 48.64[root@VM_159_22_centos logs]# uptime 16:59:09 up 23:11, 2 users, load average: 5.77, 42.90, 47.10 dmesg显示开机信息，同时也会保存在/var/log/dmesg文件中# 显示开机信息的最后十条信息[root@ip-172-31-1-8 ~]# dmesg |tail[ 2.672858] evbug: Connected device: input2 (Power Button at LNXPWRBN/button/input0)[ 2.672859] evbug: Connected device: input3 (Sleep Button at LNXSLPBN/button/input0)[ 3.670982] input: ImExPS/2 Generic Explorer Mouse as /devices/platform/i8042/serio1/input/input4[ 3.675797] evbug: Connected device: input4 (ImExPS/2 Generic Explorer Mouse at isa0060/serio1/input0)[ 5.933678] EXT4-fs (xvda1): re-mounted. Opts: (null)[ 6.067840] ip_tables: (C) 2000-2006 Netfilter Core Team[ 6.073707] nf_conntrack version 0.5.0 (7964 buckets, 31856 max)[ 6.118031] NET: Registered protocol family 10[ 7.469579] random: nonblocking pool is initialized[ 8.440910] audit: type=1305 audit(1447402398.763:2): audit_pid=2063 old=0 auid=4294967295 ses=4294967295 res=1# 显示开机信息利用正则表达式过滤后的信息[root@ip-172-31-1-8 ~]# dmesg | grep -i net[ 0.000000] Netfront and the Xen platform PCI driver have been compiled for this kernel: unplug emulated NICs.[ 0.048007] Initializing cgroup subsys net_cls[ 0.051221] Initializing cgroup subsys net_prio[ 0.164193] NET: Registered protocol family 16[ 0.380478] NetLabel: Initializing[ 0.381885] NetLabel: domain hash size = 128[ 0.383801] NetLabel: protocols = UNLABELED CIPSOv4[ 0.384016] NetLabel: unlabeled traffic allowed by default[ 0.441281] NET: Registered protocol family 2[ 0.456733] NET: Registered protocol family 1[ 0.697494] audit: initializing netlink subsys (disabled)[ 0.790387] xen_netfront: Initialising Xen virtual ethernet driver[ 0.819778] NET: Registered protocol family 17[ 6.067840] ip_tables: (C) 2000-2006 Netfilter Core Team[ 6.118031] NET: Registered protocol family 10 vmstatvmstat命令是常用的Linux/Unix监控工具，可以展现给点时间间隔的服务器的状态值，包括服务器的CPU使用率，内存使用等等，与top的不同是，top看到的是各个CPU使用率，vmstat看到的是总体CPU使用率。# 每2秒采集一次服务器的状态，总共采集5次[root@host-192-168-200-153 logs]# vmstat 2 5procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 2 0 0 14361012 14600 530480 0 0 4 9 62 117 0 0 99 0 0 4 0 0 14322284 14600 536084 0 0 0 0 20568 39009 10 7 81 0 2 3 0 0 14287296 14608 542412 0 0 0 18 21127 41069 13 8 75 0 4 1 0 0 14235668 14608 549392 0 0 0 0 23555 45623 12 9 75 0 4 3 0 0 14206072 14608 555816 0 0 0 0 23593 44988 12 8 77 0 3 对vmstat的输出结果进行解释： procsr : 表示运行队列（即多少进程真正分配到了CPU）,当这个值超过了CPU数目，就会出现CPU瓶颈;b : 表示进程阻塞；（block） memoryswpd : 虚拟内存使用的大小，如果大于0，则表示机器内存不够啦free : 空闲的物理内存buffer : 缓冲区大小cache : 告诉缓存大小（注意：buffer和cache也属于内存的一部分，他们会在free耗尽后释放，并提供给内存使用） swapsi : 每秒从磁盘读入到虚拟内存的大小，如果该值大于0，则表示磁盘内存不足或者内存泄漏so : 每秒虚拟内存写入到磁盘的大小，如果该值大雨0，则表示磁盘内存不足或者内存泄漏 iobi : 块设备每秒接受的块数量（读磁盘）bo : 块设备每秒发送的块数量（写磁盘） systemin : 每秒CPU的中断次数cs : 每秒上下文切换次数，例如我们调用系统函数，就要进行上下文切换，线程的切换，也要进程上下文切换，这个值要越小越好，太大了，要考虑调低线程或者进程的数目,例如在apache和nginx这种web服务器中，我们一般做性能测试时会进行几千并发甚至几万并发的测试，选择web服务器的进程可以由进程或者线程的峰值一直下调，压测，直到cs到一个比较小的值，这个进程和线程数就是比较合适的值了。系统调用也是，每次调用系统函数，我们的代码就会进入内核空间，导致上下文切换，这个是很耗资源，也要尽量避免频繁调用系统函数。上下文切换次数过多表示你的CPU大部分浪费在上下文切换，导致CPU干正经事的时间少了，CPU没有充分利用，是不可取的。 cpuus : 用户进程消耗CPU的时间sy : 系统进程消耗CPU的时间id : 空闲CPU时间wa : IO等待CPU时间st : 被偷走的CPU时间 mpstat用于获取系统中每个CPU的占用情况，如果有一个CPU占用率特别高，那么有可能是一个单线程应用程序引起的。# 开启一个MySQL压测程序时的mpstat输出[root@localhost ~]# mpstat -P ALL 5Linux 3.10.0-229.el7.x86_64 (localhost.localdomain) 2016年02月01日 _x86_64_ (4 CPU)01时35分27秒 CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle01时35分32秒 all 38.82 0.00 32.83 2.31 0.00 10.27 0.00 0.00 0.00 15.7701时35分32秒 0 34.76 0.00 32.89 2.67 0.00 13.10 0.00 0.00 0.00 16.5801时35分32秒 1 35.81 0.00 30.50 6.10 0.00 11.67 0.00 0.00 0.00 15.9201时35分32秒 2 42.74 0.00 33.70 0.00 0.00 7.95 0.00 0.00 0.00 15.6201时35分32秒 3 42.18 0.00 33.80 0.56 0.00 8.10 0.00 0.00 0.00 15.3601时35分32秒 CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle01时35分37秒 all 36.39 0.00 20.34 6.29 0.00 7.41 0.00 0.00 0.00 29.5701时35分37秒 0 33.49 0.00 20.28 6.37 0.00 8.73 0.00 0.00 0.00 31.1301时35分37秒 1 34.11 0.00 19.95 18.10 0.00 8.35 0.00 0.00 0.00 19.4901时35分37秒 2 39.67 0.00 20.19 0.00 0.00 5.23 0.00 0.00 0.00 34.9201时35分37秒 3 38.24 0.00 21.38 0.48 0.00 6.89 0.00 0.00 0.00 33.0201时35分37秒 CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle01时35分42秒 all 52.17 0.00 28.41 1.41 0.00 8.98 0.00 0.00 0.00 9.0401时35分42秒 0 50.37 0.00 28.12 0.73 0.00 10.02 0.00 0.00 0.00 10.7601时35分42秒 1 51.33 0.00 28.19 3.61 0.00 9.64 0.00 0.00 0.00 7.2301时35分42秒 2 54.00 0.00 27.60 0.00 0.00 8.72 0.00 0.00 0.00 9.6901时35分42秒 3 52.74 0.00 29.60 1.24 0.00 7.96 0.00 0.00 0.00 8.4601时35分42秒 CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle01时35分47秒 all 42.36 0.00 23.46 3.55 0.00 8.35 0.00 0.00 0.00 22.2701时35分47秒 0 40.24 0.00 23.53 2.12 0.00 9.18 0.00 0.00 0.00 24.9401时35分47秒 1 40.24 0.00 23.29 11.29 0.00 9.41 0.00 0.00 0.00 15.7601时35分47秒 2 45.15 0.00 23.40 0.00 0.00 7.09 0.00 0.00 0.00 24.3501时35分47秒 3 44.12 0.00 23.50 0.48 0.00 7.67 0.00 0.00 0.00 24.2201时35分47秒 CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle01时35分52秒 all 50.39 0.00 27.61 1.15 0.00 9.34 0.00 0.00 0.00 11.5101时35分52秒 0 48.56 0.00 26.44 1.44 0.00 10.34 0.00 0.00 0.00 13.2201时35分52秒 1 48.67 0.00 26.75 1.69 0.00 10.60 0.00 0.00 0.00 12.2901时35分52秒 2 52.62 0.00 29.29 0.95 0.00 7.38 0.00 0.00 0.00 9.7601时35分52秒 3 51.59 0.00 27.87 0.73 0.00 9.29 0.00 0.00 0.00 10.51# 添加-P ALL的输出区别[root@localhost ~]# mpstatLinux 3.10.0-229.el7.x86_64 (localhost.localdomain) 2016年02月01日 _x86_64_ (4 CPU)01时41分15秒 CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle01时41分15秒 all 5.00 0.00 3.36 1.30 0.00 1.85 0.00 0.00 0.00 88.50[root@localhost ~]# mpstat -P ALLLinux 3.10.0-229.el7.x86_64 (localhost.localdomain) 2016年02月01日 _x86_64_ (4 CPU)01时41分25秒 CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle01时41分25秒 all 4.95 0.00 3.33 1.28 0.00 1.83 0.00 0.00 0.00 88.6101时41分25秒 0 4.90 0.00 3.29 1.13 0.00 2.57 0.00 0.00 0.00 88.1201时41分25秒 1 4.98 0.00 3.32 1.42 0.00 2.61 0.00 0.00 0.00 87.6701时41分25秒 2 4.94 0.00 3.32 1.00 0.00 1.05 0.00 0.00 0.00 89.6901时41分25秒 3 4.97 0.00 3.38 1.59 0.00 1.08 0.00 0.00 0.00 88.97 -P ALL显示所有CPU的统计信息。对mpstat输出结果的解释： %usr表示处理用户进程所使用的CPU百分比。 %nice表示使用nice命令对进程进行降级时CPU的百分比。 %sys表示内核进程使用的CPU百分比。 %iowait表示等待进行I/O所使用的CPU百分比。 %irq表示用于处理系统中断的 CPU百分比。 %soft表示用于处理软件中断的CPU百分比。 %steal%guest%gnice%idle表示空闲CPU的百分比。 pidstat监控全部或者指定进程占用系统资源的情况，如CPU、内存等。pidstat首次运行时显示自系统启动开始的各项统计信息，之后运行自上次运行pidstat命令以后的信息。# 开启一个mysql压测程序的pidstat的输出[root@localhost ~]# pidstatLinux 3.10.0-229.el7.x86_64 (localhost.localdomain) 2016年02月01日 _x86_64_ (4 CPU)01时53分38秒 UID PID %usr %system %guest %CPU CPU Command01时53分38秒 0 1 0.01 0.08 0.00 0.10 3 systemd01时53分38秒 0 3 0.00 0.02 0.00 0.02 0 ksoftirqd/001时53分38秒 0 7 0.00 0.02 0.00 0.02 0 migration/001时53分38秒 0 137 0.00 0.02 0.00 0.02 3 rcu_sched01时53分38秒 0 138 0.00 0.01 0.00 0.01 0 rcuos/001时53分38秒 0 139 0.00 0.01 0.00 0.01 3 rcuos/101时53分38秒 0 140 0.00 0.01 0.00 0.01 3 rcuos/201时53分38秒 0 141 0.00 0.01 0.00 0.01 3 rcuos/301时53分38秒 0 266 0.00 0.00 0.00 0.00 0 watchdog/001时53分38秒 0 267 0.00 0.00 0.00 0.00 1 watchdog/101时53分38秒 0 268 0.00 0.02 0.00 0.02 1 migration/101时53分38秒 0 269 0.00 0.02 0.00 0.02 1 ksoftirqd/101时53分38秒 0 272 0.00 0.00 0.00 0.00 2 watchdog/201时53分38秒 0 273 0.00 0.01 0.00 0.01 2 migration/201时53分38秒 0 274 0.00 0.01 0.00 0.01 2 ksoftirqd/201时53分38秒 0 277 0.00 0.00 0.00 0.00 3 watchdog/301时53分38秒 0 278 0.00 0.01 0.00 0.01 3 migration/301时53分38秒 0 279 0.00 0.03 0.00 0.03 3 ksoftirqd/301时53分38秒 0 289 0.00 0.00 0.00 0.00 1 khubd01时53分38秒 0 291 0.00 0.01 0.00 0.01 0 kworker/0:101时53分38秒 0 295 0.00 0.03 0.00 0.03 3 kswapd001时53分38秒 0 297 0.00 0.00 0.00 0.00 2 khugepaged01时53分38秒 0 332 0.00 0.10 0.00 0.10 1 kworker/1:101时53分38秒 0 355 0.00 0.00 0.00 0.00 2 kauditd01时53分38秒 0 428 0.00 0.14 0.00 0.14 3 kworker/3:201时53分38秒 0 550 0.00 0.00 0.00 0.00 0 scsi_eh_101时53分38秒 0 552 0.00 0.00 0.00 0.00 1 scsi_eh_201时53分38秒 0 653 0.00 0.05 0.00 0.05 3 kworker/u256:2901时53分38秒 0 745 0.00 0.00 0.00 0.00 3 kworker/3:1H01时53分38秒 0 746 0.00 0.03 0.00 0.03 1 xfsaild/dm-001时53分38秒 0 751 0.00 0.00 0.00 0.00 0 kworker/0:1H01时53分38秒 0 822 0.00 0.01 0.00 0.01 1 systemd-journal01时53分38秒 0 835 0.00 0.00 0.00 0.00 3 lvmetad01时53分38秒 0 843 0.01 0.01 0.00 0.02 2 systemd-udevd01时53分38秒 0 959 0.00 0.00 0.00 0.00 3 auditd01时53分38秒 0 981 0.01 0.02 0.00 0.04 0 firewalld01时53分38秒 0 986 0.00 0.01 0.00 0.01 2 irqbalance01时53分38秒 0 987 0.00 0.01 0.00 0.01 3 rsyslogd01时53分38秒 0 988 0.01 0.02 0.00 0.03 1 tuned01时53分38秒 0 991 0.00 0.00 0.00 0.00 2 systemd-logind01时53分38秒 81 992 0.00 0.01 0.00 0.01 0 dbus-daemon01时53分38秒 0 996 0.00 0.03 0.00 0.03 0 crond01时53分38秒 0 1099 0.00 0.01 0.00 0.01 2 NetworkManager01时53分38秒 999 1300 0.00 0.00 0.00 0.00 0 polkitd01时53分38秒 0 1580 0.00 0.00 0.00 0.00 1 dhclient01时53分38秒 0 1608 0.00 0.00 0.00 0.00 0 sshd01时53分38秒 0 2825 0.00 0.00 0.00 0.00 1 kworker/1:1H01时53分38秒 0 2826 0.00 0.12 0.00 0.13 0 sshd01时53分38秒 0 2830 0.00 0.00 0.00 0.00 0 bash01时53分38秒 0 2851 0.01 0.01 0.00 0.01 1 bash01时53分38秒 0 2878 0.00 0.01 0.00 0.01 1 mysqld_safe01时53分38秒 998 4303 9.83 8.95 0.00 18.77 1 mysqld01时53分38秒 0 4418 0.00 0.02 0.00 0.02 2 kworker/2:001时53分38秒 0 4509 0.00 0.17 0.00 0.17 0 kworker/u256:001时53分38秒 0 4642 0.00 0.01 0.00 0.01 2 kworker/2:101时53分38秒 0 4650 0.00 0.00 0.00 0.00 2 kworker/2:201时53分38秒 0 4689 0.69 0.58 0.00 1.27 3 java01时53分38秒 0 4714 0.00 0.01 0.00 0.01 3 kworker/u256:101时53分38秒 0 4715 0.00 0.00 0.00 0.00 1 pidstat# 输出指定pid(即指定进程)的输出[root@localhost ~]# pidstat -p 4303 1Linux 3.10.0-229.el7.x86_64 (localhost.localdomain) 2016年02月01日 _x86_64_ (4 CPU)01时59分27秒 UID PID %usr %system %guest %CPU CPU Command01时59分28秒 998 4303 0.00 0.99 0.00 0.99 1 mysqld01时59分29秒 998 4303 1.00 0.00 0.00 1.00 1 mysqld01时59分30秒 998 4303 0.00 0.00 0.00 0.00 1 mysqld01时59分31秒 998 4303 0.00 1.00 0.00 1.00 1 mysqld01时59分32秒 998 4303 1.00 0.00 0.00 1.00 1 mysqld01时59分33秒 998 4303 0.00 0.00 0.00 0.00 1 mysqld01时59分34秒 998 4303 1.00 0.00 0.00 1.00 1 mysqld01时59分35秒 998 4303 0.00 0.00 0.00 0.00 1 mysqld01时59分36秒 998 4303 0.00 1.00 0.00 1.00 2 mysqld01时59分37秒 998 4303 1.00 0.00 0.00 1.00 2 mysqld01时59分38秒 998 4303 0.00 0.00 0.00 0.00 2 mysqld01时59分39秒 998 4303 0.00 0.00 0.00 0.00 2 mysqld01时59分40秒 998 4303 1.00 1.00 0.00 2.00 2 mysqld01时59分41秒 998 4303 0.00 0.00 0.00 0.00 2 mysqld01时59分42秒 998 4303 1.00 0.00 0.00 1.00 2 mysqld01时59分43秒 998 4303 0.00 0.00 0.00 0.00 2 mysqld01时59分44秒 998 4303 2.97 1.98 0.00 4.95 2 mysqld01时59分45秒 998 4303 1.00 0.00 0.00 1.00 2 mysqld01时59分46秒 998 4303 0.00 1.00 0.00 1.00 2 mysqld01时59分47秒 998 4303 20.00 24.00 0.00 44.00 0 mysqld01时59分48秒 998 4303 98.00 77.00 0.00 175.00 0 mysqld01时59分49秒 998 4303 130.00 113.00 0.00 243.00 0 mysqld01时59分50秒 998 4303 141.00 112.00 0.00 253.00 0 mysqld01时59分51秒 998 4303 149.00 95.00 0.00 244.00 0 mysqld01时59分52秒 998 4303 121.00 94.00 0.00 215.00 0 mysqld01时59分53秒 998 4303 141.00 110.00 0.00 251.00 0 mysqld01时59分54秒 998 4303 140.00 103.00 0.00 243.00 0 mysqld01时59分55秒 998 4303 124.00 92.00 0.00 216.00 0 mysqld01时59分56秒 998 4303 136.00 105.00 0.00 241.00 0 mysqld01时59分57秒 998 4303 138.00 102.00 0.00 240.00 0 mysqld01时59分58秒 998 4303 112.00 82.00 0.00 194.00 0 mysqld01时59分59秒 998 4303 138.00 106.00 0.00 244.00 0 mysqld02时00分00秒 998 4303 136.00 106.00 0.00 242.00 0 mysqld02时00分01秒 998 4303 131.00 101.00 0.00 232.00 0 mysqld02时00分02秒 998 4303 46.00 37.00 0.00 83.00 0 mysqld02时00分03秒 998 4303 140.00 106.00 0.00 246.00 0 mysqld02时00分04秒 998 4303 143.00 104.00 0.00 247.00 0 mysqld02时00分05秒 998 4303 127.00 99.00 0.00 226.00 0 mysqld02时00分06秒 998 4303 147.00 105.00 0.00 252.00 0 mysqld02时00分07秒 998 4303 126.00 102.00 0.00 228.00 0 mysqld02时00分08秒 998 4303 129.00 95.00 0.00 224.00 0 mysqld02时00分09秒 998 4303 141.00 103.00 0.00 244.00 0 mysqld02时00分10秒 998 4303 125.00 93.00 0.00 218.00 0 mysqld02时00分11秒 998 4303 138.00 105.00 0.00 243.00 0 mysqld02时00分11秒 UID PID %usr %system %guest %CPU CPU Command02时00分12秒 998 4303 142.00 113.00 0.00 255.00 0 mysqld02时00分13秒 998 4303 125.00 94.00 0.00 219.00 0 mysqld02时00分14秒 998 4303 78.00 61.00 0.00 139.00 0 mysqld02时00分15秒 998 4303 134.00 107.00 0.00 241.00 0 mysqld02时00分16秒 998 4303 142.57 107.92 0.00 250.50 0 mysqld02时00分17秒 998 4303 137.00 102.00 0.00 239.00 0 mysqld02时00分18秒 998 4303 140.00 105.00 0.00 245.00 0 mysqld02时00分20秒 998 4303 124.00 106.00 0.00 230.00 0 mysqld02时00分21秒 998 4303 128.00 94.00 0.00 222.00 0 mysqld02时00分22秒 998 4303 142.00 114.00 0.00 256.00 0 mysqld02时00分23秒 998 4303 140.00 114.00 0.00 254.00 0 mysqld平均时间: 998 4303 82.88 63.55 0.00 146.43 - mysqld# 输出指定pid的CPU统计信息[root@localhost ~]# pidstat -u -p 4303 1Linux 3.10.0-229.el7.x86_64 (localhost.localdomain) 2016年02月01日 _x86_64_ (4 CPU)02时00分29秒 UID PID %usr %system %guest %CPU CPU Command02时00分30秒 998 4303 134.00 100.00 0.00 234.00 0 mysqld02时00分31秒 998 4303 120.00 94.00 0.00 214.00 0 mysqld02时00分32秒 998 4303 143.00 104.00 0.00 247.00 0 mysqld^C平均时间: 998 4303 132.33 99.33 0.00 231.67 - mysqld# 输出指定进程的内存统计信息[root@localhost ~]# pidstat -r -p 4303 1 Linux 3.10.0-229.el7.x86_64 (localhost.localdomain) 2016年02月01日 _x86_64_ (4 CPU)02时00分59秒 UID PID minflt/s majflt/s VSZ RSS %MEM Command02时01分00秒 998 4303 0.00 0.00 2793112 167380 8.95 mysqld02时01分01秒 998 4303 228.00 0.00 2793112 167380 8.95 mysqld02时01分02秒 998 4303 0.00 0.00 2793112 167360 8.95 mysqld02时01分03秒 998 4303 3.00 2.00 2793112 167356 8.95 mysqld02时01分04秒 998 4303 226.00 1.00 2793112 168112 8.99 mysqld02时01分05秒 998 4303 0.00 0.00 2793112 167352 8.95 mysqld02时01分06秒 998 4303 0.00 1.00 2793112 167352 8.95 mysqld02时01分07秒 998 4303 3.92 0.98 2793112 167352 8.95 mysqld02时01分08秒 998 4303 95.00 4.00 2793112 167624 8.96 mysqld02时01分09秒 998 4303 226.00 0.00 2793112 167652 8.96 mysqld02时01分10秒 998 4303 0.00 0.00 2793112 167652 8.96 mysqld02时01分11秒 998 4303 227.00 0.00 2793112 167648 8.96 mysqld02时01分12秒 998 4303 0.00 0.00 2793112 167648 8.96 mysqld02时01分13秒 998 4303 227.00 0.00 2793112 167648 8.96 mysqld02时01分14秒 998 4303 2.00 0.00 2793112 167648 8.96 mysqld02时01分15秒 998 4303 226.00 0.00 2793112 167640 8.96 mysqld^C平均时间: 998 4303 91.04 0.56 2793112 167550 8.96 mysqld# 输出I/O的统计信息[root@localhost ~]# pidstat -dLinux 3.10.0-229.el7.x86_64 (localhost.localdomain) 2016年02月01日 _x86_64_ (4 CPU)02时01分32秒 UID PID kB_rd/s kB_wr/s kB_ccwr/s Command02时01分32秒 0 1 49.20 0.09 0.01 systemd02时01分32秒 0 653 0.00 0.00 0.00 kworker/u256:2902时01分32秒 0 746 0.02 0.00 0.00 xfsaild/dm-002时01分32秒 0 822 1.05 0.00 0.00 systemd-journal02时01分32秒 0 835 0.02 0.00 0.00 lvmetad02时01分32秒 0 843 5.14 0.00 0.00 systemd-udevd02时01分32秒 0 959 0.85 0.15 0.00 auditd02时01分32秒 0 981 5.21 0.00 0.00 firewalld02时01分32秒 0 986 0.46 0.00 0.00 irqbalance02时01分32秒 0 987 1.63 0.12 0.00 rsyslogd02时01分32秒 0 988 3.81 0.01 0.00 tuned02时01分32秒 0 991 0.65 0.00 0.00 systemd-logind02时01分32秒 81 992 1.05 0.00 0.00 dbus-daemon02时01分32秒 0 996 9.42 0.01 0.00 crond02时01分32秒 0 1004 0.02 0.00 0.00 agetty02时01分32秒 0 1099 9.04 0.01 0.00 NetworkManager02时01分32秒 999 1300 4.15 0.00 0.00 polkitd02时01分32秒 0 1580 4.13 0.01 0.00 dhclient02时01分32秒 0 1608 0.64 0.00 0.00 sshd02时01分32秒 0 2826 5.21 0.00 0.00 sshd02时01分32秒 0 2830 23.59 0.01 0.00 bash02时01分32秒 0 2851 53.59 3.08 0.12 bash02时01分32秒 0 2878 0.01 0.00 0.00 mysqld_safe02时01分32秒 998 4303 41.58 1891.53 4.24 mysqld02时01分32秒 0 4714 0.00 7.51 0.00 kworker/u256:102时01分32秒 0 4726 27.46 2.16 0.00 java02时01分32秒 0 4751 0.00 8.99 0.00 kworker/u256:002时01分32秒 0 4786 2.79 0.00 0.00 pidstat# 输出指定进程I/O的统计信息[root@localhost ~]# pidstat -d -p 4303 1Linux 3.10.0-229.el7.x86_64 (localhost.localdomain) 2016年02月01日 _x86_64_ (4 CPU)02时01分43秒 UID PID kB_rd/s kB_wr/s kB_ccwr/s Command02时01分44秒 998 4303 35.64 14756.44 0.00 mysqld02时01分45秒 998 4303 124.00 19640.00 0.00 mysqld02时01分46秒 998 4303 44.00 17540.00 0.00 mysqld02时01分47秒 998 4303 16.00 16740.00 0.00 mysqld02时01分48秒 998 4303 44.00 18280.00 0.00 mysqld02时01分49秒 998 4303 48.00 14132.00 0.00 mysqld02时01分50秒 998 4303 1300.00 18500.00 0.00 mysqld02时01分51秒 998 4303 728.00 12368.00 0.00 mysqld02时01分52秒 998 4303 5348.00 18224.00 0.00 mysqld02时01分53秒 998 4303 16.00 11244.00 0.00 mysqld^C平均时间: 998 4303 769.63 16141.06 0.00 mysqld# 输出正则匹配的进程统计信息[root@localhost ~]# pidstat -C mysql 1Linux 3.10.0-229.el7.x86_64 (localhost.localdomain) 2016年02月01日 _x86_64_ (4 CPU)02时23分57秒 UID PID %usr %system %guest %CPU CPU Command02时23分58秒 998 4303 8.91 77.23 0.00 86.14 3 mysqld02时23分58秒 UID PID %usr %system %guest %CPU CPU Command02时23分59秒 998 4303 16.83 35.64 0.00 52.48 3 mysqld02时23分59秒 UID PID %usr %system %guest %CPU CPU Command02时24分00秒 998 4303 5.00 27.00 0.00 32.00 3 mysqld^C平均时间: UID PID %usr %system %guest %CPU CPU Command平均时间: 998 4303 10.26 46.69 0.00 56.95 - mysqld pidstat的选项详解： -C comm输出与comm模糊匹配的进程。 -d输出I/O统计信息。 UID 任务的UID USER 任务的所有者 PID 任务的进程号 kB_rd/s 每秒进程从磁盘读取的数据量（单位：kB） kB_wr/s 每秒进程向磁盘写的数据量（单位：kB） kB_ccwr/s-]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[yum安装MySQLdb模块]]></title>
      <url>%2F2015%2F12%2F25%2Fyum%E5%AE%89%E8%A3%85MySQLdb%E6%A8%A1%E5%9D%97%2F</url>
      <content type="text"><![CDATA[在我使用的python2.7版本中，MySQLdb模块还不是python的内置模块，但是MySQLdb模块又是Python与MySQL连接的桥梁，对于作为MySQL DBA又很喜欢Python语言的我来说，MySQLdb真的是必需品呢。 MySQLdb依赖于mysql-devel包，所以首先我们需要先安装mysql-devel包可以去官网下载mysqldevel的rpm包，然后安装在服务器上。 直接用yum安装MySQLdbyum install -y MySQLdb-python 检验MySQLdb模块是否安装成功 [root@ip-172-31-1-8 ~]# pythonPython 2.7.10 (default, Dec 8 2015, 18:25:23) [GCC 4.8.3 20140911 (Red Hat 4.8.3-9)] on linux2Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import MySQLdb&gt;&gt;&gt; # import之后不出现任何提示或者错误，即使MySQLdb模块包安装成功]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[argparse命令行解析库]]></title>
      <url>%2F2015%2F12%2F18%2Fargparse%E5%91%BD%E4%BB%A4%E8%A1%8C%E8%A7%A3%E6%9E%90%E5%BA%93%2F</url>
      <content type="text"><![CDATA[argparse为命令行解析库，可以轻松的写出交互友好式的命令行。 开胃菜以官方一个简单例子展示argparse模块的用处：#!/usr/bin/env python#coding:utf-8import argparseparser = argparse.ArgumentParser(description='Process some integers.')parser.add_argument('integers', metavar='N', type=int, nargs='+', help='an integer for the accumulator')parser.add_argument('--sum', dest='accumulate', action='store_const', const=sum, default=max, help='sum the integers')args = parser.parse_args()print args.accumulate(args.integers) 运行结果如下：[root@ip-172-31-1-8 python]# python argparse_demo.py -husage: argparse_test.py [-h] [--sum] N [N ...]Process some integers.positional arguments: N an integer for the accumulatoroptional arguments: -h, --help show this help message and exit --sum sum the integers 创建解析通过argparse.ArgumentParser()创建解析。ArgumentParser(prog='', usage=None, description='Process some integers.', version=None, formatter_class=&lt;class 'argparse.HelpFormatter'&gt;, conflict_handler='error', add_help=True)# 例如parser = argparse.ArgumentParser(description='Process some integers.') 解释 prog程序名称(默认sys.argv[0]) # 变更参数parser = argparse.ArgumentParser(prog='sum or max',description='Process some integers.')#运行结果#原始[root@ip-172-31-1-8 python]# python argparse_test.py -husage: argparse_test.py [-h] [--sum] N [N ...]#变更后[root@ip-172-31-1-8 python]# python argparse_test.py -husage: sum or max [-h] [--sum] N [N ...] usage用于描述程序的使用用法（默认为添加到解析器中的参数） # 变更参数parser = argparse.ArgumentParser(usage='python argparse_demo.py arguments',description='Process some integers.')#运行结果#原始[root@ip-172-31-1-8 python]# python argparse_demo.py -husage: argparse_demo.py [-h] [--sum] N [N ...]#变更后[root@ip-172-31-1-8 python]# python argparse_demo.py -husage: python argparse_demo.py arguments description参数帮助前的显示文本，可以通过该参数来描述该程序的功能 # 变更参数parser = argparse.ArgumentParser(description='Process some integers.')# 运行结果[root@ip-172-31-1-8 python]# python argparse_demo.py -husage: python argparse_demo.py argumentProcess some integers. epilog参数选项帮助后的显示文本 # 变更参数epilog='And What can I help U?'# 运行结果optional arguments: -h, --help show this help message and exit --sum sum the intergersAnd What can I help U? parents共享同一个父类解析器，由ArgumentParser对象组成的列表，它们的arguments选项会被包含到新ArgumentParser对象中。 formatter_classhelp信息输出格式共有三种形式：argparse.RawDescriptionHelpFormatter、argparse.RawTextHelpFormatter、argparse.ArgumentDefaultsHelpFormatterargparse.RawDescriptionHelpFormatter:description以输入格式输出，并不将其合并为一行argparse.RawTextHelpFormatter:所有信息以输入格式输出，并不将其合并为一行argparse.ArgumentDefaultsHelpFormatter:输出参数的defalut值 # 变更参数formatter_class=argparse.ArgumentDefaultsHelpFormatter# 运行结果optional arguments: -h, --help show this help message and exit --sum sum the intergers (default: max) prefix_chars参数前缀，默认为’-‘ fromfile_prefix_chars前缀字符，放在文件名之前当参数过多时，可以将参数放在文件中读取。 argument_default参数的全局默认值 conflict_handler解决冲突的策略 # 没有conflict_handler冲突策略&gt;&gt;&gt; parser=argparse.ArgumentParser(prog='TEST')&gt;&gt;&gt; parser.add_argument('-f','--foo',help='old foo help')_StoreAction(option_strings=['-f', '--foo'], dest='foo', nargs=None, const=None, default=None, type=None, choices=None, help='old foo help', metavar=None)&gt;&gt;&gt; parser.add_argument('-f','--foo',help='new foo help') Traceback (most recent call last):......argparse.ArgumentError: argument -f/--foo: conflicting option string(s): -f, --foo#有conflict_handler策略&gt;&gt;&gt; parser=argparse.ArgumentParser(prog='TEST',conflict_handler='resolve')&gt;&gt;&gt; parser.add_argument('-f','--foo',help='old foo help')_StoreAction(option_strings=['-f', '--foo'], dest='foo', nargs=None, const=None, default=None, type=None, choices=None, help='old foo help', metavar=None)&gt;&gt;&gt; parser.add_argument('-f','--foo',help='new foo help') _StoreAction(option_strings=['-f', '--foo'], dest='foo', nargs=None, const=None, default=None, type=None, choices=None, help='new foo help', metavar=None)&gt;&gt;&gt; parser.print_help()usage: TEST [-h] [-f FOO]optional arguments: -h, --help show this help message and exit -f FOO, --foo FOO new foo help add_help设为False时，不再显示-h,–help信息，建议使用True 添加参数通过argparse.ArgumentParser.add_argument()添加参数# 例如parser.add_argument('intergers',metavar='N',type=int,nargs='+',help='an interger for the accumulator')parser.add_argument('--sum',dest='accumulate',action='store_const',const=sum,default=max,help='sum the intergers (default:find the max)')args = parser.parse_args() name or flagoptional arguments以’-‘为前缀的参数,其他的为positional arguments # 添加optional argument&gt;&gt;&gt; parser = argparse.ArgumentParser()&gt;&gt;&gt; parser.add_argument('-t','--test')_StoreAction(option_strings=['-t', '--test'], dest='test', nargs=None, const=None, default=None, type=None, choices=None, help=None, metavar=None)# 添加position argument&gt;&gt;&gt; parser = argparse.ArgumentParser()&gt;&gt;&gt; parser.add_argument('test')_StoreAction(option_strings=[], dest='test', nargs=None, const=None, default=None, type=None, choices=None, help=None, metavar=None) action命令行参数的操作store:仅仅存储参数的值（默认） &gt;&gt;&gt; parser=argparse.ArgumentParser(conflict_handler='resolve')&gt;&gt;&gt; parser.add_argument('-t')_StoreAction(option_strings=['-t'], dest='t', nargs=None, const=None, default=None, type=None, choices=None, help=None, metavar=None)&gt;&gt;&gt; parser.parse_args('-t 1'.split()) Namespace(t='1') store_const:存储const关键字指定的值&gt;&gt;&gt; import argparse&gt;&gt;&gt; parser=argparse.ArgumentParser(conflict_handler='resolve')&gt;&gt;&gt; parser.add_argument('-t',action='store_const',const=7)_StoreConstAction(option_strings=['-t'], dest='t', nargs=0, const=7, default=None, type=None, choices=None, help=None, metavar=None)&gt;&gt;&gt; parser.parse_args('-t'.split())Namespace(t=7) store_true/store_false:值为True/Falseappend:值追加到list中append_const:将const的值追加到list中count:统计参数出现的次数help:显示help信息version:显示version信息 nrgs参数的数量N：N个参数?:首先从命令行中获取，若没有则从const中获取，仍然没有则从default中获取*/+:任意多个参数 const保存为一个常量 default默认值 type参数类型 choices可供选择的值，该值适合于所有类型，如list、set等 required是否为必选参数 desk参数别名 help参数的帮助信息，即解释信息 # 添加help参数parser.add_argument('intergers',metavar='N',type=int,nargs='+',help='an interger for the accumulator')parser.add_argument('--sum',dest='accumulate',action='store_const',const=sum,default=max,help='sum the intergers')# 运行结果[root@ip-172-31-1-8 python]# python argparse_demo.py -husage: python argparse_demo.py argumentsProcess some integers.positional arguments: N an interger for the accumulatoroptional arguments: -h, --help show this help message and exit --sum sum the intergers (default: max) metavar相当于参数别名 解析参数通过argparse.ArgumentParser.parse_args()解析参数。注意argparse.ArgumentParser.parse_args()中的参数可以模糊匹配，如下：&gt;&gt;&gt; parser = argparse.ArgumentParser(prog='PROG')&gt;&gt;&gt; parser.add_argument('-bacon')&gt;&gt;&gt; parser.add_argument('-badger')&gt;&gt;&gt; parser.parse_args('-bac MMM'.split())Namespace(bacon='MMM', badger=None)&gt;&gt;&gt; parser.parse_args('-bad WOOD'.split())Namespace(bacon=None, badger='WOOD')&gt;&gt;&gt; parser.parse_args('-ba BA'.split())usage: PROG [-h] [-bacon BACON] [-badger BADGER]PROG: error: ambiguous option: -ba could match -badger, -bacon 参数组使用ArgumentParser.add_argument_group(title=None, description=None)添加参数组# 执行脚本#!/usr/bin/env python#coding:utf-8import argparse# 参数解析器重命名parser = argparse.ArgumentParser(usage='python argparse_demo.py arguments',description='Process some integers.')# 添加参数parser.add_argument('--sum',dest='accumulate',action='store_const',const=sum,default=max,help='sum the intergers (default:find the max)')# 添加参数组1group1 = parser.add_argument_group('group1','group test 1')#为参数组添加参数group1.add_argument('intergers',metavar='N',type=int,nargs='+',help='an interger for the accumulator')group1.add_argument('lists',metavar='L',type=list,nargs='+',help='a list for the accumulator')args = parser.parse_args()print args.accumulate(args,intergers)# 运行结果[root@ip-172-31-1-8 python]# python argparse_demo.py --helpusage: python argparse_demo.py argumentsProcess some integers.optional arguments: -h, --help show this help message and exit --sum sum the intergers (default:find the max)group1: group test 1 N an interger for the accumulator L a list for the accumulator 设置默认值使用ArgumentParser.set_defaults(kwargs)设置参数默认值,ArgumentParser.get_default(dest)获取参数默认值。 注意**解释器级别的默认值会覆盖参数级别的默认值。&gt;&gt;&gt; import argparse&gt;&gt;&gt; prser=argparse.ArgumentParser()&gt;&gt;&gt; parser=argparse.ArgumentParser()# 参数级别默认值&gt;&gt;&gt; parser.add_argument('--foo',default='7') _StoreAction(option_strings=['--foo'], dest='foo', nargs=None, const=None, default='7', type=None, choices=None, help=None, metavar=None)# 解析器级别默认值&gt;&gt;&gt; parser.set_defaults(foo='77')# 解析器级别的默认值覆盖参数级别的默认值&gt;&gt;&gt; parser.parse_args([])Namespace(foo='77')# 获取参数默认值&gt;&gt;&gt; parser.get_default('foo')'77']]></content>
    </entry>

    
    <entry>
      <title><![CDATA[keepalived的安装]]></title>
      <url>%2F2015%2F12%2F14%2Fkeepalived%E7%9A%84%E5%AE%89%E8%A3%85%2F</url>
      <content type="text"><![CDATA[由于日后在工作中会需要用到keepalived做高可用，故记录安装过程，为日后工作方便。。。 创建keepalived目录 mkdir -p /usr/keepalived 解压keepalvied安装包到keepalived目录下 tar -zxvf keepalived-1.2.19.tar.gz -C /usr/keepalived/ 安装依赖包 yum install -y gcc gcc-c++yum install -y openssl openssl-devel -y 编译安装keepalived ./configure --prefix=/usr/keepalived makemake install 设置keepalived自启动 cp /usr/keepalived/sbin/keepalived /usr/sbin/cp /usr/keepalived/etc/sysconfig/keepalived /etc/sysconfig/cp /usr/keepalived/etc/rc.d/init.d/keepalived /etc/init.d/ 设置keepalived的配置文件 mkdir -p /etc/keepalivedcd /etc/keepalived/vi keepalived.conf]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Redis命令随笔]]></title>
      <url>%2F2015%2F12%2F11%2FRedis%E5%91%BD%E4%BB%A4%E9%9A%8F%E7%AC%94%2F</url>
      <content type="text"><![CDATA[Redis有着各种各样的命令，并且记录是个很好的习惯…… 事务使用’multi’开启一个事务，期间执行各种命令都是在一个线程上操作，保持其原子性；使用’exec’提交事务，执行事务中的所有命令；# session 1127.0.0.1:6379&gt; flushdbOK127.0.0.1:6379&gt; get user(nil)127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; set user 1QUEUED127.0.0.1:6379&gt; incr user QUEUED127.0.0.1:6379&gt; incr user QUEUED127.0.0.1:6379&gt; incr user QUEUED127.0.0.1:6379&gt; get userQUEUED# session 2127.0.0.1:6379&gt; get user(nil)# session 1127.0.0.1:6379&gt; exec1) OK2) (integer) 23) (integer) 34) (integer) 4127.0.0.1:6379&gt; get user&quot;4&quot;# session 2127.0.0.1:6379&gt; get user&quot;4&quot; 慢日志cofig set slowlog-log-slower-than times设置慢日志的时长 slowlog get获取所有的慢日志时间 slowlog get counts获取最近的counts个数的慢日志# 设置slowlog的时长为0127.0.0.1:6379&gt; config set slowlog-log-slower-than 0# 获取所有的慢日志127.0.0.1:6379&gt; slowlog get1) 1) (integer) 1 2) (integer) 1449934538 3) (integer) 32 4) 1) &quot;slowlog&quot; 2) &quot;get&quot;# 获取最近出现的两个慢日志127.0.0.1:6379&gt; slowlog get 21) 1) (integer) 9 2) (integer) 1449934878 3) (integer) 23 4) 1) &quot;slowlog&quot; 2) &quot;get&quot;2) 1) (integer) 8 2) (integer) 1449934877 3) (integer) 3 4) 1) &quot;flushdb&quot; 获取慢日志结果详解127.0.0.1:6379&gt; slowlog get8) 1) (integer) 1 2) (integer) 1449934514 3) (integer) 54 4) 1) &quot;config&quot; 2) &quot;get&quot; 3) &quot;*&quot;9) 1) (integer) 0# 自增ID 2) (integer) 1449934507# 命令发生的时间戳 3) (integer) 5# 命令运行的时长（微秒） 4) 1) &quot;config&quot; 2) &quot;set&quot; 3) &quot;slowlog-log-slower-than&quot; 4) &quot;0&quot;# 运行的命令与其参数 Redis的慢日志在默认情况下，只保留1024条慢日志，并且slowlog是保存在内存里的，所以一旦reids重启之后，slowlog也会丢失。 排序sort对list、set、sorted set进行排序# list127.0.0.1:6379&gt; lrange name 0 -11) &quot;5&quot;2) &quot;2&quot;3) &quot;3&quot;127.0.0.1:6379&gt; sort name1) &quot;2&quot;2) &quot;3&quot;3) &quot;5&quot;127.0.0.1:6379&gt; lrange name 0 -11) &quot;5&quot;2) &quot;2&quot;3) &quot;3&quot;# set127.0.0.1:6379&gt; sadd name wing fianna bibi chou zhou(integer) 5127.0.0.1:6379&gt; smembers name1) &quot;bibi&quot;2) &quot;fianna&quot;3) &quot;chou&quot;4) &quot;wing&quot;5) &quot;zhou&quot;127.0.0.1:6379&gt; sort name alpha1) &quot;bibi&quot;2) &quot;chou&quot;3) &quot;fianna&quot;4) &quot;wing&quot;5) &quot;zhou&quot;127.0.0.1:6379&gt; sort name limit 0 3 alpha1) &quot;bibi&quot;2) &quot;chou&quot;3) &quot;fianna&quot;# sorted name127.0.0.1:6379&gt; zadd name 0 wing(integer) 1127.0.0.1:6379&gt; zadd name 1 fianna(integer) 1127.0.0.1:6379&gt; zadd name 2 bibi(integer) 1127.0.0.1:6379&gt; zadd name 3 chou(integer) 1127.0.0.1:6379&gt; zadd name 4 zhou(integer) 1127.0.0.1:6379&gt; zrange name 0 -11) &quot;wing&quot;2) &quot;fianna&quot;3) &quot;bibi&quot;4) &quot;chou&quot;5) &quot;zhou&quot;127.0.0.1:6379&gt; sort name alpha1) &quot;bibi&quot;2) &quot;chou&quot;3) &quot;fianna&quot;4) &quot;wing&quot;5) &quot;zhou&quot;127.0.0.1:6379&gt; sort name limit 0 3 desc alpha1) &quot;zhou&quot;2) &quot;wing&quot;3) &quot;fianna&quot;127.0.0.1:6379&gt; sort name limit 0 3 asc alpha1) &quot;bibi&quot;2) &quot;chou&quot;3) &quot;fianna&quot; 管理命令 config get parameter获取配置参数信息 # 获取全部参数信息127.0.0.1:6379&gt; config get * 1) &quot;dbfilename&quot; 2) &quot;dump.rdb&quot; 3) &quot;requirepass&quot; 4) &quot;&quot;...127) &quot;notify-keyspace-events&quot;128) &quot;&quot;129) &quot;bind&quot;130) &quot;&quot;# 获取模糊参数信息127.0.0.1:6379&gt; config get *log* 1) &quot;logfile&quot; 2) &quot;&quot;...13) &quot;loglevel&quot;14) &quot;notice&quot;# 获取指定参数信息127.0.0.1:6379&gt; config get slowlog-log-slower-than1) &quot;slowlog-log-slower-than&quot;2) &quot;10000&quot; 关键字（key） DEL删除命令，返回的是实际删除的key的数量。 127.0.0.1:6379&gt; set name wingOK127.0.0.1:6379&gt; get name&quot;wing&quot;127.0.0.1:6379&gt; del name (integer) 1127.0.0.1:6379&gt; get name(nil)# 注意 del命令返回的整数是实际删除的key的数量127.0.0.1:6379&gt; set name wingOK127.0.0.1:6379&gt; set user wingOK127.0.0.1:6379&gt; del name user school(integer) 2 DUMP用于获取存储在Redis中键数据的序列化版本，返回的是序列化的值。 127.0.0.1:6379&gt; set name wing OK127.0.0.1:6379&gt; dump name&quot;\x00\x04wing\x06\x00\x0fQD\x89^\xc7u\xc5&quot;127.0.0.1:6379&gt; set name fiannaOK127.0.0.1:6379&gt; get name&quot;fianna&quot;127.0.0.1:6379&gt; dump name&quot;\x00\x06fianna\x06\x00\xce\x1aa\xf3K\x0eh\x1b&quot; EXISTS返回key是否存在,返回值1表示存在，返回值0表示不存在。 127.0.0.1:6379&gt; get name&quot;fianna&quot;127.0.0.1:6379&gt; exists name(integer) 1127.0.0.1:6379&gt; del name(integer) 1127.0.0.1:6379&gt; exists name(integer) 0 EXPIRE key seconds设置key过期时间。返回值1表示key已经过期，返回值0表示key已经删除或者不能设置为过期。 127.0.0.1:6379&gt; set name wingOK127.0.0.1:6379&gt; expire name 10(integer) 1127.0.0.1:6379&gt; ttl name(integer) 7127.0.0.1:6379&gt; ttl name(integer) 4127.0.0.1:6379&gt; ttl name(integer) 3127.0.0.1:6379&gt; ttl name(integer) -2127.0.0.1:6379&gt; ttl name(integer) -2127.0.0.1:6379&gt; ttl name(integer) -2127.0.0.1:6379&gt; get name(nil)127.0.0.1:6379&gt; exists name(integer) 0 EXPIREAT unix timestamp ‘与EXPIRE相似，EXPIREAT后接的是unix时间戳 # unix时间戳mysql&gt; select unix_timestamp(&apos;2015-12-27 00:00:00&apos;);+---------------------------------------+| unix_timestamp(&apos;2015-12-27 00:00:00&apos;) |+---------------------------------------+| 1451174400 |+---------------------------------------+1 row in set (0.12 sec)# 当前时间2015/12/27 20：11：00127.0.0.1:6379&gt; set name wingOK127.0.0.1:6379&gt; expireat name 1451174400(integer) 1127.0.0.1:6379&gt; exists name(integer) 0127.0.0.1:6379&gt; exists name(integer) 0127.0.0.1:6379&gt; ttl name(integer) -2127.0.0.1:6379&gt; ttl name# unix时间戳mysql&gt; select unix_timestamp(&apos;2015-12-27 20:13:00&apos;); +---------------------------------------+| unix_timestamp(&apos;2015-12-27 20:13:00&apos;) |+---------------------------------------+| 1451247180 |+---------------------------------------+1 row in set (0.00 sec)# 当前时间2015/12/27 20：11：53127.0.0.1:6379&gt; set name wingOK127.0.0.1:6379&gt; expireat name 1451247180(integer) 1127.0.0.1:6379&gt; ttl name(integer) 28866127.0.0.1:6379&gt; ttl name(integer) 28863127.0.0.1:6379&gt; ttl name(integer) 0 KEYS]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Redis的数据结构]]></title>
      <url>%2F2015%2F12%2F06%2FRedis%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
      <content type="text"><![CDATA[Redis的数据结构多种多样，千姿百态~ String字符串 set设置键值 127.0.0.1:6379&gt; set resource:look &quot;redis demo&quot;OK get获得键值 127.0.0.1:6379&gt; get resource:look&quot;redis demo&quot; expire设置键值过期时间 ttl显示键值是否过期及过期时间（正数：过期秒数，-1：永不过期，-2：已经过期） 127.0.0.1:6379&gt; set resource:look &apos;redis demo&apos;OK127.0.0.1:6379&gt; ttl resource:look(integer) -1# 键值永不过期127.0.0.1:6379&gt; expire resource:look 10(integer) 1127.0.0.1:6379&gt; ttl resource:look(integer) 9# 键值还有9s过期127.0.0.1:6379&gt; ttl resource:look(integer) 7# 键值还有7s过期127.0.0.1:6379&gt; ttl resource:look(integer) -2# 键值已经过期 INCR让键值自增 127.0.0.1:6379&gt; set connections 10OK127.0.0.1:6379&gt; incr connections(integer) 11127.0.0.1:6379&gt; incr connections(integer) 12 DEL删除键值 127.0.0.1:6379&gt; set name wingOK127.0.0.1:6379&gt; get name &quot;wing&quot;127.0.0.1:6379&gt; del name(integer) 1127.0.0.1:6379&gt; get name(nil) list列表 RPUSH在列表的右侧添加值 127.0.0.1:6379&gt; rpush name &apos;wing&apos;(integer) 1127.0.0.1:6379&gt; rpush name &apos;fianna&apos;(integer) 2127.0.0.1:6379&gt; rpush name &apos;bibi&apos;(integer) 3# 列表中可以添加重复值127.0.0.1:6379&gt; rpush test &apos;wing&apos;(integer) 1127.0.0.1:6379&gt; rpush test &apos;wing&apos;(integer) 2127.0.0.1:6379&gt; rpush test &apos;wing&apos;(integer) 3127.0.0.1:6379&gt; lrange test 0 -11) &quot;wing&quot;2) &quot;wing&quot;3) &quot;wing&quot; LPUSH在列表的左侧添加值 127.0.0.1:6379&gt; lpush name &apos;zhou&apos;(integer) 4 LRANGE从左开始按照对应的范围去取列表值，-1表示一直到最后一个列表值，注意Redis的列表从0开始标记元素 127.0.0.1:6379&gt; lrange name 0 -11) &quot;zhou&quot;2) &quot;wing&quot;3) &quot;fianna&quot;4) &quot;bibi&quot;127.0.0.1:6379&gt; lrange name 0 11) &quot;zhou&quot;2) &quot;wing&quot;127.0.0.1:6379&gt; lrange name 2 41) &quot;fianna&quot;2) &quot;bibi&quot;127.0.0.1:6379&gt; lrange name 2 61) &quot;fianna&quot;2) &quot;bibi&quot;127.0.0.1:6379&gt; lrange name 2 -11) &quot;fianna&quot;2) &quot;bibi&quot; LLEN显示列表的长度 127.0.0.1:6379&gt; LLEN name(integer) 4 LPOP删除列表左起第一个元素 127.0.0.1:6379&gt; LRANGE name 0 -11) &quot;zhou&quot;2) &quot;wing&quot;3) &quot;fianna&quot;4) &quot;bibi&quot;127.0.0.1:6379&gt; LPOP name &quot;zhou&quot;127.0.0.1:6379&gt; LRANGE name 0 -11) &quot;wing&quot;2) &quot;fianna&quot;3) &quot;bibi&quot; RPOP删除列表右起第一个元素 127.0.0.1:6379&gt; LRANGE name 0 -11) &quot;wing&quot;2) &quot;fianna&quot;3) &quot;bibi&quot;127.0.0.1:6379&gt; RPOP name&quot;bibi&quot;127.0.0.1:6379&gt; LRANGE name 0 -11) &quot;wing&quot;2) &quot;fianna&quot; SET集合 SADD在集合中添加值 127.0.0.1:6379&gt; SADD name &apos;wing&apos;(integer) 1127.0.0.1:6379&gt; SADD name &apos;fianna&apos;(integer) 1127.0.0.1:6379&gt; SADD name &apos;bibi&apos;(integer) 1127.0.0.1:6379&gt; SADD name &apos;zhou&apos;(integer) 1 SREM删除集合中的指定的值 127.0.0.1:6379&gt; SREM name &apos;zhou&apos;(integer) 1 SISMEMBER测试某个值是否在集合中，1表示存在，0表示不存在 127.0.0.1:6379&gt; SISMEMBER name &apos;fianna&apos;(integer) 1127.0.0.1:6379&gt; SISMEMBER name &apos;zhou&apos;(integer) 0 SMEMBERS显示当前集合的所有值 127.0.0.1:6379&gt; SMEMBERS name1) &quot;fianna&quot;2) &quot;wing&quot;3) &quot;bibi&quot; SUNION显示所有集合的合并值 # 五重复值时显示所有值127.0.0.1:6379&gt; SMEMBERS name1) &quot;fianna&quot;2) &quot;wing&quot;3) &quot;bibi&quot;127.0.0.1:6379&gt; SMEMBERS test1) &quot;i&quot;2) &quot;who&quot;3) &quot;am&quot;127.0.0.1:6379&gt; SUNION name test1) &quot;wing&quot;2) &quot;fianna&quot;3) &quot;bibi&quot;4) &quot;i&quot;5) &quot;who&quot;6) &quot;am&quot;# 存在重复值时，去重127.0.0.1:6379&gt; SMEMBERS test1) &quot;i&quot;2) &quot;fianna&quot;3) &quot;who&quot;4) &quot;am&quot;127.0.0.1:6379&gt; SMEMBERS name1) &quot;fianna&quot;2) &quot;wing&quot;3) &quot;bibi&quot;127.0.0.1:6379&gt; SUNION name test1) &quot;bibi&quot;2) &quot;fianna&quot;3) &quot;i&quot;4) &quot;wing&quot;5) &quot;who&quot;6) &quot;am&quot; SINTER显示两个集合中的交集，即共同元素 127.0.0.1:6379&gt; smembers friends:wing1) &quot;c&quot;2) &quot;b&quot;3) &quot;d&quot;4) &quot;a&quot;127.0.0.1:6379&gt; smembers friends:fianna1) &quot;f&quot;2) &quot;e&quot;3) &quot;b&quot;4) &quot;g&quot;5) &quot;a&quot;127.0.0.1:6379&gt; sinter friends:wing friends:fianna1) &quot;b&quot;2) &quot;a&quot; SINTERSTORE将两个集合中的交集，存入到SINTERSTORE后的第一个参数中 127.0.0.1:6379&gt; smembers friends:wing1) &quot;c&quot;2) &quot;b&quot;3) &quot;d&quot;4) &quot;a&quot;127.0.0.1:6379&gt; smembers friends:fianna1) &quot;f&quot;2) &quot;e&quot;3) &quot;b&quot;4) &quot;g&quot;5) &quot;a&quot;127.0.0.1:6379&gt; sinter friends:wing friends:fianna1) &quot;b&quot;2) &quot;a&quot;127.0.0.1:6379&gt; sinterstore friends:wing_fianna friends:wing friends:fianna (integer) 2127.0.0.1:6379&gt; smembers friends:wing_fianna1) &quot;b&quot;2) &quot;a&quot; SORTED SET有序的集合 ZADD在有序的集合中添加元素 127.0.0.1:6379&gt; zadd name 10 &apos;wing&apos;(integer) 1# 相同的数据是不会被添加到集合中127.0.0.1:6379&gt; zadd name 11 &apos;wing&apos;(integer) 0127.0.0.1:6379&gt; zadd name 9 &apos;fianna&apos;(integer) 1 ZRANGE查看有序集合的元素 127.0.0.1:6379&gt; zrange name 0 -11) &quot;fianna&quot;2) &quot;wing&quot; ZCOUNT显示有序集合在指定的排名中的数量 127.0.0.1:6379&gt; zadd friends:wing 100 fianna 90 mama 80 daday 70 sister 60 brother 50 friend 95 who127.0.0.1:6379&gt; zrange friends:wing 0 -11) &quot;friend&quot;2) &quot;brother&quot;3) &quot;sister&quot;4) &quot;daday&quot;5) &quot;mama&quot;6) &quot;who&quot;7) &quot;fianna&quot;127.0.0.1:6379&gt; zcount friends:wing 90 100(integer) 3 ZREVRANK显示指定有序集合元素的对应等级 127.0.0.1:6379&gt; zadd friends:wing 100 fianna 90 mama 80 daday 70 sister 60 brother 50 friend 95 who127.0.0.1:6379&gt; zrange friends:wing 0 -11) &quot;friend&quot;2) &quot;brother&quot;3) &quot;sister&quot;4) &quot;daday&quot;5) &quot;mama&quot;6) &quot;who&quot;7) &quot;fianna&quot;127.0.0.1:6379&gt; zrevrank friends:wing who(integer) 1 Hashes哈希 HSET在哈希中添加元素 127.0.0.1:6379&gt; HGETALL user:1000(empty list or set)127.0.0.1:6379&gt; HSET user:1000 name &apos;wing&apos;(integer) 1127.0.0.1:6379&gt; HSET user:1000 email &apos;wing324@126.com&apos;(integer) 1 HGETALL查看哈希中的所有元素 127.0.0.1:6379&gt; HGETALL user:10001) &quot;name&quot;2) &quot;wing&quot;3) &quot;email&quot;4) &quot;wing324@126.com&quot; HMSET对于哈希类型，一次设置多个值 127.0.0.1:6379&gt; HMSET user:1000 name &apos;fianna&apos; email &apos;fianna125@126.com&apos;OK127.0.0.1:6379&gt; HGETALL user:10001) &quot;name&quot;2) &quot;fianna&quot;3) &quot;email&quot;4) &quot;fianna125@126.com&quot; HGET只获取哈希类型的部分指定元素 127.0.0.1:6379&gt; HGET user:1000 name&quot;fianna&quot; HINCRBY对哈希值自增 127.0.0.1:6379&gt; HSET user:1000 visit 10(integer) 1127.0.0.1:6379&gt; HINCRBY user:1000 visit 1(integer) 11127.0.0.1:6379&gt; HINCRBY user:1000 visit 1(integer) 12127.0.0.1:6379&gt; HINCRBY user:1000 visit 10(integer) 22 HDEL删除对应的哈希值 127.0.0.1:6379&gt; HDEL user:1000 visit(integer) 1127.0.0.1:6379&gt; HINCRBY user:1000 visit 1(integer) 1127.0.0.1:6379&gt; HINCRBY user:1000 visit 10(integer) 11]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Redis的quickstart]]></title>
      <url>%2F2015%2F12%2F05%2FRedis%E7%9A%84quickstart%2F</url>
      <content type="text"><![CDATA[NoSQL现在真是如火如荼，听的最多的局势Redis了，而我并不了解这个东西，但是有很多很多人都在用，所以我决定学习学习，看看这个东东好在哪里。。。 Redis安装参考链接：http://redis.io/download操作系统：CentOS 7Redis版本：3.0.5 在参考链接中获得Redis安装包，或者通过如下方式获得：wget http://download.redis.io/releases/redis-3.0.5.tar.gz 解压软件至指定目录mkdir -p /usr/local/redistar -zxvf redis-3.0.5.tar.gz -C /usr/local/redis/ 编译安装包cd /usr/local/redis/redis-3.0.5make 启动Redis服务/usr/local/redis/redis-3.0.5/src/redis-server &amp; 验证Redis启动/usr/local/redis/redis-3.0.5/src/redis-cli127.0.0.1:6379&gt; keys *(empty list or set) 推出Redisctrl+D Redis启动 直接启动redis-server &amp; 指定配置文件启动redis-server /usr/local/redis/etc/redis.conf # 修改配置文件redis.conf设置redis为后台启动# 将redis.conf文件中daemonize no修改为：daemonize yes# 启动Redis[root@localhost etc]# redis-server /usr/local/redis/etc/redis.conf [root@localhost etc]# ps -ef | grep redisroot 2907 1 0 01:45 ? 00:00:00 redis-server *:6379root 2911 2827 0 01:45 pts/0 00:00:00 grep --color=auto redis 使用Redis自启动脚本Redis的启动脚本位于redis-3.0.5/utils/redis_init_script下启动Redis自启动脚本步骤1.新建目录 /etc/redis用来存放Redis的配置文件2.复制redis.conf到/etc/redis目录下并命名为6379.conf3.修改6379.conf配置文件4.复制redis_init_script脚本文件到/etc/init.d目录中，并命名为redis5.执行随系统自动启动命令chmod a+x /etc/init.d/redisservice redis start Redis停止 使用Ctrl+C 在客户端执行SHUTDOWN kill -9 PID Redis配置文件Redis有两种配置文件，redis.conf文件用于redis server的配置文件，sentinel.conf用于redis sentinel配置文件，用于监控。更详细的内容请看未来更新的文章~]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[MySQL Got error 28 from storage engine]]></title>
      <url>%2F2015%2F11%2F17%2FMySQL-Got-error-28-from-storage-engine%2F</url>
      <content type="text"><![CDATA[今天进入数据库后，show tables居然报了一个error:Got error 28 from storage engine，第一次遇到，我还以为我的数据库出现了大问题了，但心里不着急，因为这只是我个人使用的测试库，但还是想赶紧去看看这个error是什么鬼。 经过google，在stackoverflow中别人说是磁盘不够的原因，于是我敲下了如下命令：[root@host-192-168-200-153 ~]# df -hFilesystem Size Used Avail Use% Mounted on/dev/mapper/VolGroup-lv_root 6.7G 6.6G 0 100% /tmpfs 939M 0 939M 0% /dev/shm/dev/vda1 485M 32M 428M 7% /boot 纳尼，果然是磁盘是真的不够，但是不知道是不是这个原因导致的呢。于是我删除了一些无用的东西之后，再敲下同样的命令：[root@host-192-168-200-153 ~]# df -hFilesystem Size Used Avail Use% Mounted on/dev/mapper/VolGroup-lv_root 6.7G 6.2G 118M 99% /tmpfs 939M 0 939M 0% /dev/shm/dev/vda1 485M 32M 428M 7% /boot 现在磁盘够了，我赶紧去数据库再去执行show tables，然后然后，重要的时刻出现了，居然不再报错了。。原来真的是磁盘不够哇。。。这个问题就这么解决了。。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[MySQL之sql_mode]]></title>
      <url>%2F2015%2F11%2F12%2FMySQL%D6%AEsql-mode%2F</url>
      <content type="text"><![CDATA[曾经我一直忽视了sql_mode的作用，直到有一天认识了ONLE_FULL_GROUP_BY模式，我才重新认识了sql_mode,才发现sql_mode还是有很多不容小视的设置。 说明MySQL版本root@localhost : wing 06:18:33&gt; select version();+------------+| version() |+------------+| 5.6.26-log |+------------+1 row in set (0.00 sec) SQL Modes常用的sql_mode ANSI STRICT_TRANS_TABLES对于事务型表，insert非法值之后，回滚整条语句；对于非事务型表，如果是单值语句或者是第一条记录就是非法的多值插入将回滚整条语句，对于第一条记录不是非法的多值插入，将会给非法值替换为可能值。 # 事务型表结构root@localhost : wing 06:24:46&gt; show create table innodb_test;+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+| innodb_test | CREATE TABLE `innodb_test` ( `id` int(10) unsigned NOT NULL, `name` varchar(8) NOT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 |+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec)# 非事务型表结构root@localhost : wing 06:27:28&gt; show create table myisam_test; +-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+| myisam_test | CREATE TABLE `myisam_test` ( `id` int(10) unsigned NOT NULL, `name` varchar(8) NOT NULL, PRIMARY KEY (`id`)) ENGINE=MyISAM DEFAULT CHARSET=utf8 |+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec)root@localhost : wing 06:42:03&gt; show variables like &apos;sql_mode&apos;;+---------------+---------------------+| Variable_name | Value |+---------------+---------------------+| sql_mode | STRICT_TRANS_TABLES |+---------------+---------------------+1 row in set (0.00 sec)# 单值插入非法语句root@localhost : wing 06:23:56&gt; insert into innodb_test values(-9,&apos;ABCDEFGHIJKLMNOPQ&apos;);ERROR 1264 (22003): Out of range value for column &apos;id&apos; at row 1root@localhost : wing 06:24:22&gt; insert into myisam_test values(-9,&apos;ABCDEFGHIJKLMNOPQ&apos;);ERROR 1264 (22003): Out of range value for column &apos;id&apos; at row 1root@localhost : wing 06:24:30&gt; select * from innodb_test;Empty set (0.00 sec)root@localhost : wing 06:24:36&gt; select * from myisam_test;Empty set (0.00 sec)# 第一条记录非法的多值插入root@localhost : wing 06:28:03&gt; insert into innodb_test values(-1,&apos;A&apos;),(2,&apos;B&apos;),(3,&apos;C&apos;);ERROR 1264 (22003): Out of range value for column &apos;id&apos; at row 1root@localhost : wing 06:35:08&gt; insert into myisam_test values(-1,&apos;A&apos;),(2,&apos;B&apos;),(3,&apos;C&apos;);ERROR 1264 (22003): Out of range value for column &apos;id&apos; at row 1root@localhost : wing 06:35:14&gt; select * from innodb_test;Empty set (0.00 sec)root@localhost : wing 06:35:22&gt; select * from myisam_test; Empty set (0.00 sec)# 第一条记录不是非法的多值插入root@localhost : wing 06:35:26&gt; insert into innodb_test values(1,&apos;A&apos;),(2,&apos;B&apos;),(-3,&apos;C&apos;),(4,&apos;D&apos;);ERROR 1264 (22003): Out of range value for column &apos;id&apos; at row 3root@localhost : wing 06:36:28&gt; insert into myisam_test values(1,&apos;A&apos;),(2,&apos;B&apos;),(-3,&apos;C&apos;),(4,&apos;D&apos;);Query OK, 4 rows affected, 1 warning (0.00 sec)Records: 4 Duplicates: 0 Warnings: 1Warning (Code 1264): Out of range value for column &apos;id&apos; at row 3root@localhost : wing 06:36:35&gt; select * from innodb_test;Empty set (0.00 sec)root@localhost : wing 06:36:42&gt; select * from myisam_test; +----+------+| id | name |+----+------+| 1 | A || 2 | B || 0 | C || 4 | D |+----+------+4 rows in set (0.00 sec) TRADITIONAL sql_mode全员 ALLOW_INVALID_DATES对于日期只检查月份在1到12之间，日期在1到31之间。该sql_mode只适用于DATE和DATETIME类型，对于TIMESTAMP类型还是需要有效的日期。 # 只开启严格模式（不能插入非法日期）root@localhost : wing 06:56:03&gt; set sql_mode=&apos;STRICT_TRANS_TABLES&apos;; Query OK, 0 rows affected (0.00 sec)root@localhost : wing 06:56:08&gt; truncate table t;Query OK, 0 rows affected (2.64 sec)root@localhost : wing 06:56:14&gt; insert into t(d,dt) values(&apos;2015-4-31&apos;,&apos;2015-4-31 00:00:00&apos;);ERROR 1292 (22007): Incorrect date value: &apos;2015-4-31&apos; for column &apos;d&apos; at row 1Error (Code 1292): Incorrect date value: &apos;2015-4-31&apos; for column &apos;d&apos; at row 1Error (Code 1292): Incorrect datetime value: &apos;2015-4-31 00:00:00&apos; for column &apos;dt&apos; at row 1root@localhost : wing 06:56:18&gt; select * from t;Empty set (0.00 sec)# 不开启严格模式（非法日期转换）root@localhost : wing 06:56:23&gt; set sql_mode=&apos;&apos;;Query OK, 0 rows affected (0.00 sec)root@localhost : wing 06:56:31&gt; insert into t(d,dt) values(&apos;2015-4-31&apos;,&apos;2015-4-31 00:00:00&apos;);Query OK, 1 row affected, 2 warnings (0.01 sec)Warning (Code 1264): Out of range value for column &apos;d&apos; at row 1Warning (Code 1264): Out of range value for column &apos;dt&apos; at row 1root@localhost : wing 06:56:33&gt; select * from t;+------------+---------------------+---------------------+| d | dt | ts |+------------+---------------------+---------------------+| 0000-00-00 | 0000-00-00 00:00:00 | 2015-11-06 06:56:33 |+------------+---------------------+---------------------+1 row in set (0.00 sec)# 只开启ALLOW_INVALID_DATES（允许插入非法日期）root@localhost : wing 06:56:38&gt; set sql_mode=&apos;ALLOW_INVALID_DATES&apos;;Query OK, 0 rows affected (0.00 sec)root@localhost : wing 06:58:09&gt; truncate table t;Query OK, 0 rows affected (0.00 sec)root@localhost : wing 06:58:15&gt; insert into t(d,dt) values(&apos;2015-4-31&apos;,&apos;2015-4-31 00:00:00&apos;);Query OK, 1 row affected (0.00 sec)root@localhost : wing 06:58:17&gt; select * from t;+------------+---------------------+---------------------+| d | dt | ts |+------------+---------------------+---------------------+| 2015-04-31 | 2015-04-31 00:00:00 | 2015-11-06 06:58:17 |+------------+---------------------+---------------------+1 row in set (0.00 sec)# 即开启严格模式，又开启ALLOW_INVALID_DATES（允许插入非法日期）root@localhost : wing 06:58:21&gt; set sql_mode=&apos;STRICT_TRANS_TABLES,ALLOW_INVALID_DATES&apos;;Query OK, 0 rows affected (0.00 sec)root@localhost : wing 06:59:10&gt; truncate table t;Query OK, 0 rows affected (0.00 sec)root@localhost : wing 06:59:14&gt; insert into t(d,dt) values(&apos;2015-4-31&apos;,&apos;2015-4-31 00:00:00&apos;);Query OK, 1 row affected (0.00 sec)root@localhost : wing 06:59:16&gt; select * from t;+------------+---------------------+---------------------+| d | dt | ts |+------------+---------------------+---------------------+| 2015-04-31 | 2015-04-31 00:00:00 | 2015-11-06 06:59:16 |+------------+---------------------+---------------------+1 row in set (0.00 sec) ANSI_QUOTES未开启ANSI_QUOTES模式时，和&quot;&quot;是两种不同的标识符，&quot;&quot;与&#39;&#39;作用相同；开启ANSI_QUOTES模式时，和””是作用相同的标识符，此时””与’’作用不同。 # 未启用ANSI_QUOTES模式root@localhost : wing 12:54:34&gt; set sql_mode=&apos;&apos;;Query OK, 0 rows affected (0.00 sec)root@localhost : wing 12:56:54&gt; select * from t where ts=&apos;2015-11-06 06:59:16&apos;;+------------+---------------------+---------------------+| d | dt | ts |+------------+---------------------+---------------------+| 2015-04-31 | 2015-04-31 00:00:00 | 2015-11-06 06:59:16 |+------------+---------------------+---------------------+1 row in set (0.00 sec)root@localhost : wing 12:57:23&gt; select * from t where ts=`2015-11-06 06:59:16`; ERROR 1054 (42S22): Unknown column &apos;2015-11-06 06:59:16&apos; in &apos;where clause&apos;root@localhost : wing 12:57:34&gt; select * from t where ts=&quot;2015-11-06 06:59:16&quot;;+------------+---------------------+---------------------+| d | dt | ts |+------------+---------------------+---------------------+| 2015-04-31 | 2015-04-31 00:00:00 | 2015-11-06 06:59:16 |+------------+---------------------+---------------------+1 row in set (0.00 sec)# 启用ANSI_QUOTES模式root@localhost : wing 12:57:41&gt; set sql_mode=&apos;ANSI_QUOTES&apos;;Query OK, 0 rows affected (0.00 sec)root@localhost : wing 12:57:47&gt; select * from t where ts=&apos;2015-11-06 06:59:16&apos;;+------------+---------------------+---------------------+| d | dt | ts |+------------+---------------------+---------------------+| 2015-04-31 | 2015-04-31 00:00:00 | 2015-11-06 06:59:16 |+------------+---------------------+---------------------+1 row in set (0.00 sec)root@localhost : wing 12:57:57&gt; select * from t where ts=`2015-11-06 06:59:16`;ERROR 1054 (42S22): Unknown column &apos;2015-11-06 06:59:16&apos; in &apos;where clause&apos;root@localhost : wing 12:58:01&gt; select * from t where ts=&quot;2015-11-06 06:59:16&quot;;ERROR 1054 (42S22): Unknown column &apos;2015-11-06 06:59:16&apos; in &apos;where clause&apos; ERROR_FOR_DIVISION_BY_ZERO声明：该参数将会被弃用，对于这类模式均生成一个warning。未启用ERROR_FOR_DIVISION_BY_ZERO模式，对于使用被除数为0的值，会用NULL代替，但不会报错warning；启用ERROR_FOR_DIVISION_BY_ZERO模式将会在INSERT/UPDATE时，对于使用被除数为0的值，将会报出warning,并用NULL代替；启用ERROR_FOR_DIVISION_BY_ZERO+STRICT_TRANS_TABLES模式，在INSERT/UPDATE时，对于使用被除数为0的值，将会报错error,并不会INSERT或UPDATE任何值。 # 未启用ERROR_FOR_DIVISION_BY_ZERO模式root@localhost : wing 01:09:37&gt; set sql_mode=&apos;&apos;;Query OK, 0 rows affected (0.00 sec)root@localhost : wing 01:09:56&gt; insert into sql_mode_test values(mod(5,0));Query OK, 1 row affected (0.00 sec)root@localhost : wing 01:10:22&gt; select * from sql_mode_test;+------+| num |+------+| NULL |+------+1 row in set (0.00 sec)# 启用ERROR_FOR_DIVISION_BY_ZERO模式root@localhost : wing 01:10:32&gt; set sql_mode=&apos;ERROR_FOR_DIVISION_BY_ZERO&apos;;Query OK, 0 rows affected, 1 warning (0.00 sec)Warning (Code 1681): &apos;ERROR_FOR_DIVISION_BY_ZERO&apos; is deprecated and will be removed in a future release.root@localhost : wing 01:10:43&gt; insert into sql_mode_test values(mod(5,0));Query OK, 1 row affected, 1 warning (0.00 sec)Warning (Code 1365): Division by 0root@localhost : wing 01:10:49&gt; select * from sql_mode_test;+------+| num |+------+| NULL || NULL |+------+2 rows in set (0.00 sec)# 启用ERROR_FOR_DIVISION_BY_ZERO+STRICT_TRANS_TABLES模式root@localhost : wing 01:10:54&gt; set sql_mode=&apos;ERROR_FOR_DIVISION_BY_ZERO,STRICT_TRANS_TABLES&apos;;Query OK, 0 rows affected, 1 warning (0.00 sec)Warning (Code 1681): &apos;ERROR_FOR_DIVISION_BY_ZERO&apos; is deprecated and will be removed in a future release.root@localhost : wing 01:11:04&gt; insert into sql_mode_test values(mod(5,0));ERROR 1365 (22012): Division by 0root@localhost : wing 01:11:12&gt; select * from sql_mode_test;+------+| num |+------+| NULL || NULL |+------+2 rows in set (0.00 sec) HIGH_NOT_PRECEDENCE未开启HIGH_NOT_PRECEDENCE模式，NOT为最低优先级，例如对于NOT a BETWEEN b AND c的表达式可表示为：NOT （a BETWEEN b AND c）;开启HIGH_NOT_PRECEDENCE模式，NOT为最高优先级，例如对于NOT a BETWEEN b AND c的表达式可表示为：(NOT a) BETWEEN b AND c; # 未开启HIGH_NOT_PRECEDENCE模式root@localhost : wing 01:30:40&gt; set sql_mode=&apos;&apos;;Query OK, 0 rows affected (0.00 sec)root@localhost : wing 01:30:56&gt; SELECT NOT 1 BETWEEN -5 AND 5;+------------------------+| NOT 1 BETWEEN -5 AND 5 |+------------------------+| 0 |+------------------------+1 row in set (0.00 sec)# 开启HIGH_NOT_PRECEDENCE模式root@localhost : wing 01:30:58&gt; set sql_mode=&apos;HIGH_NOT_PRECEDENCE&apos;;Query OK, 0 rows affected (0.00 sec)root@localhost : wing 01:31:09&gt; SELECT NOT 1 BETWEEN -5 AND 5;+------------------------+| NOT 1 BETWEEN -5 AND 5 |+------------------------+| 1 |+------------------------+1 row in set (0.00 sec) IGNORE_SPACEMySQL默认是不忽略空格，’count’和’count ‘是不相同的，开启IGNORE_SPACE模式后，’count’和’count ‘是相同的。 # 未启用IGNORE_SPACE模式root@localhost : wing 02:03:57&gt; set sql_mode=&apos;&apos;;Query OK, 0 rows affected (0.00 sec)root@localhost : wing 02:04:14&gt; CREATE TABLE count(id int);ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &apos;count(id int)&apos; at line 1root@localhost : wing 02:04:17&gt; CREATE TABLE count (id int);Query OK, 0 rows affected (0.01 sec)root@localhost : wing 02:04:21&gt; drop table count;Query OK, 0 rows affected (0.10 sec)# 启用IGNORE_SPACE模式root@localhost : wing 02:04:38&gt; set sql_mode=&apos;IGNORE_SPACE&apos;;Query OK, 0 rows affected (0.00 sec)root@localhost : wing 02:04:54&gt; CREATE TABLE count(id int);ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &apos;count(id int)&apos; at line 1root@localhost : wing 02:04:59&gt; CREATE TABLE count (id int);ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &apos;count (id int)&apos; at line 1 NO_AUTO_CREATE_USER未开启NO_AUTO_CREATE_USER模式，对于没有使用IDENTIFIED BY/IDENTIFIED WITH的GRANT语句赋予权限时，若用户不存在，则MySQL会自动创建该用户；开启NO_AUTO_CREATE_USER模式，对于没有使用IDENTIFIED BY/IDENTIFIED WITH的GRANT语句赋予权限时，若用户不存在，则会报出Error错误。 # 未启用NO_AUTO_CREATE_USER模式root@localhost : wing 02:34:14&gt; set sql_mode=&apos;&apos;;Query OK, 0 rows affected (0.00 sec)root@localhost : wing 02:34:18&gt; show grants for &apos;wing&apos;@&apos;localhost&apos;;ERROR 1141 (42000): There is no such grant defined for user &apos;wing&apos; on host &apos;localhost&apos;root@localhost : wing 02:34:19&gt; grant select on *.* to &apos;wing_test&apos;@&apos;localhost&apos;;Query OK, 0 rows affected (0.00 sec)root@localhost : wing 02:34:31&gt; show grants for &apos;wing&apos;@&apos;localhost&apos;; ERROR 1141 (42000): There is no such grant defined for user &apos;wing&apos; on host &apos;localhost&apos;root@localhost : wing 02:34:32&gt; show grants for &apos;wing_test&apos;@&apos;localhost&apos;;+------------------------------------------------+| Grants for wing_test@localhost |+------------------------------------------------+| GRANT SELECT ON *.* TO &apos;wing_test&apos;@&apos;localhost&apos; |+------------------------------------------------+1 row in set (0.00 sec)# 启用NO_AUTO_CREATE_USER模式root@localhost : wing 02:34:53&gt; set sql_mode=&apos;NO_AUTO_CREATE_USER&apos;;Query OK, 0 rows affected (0.00 sec)root@localhost : wing 02:35:18&gt; show grants for &apos;wing&apos;@&apos;localhost&apos;;ERROR 1141 (42000): There is no such grant defined for user &apos;wing&apos; on host &apos;localhost&apos;root@localhost : wing 02:35:55&gt; grant select on *.* to &apos;wing&apos;@&apos;localhost&apos;; ERROR 1133 (42000): Can&apos;t find any matching row in the user tableroot@localhost : wing 02:36:06&gt; grant select on *.* to &apos;wing&apos;@&apos;localhost&apos; identified by &apos;123456&apos;;Query OK, 0 rows affected (0.00 sec)root@localhost : wing 02:36:25&gt; show grants for &apos;wing&apos;@&apos;localhost&apos;; +--------------------------------------------------------------------------------------------------------------+| Grants for wing@localhost |+--------------------------------------------------------------------------------------------------------------+| GRANT SELECT ON *.* TO &apos;wing&apos;@&apos;localhost&apos; IDENTIFIED BY PASSWORD &apos;*6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9&apos; |+--------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec) NO_AUTO_VALUE_ON_ZERO该参数对auto_increment列有影响，未启用NO_AUTO_VALUE_ON_ZERO时，当auto_increment列遇到0时，会自动插入当前自增列的最大值代替，启用NO_AUTO_VALUE_ON_ZERO时，当auto_increment列遇到0时，会在auto_increment列插入0。 # 未启用NO_AUTO_VALUE_ON_ZERO模式root@localhost : wing 06:27:42&gt; set sql_mode=&apos;&apos;;Query OK, 0 rows affected (0.00 sec)root@localhost : wing 06:27:55&gt; show create table sql_mode_test;+---------------+----------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+---------------+----------------------------------------------------------------------------------------------------------------------------------+| sql_mode_test | CREATE TABLE `sql_mode_test` ( `id` int(11) NOT NULL AUTO_INCREMENT, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 |+---------------+----------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec)root@localhost : wing 06:28:04&gt; select * from sql_mode_test;Empty set (0.00 sec)root@localhost : wing 06:28:12&gt; insert into sql_mode_test values(0);Query OK, 1 row affected (0.00 sec)root@localhost : wing 06:28:24&gt; select * from sql_mode_test;+----+| id |+----+| 1 |+----+1 row in set (0.00 sec)# 已启用NO_AUTO_VALUE_ON_ZERO模式root@localhost : wing 06:29:00&gt; set sql_mode=&apos;NO_AUTO_VALUE_ON_ZERO&apos;;Query OK, 0 rows affected (0.00 sec)root@localhost : wing 06:29:16&gt; show create table sql_mode_test;+---------------+----------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+---------------+----------------------------------------------------------------------------------------------------------------------------------+| sql_mode_test | CREATE TABLE `sql_mode_test` ( `id` int(11) NOT NULL AUTO_INCREMENT, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 |+---------------+----------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec)root@localhost : wing 06:29:32&gt; select * from sql_moed_test;ERROR 1146 (42S02): Table &apos;wing.sql_moed_test&apos; doesn&apos;t existroot@localhost : wing 06:29:39&gt; select * from sql_mode_test; Empty set (0.00 sec)root@localhost : wing 06:29:43&gt; insert into sql_mode_test values(0);Query OK, 1 row affected (0.00 sec)root@localhost : wing 06:29:55&gt; select * from sql_mode_test;+----+| id |+----+| 0 |+----+1 row in set (0.00 sec)# 此时再插入0时，会造成主键重复root@localhost : wing 06:34:32&gt; insert into sql_mode_test values(0);ERROR 1062 (23000): Duplicate entry &apos;0&apos; for key &apos;PRIMARY&apos; NO_BACKSLASH_ESCAPES不适用该参数’\’反斜杠符号表示转义符，启用该参数’\’反斜杠符号仅代表它自己 # 未启用NO_BACKSLASH_ESCAPES模式root@localhost : wing 06:36:42&gt; create table sql_mode_test(vc varchar(16));Query OK, 0 rows affected (1.84 sec)root@localhost : wing 06:34:35&gt; set sql_mode=&apos;&apos;;Query OK, 0 rows affected (0.00 sec)root@localhost : wing 06:37:01&gt; insert into sql_mode_test values(&apos;i\&apos;m&apos;);Query OK, 1 row affected (0.00 sec)root@localhost : wing 06:37:36&gt; select * from sql_mode_test;+------+| vc |+------+| i&apos;m |+------+1 row in set (0.00 sec)# 启用NO_BACKSLASH_ESCAPES模式root@localhost : wing 06:37:42&gt; set sql_mode=&apos;NO_BACKSLASH_ESCAPES&apos;;Query OK, 0 rows affected (0.00 sec)root@localhost : wing 06:37:59&gt; insert into sql_mode_test values(&apos;i\&apos;m&apos;); &apos;&gt; &apos; -&gt; \croot@localhost : wing 06:38:05&gt; insert into sql_mode_test values(&quot;i\&apos;m&quot;); Query OK, 1 row affected (0.00 sec)root@localhost : wing 06:38:15&gt; select * from sql_mode_test;+------+| vc |+------+| i&apos;m || i\&apos;m |+------+2 rows in set (0.00 sec) NO_DIR_IN_CREATE创建表时，忽视所有INDEX DIRECTORY和DATA DIRECTORY命令，该选项对slave较有用。 NO_ENGINE_SUBSTITUTION对于使用不支持的ENGINE(存储引擎)类型时，未启用NO_ENGINE_SUBSTITUTION模式时，CREATE TABLE或ALTER TABLE会成功，但使用的存储类型为默认类型（当前版本InnoDB）,报出warning,启用NO_ENGINE_SUBSTITUTION模式时，CREATE TABLE或ALTER TABLE时会报出error执行终止。 # 未启用NO_ENGINE_SUBSTITUTION模式root@localhost : wing 11:08:39&gt; show engines;+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+| Engine | Support | Comment | Transactions | XA | Savepoints |+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+| MyISAM | YES | MyISAM storage engine | NO | NO | NO || CSV | YES | CSV storage engine | NO | NO | NO || MRG_MYISAM | YES | Collection of identical MyISAM tables | NO | NO | NO || BLACKHOLE | NO | /dev/null storage engine (anything you write to it disappears) | NULL | NULL | NULL || MEMORY | YES | Hash based, stored in memory, useful for temporary tables | NO | NO | NO || InnoDB | DEFAULT | Supports transactions, row-level locking, and foreign keys | YES | YES | YES || ARCHIVE | YES | Archive storage engine | NO | NO | NO || FEDERATED | NO | Federated MySQL storage engine | NULL | NULL | NULL || PERFORMANCE_SCHEMA | YES | Performance Schema | NO | NO | NO |+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+9 rows in set (0.00 sec)root@localhost : wing 11:12:17&gt; set sql_mode=&apos;&apos;;Query OK, 0 rows affected (0.00 sec)root@localhost : wing 11:12:35&gt; create table sql_mode_test(id int) engine=BLACKHOLE ;Query OK, 0 rows affected, 2 warnings (0.09 sec)Warning (Code 1286): Unknown storage engine &apos;BLACKHOLE&apos;Warning (Code 1266): Using storage engine InnoDB for table &apos;sql_mode_test&apos;root@localhost : wing 11:12:36&gt; show create table sql_mode_test;+---------------+-------------------------------------------------------------------------------------------------+| Table | Create Table |+---------------+-------------------------------------------------------------------------------------------------+| sql_mode_test | CREATE TABLE `sql_mode_test` ( `id` int(11) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8 |+---------------+-------------------------------------------------------------------------------------------------+1 row in set (0.00 sec)# 启用NO_ENGINE_SUBSTITUTION模式root@localhost : wing 11:12:45&gt; set sql_mode=&apos;NO_ENGINE_SUBSTITUTION&apos;;Query OK, 0 rows affected (0.00 sec)root@localhost : wing 11:13:10&gt; drop table sql_mode_test;Query OK, 0 rows affected (0.10 sec)root@localhost : wing 11:13:17&gt; create table sql_mode_test(id int) engine=BLACKHOLE ;ERROR 1286 (42000): Unknown storage engine &apos;BLACKHOLE&apos;root@localhost : wing 11:13:25&gt; show create table sql_mode_test;ERROR 1146 (42S02): Table &apos;wing.sql_mode_test&apos; doesn&apos;t exist NO_FIELD_OPTIONS、NO_KEY_OPTIONS、NO_TABLE_OPTIONS用于mysqldump的兼容性，实际操作未实现。 NO_UNSIGNED_SUBTRACTION未启用NO_UNSIGNED_SUBTRACTION模式，无符号类型的数值减法后还是无符号类型，对于负数将直接报错，启用NO_UNSIGNED_SUBTRACTION模式，无符号类型的数值减法可以是负数，而不会报错。 # 未启用NO_UNSIGNED_SUBTRACTION模式root@localhost : wing 12:29:46&gt; set sql_mode=&apos;&apos;;Query OK, 0 rows affected (0.00 sec)root@localhost : wing 12:30:26&gt; select cast(3 as unsigned) - 4;ERROR 1690 (22003): BIGINT UNSIGNED value is out of range in &apos;(cast(3 as unsigned) - 4)&apos;# 启用NO_UNSIGNED_SUBTRACTION模式root@localhost : wing 12:30:29&gt; set sql_mode=&apos;NO_UNSIGNED_SUBTRACTION&apos;;Query OK, 0 rows affected (0.00 sec)root@localhost : wing 12:30:44&gt; select cast(3 as unsigned) - 4;+-------------------------+| cast(3 as unsigned) - 4 |+-------------------------+| -1 |+-------------------------+1 row in set (0.00 sec) NO_ZERO_DATE声明：该参数将会被弃用，对于这类模式均生成一个warning。对于插入’0000-00-00 00:00:00’这种类型包含0月份或者0日期的日期，未启用NO_ZERO_DATE模式时，可以插入并且无任何warning,启用NO_ZERO_DATE时，可以插入但是会有warning,启用NO_ZERO_DATE+STRICT_TRANS_TABLES模式，无法插入且报出error,只启用STRICT_TRANS_TABLES模式，可以插入并且无任何warning。 # 未启用NO_ZERO_DATE模式root@localhost : wing 01:19:31&gt; set sql_mode=&apos;&apos;;Query OK, 0 rows affected (0.00 sec)root@localhost : wing 01:19:39&gt; insert into sql_mode_test values(&apos;0000-00-00 00:00:00&apos;);Query OK, 1 row affected (1.13 sec)root@localhost : wing 01:20:07&gt; select * from sql_mode_test;+---------------------+| ts |+---------------------+| 0000-00-00 00:00:00 |+---------------------+1 row in set (0.00 sec)# 启用NO_ZERO_DATE模式root@localhost : wing 01:20:16&gt; set sql_mode=&apos;NO_ZERO_DATE&apos;;Query OK, 0 rows affected, 1 warning (0.00 sec)Warning (Code 1681): &apos;NO_ZERO_DATE&apos; is deprecated and will be removed in a future release.root@localhost : wing 01:20:37&gt; insert into sql_mode_test values(&apos;0000-00-00 00:00:00&apos;);Query OK, 1 row affected, 1 warning (0.00 sec)Warning (Code 1264): Out of range value for column &apos;ts&apos; at row 1root@localhost : wing 01:20:42&gt; select * from sql_mode_test;+---------------------+| ts |+---------------------+| 0000-00-00 00:00:00 || 0000-00-00 00:00:00 |+---------------------+2 rows in set (0.00 sec)# 启用NO_ZERO_DATE和STRICT_TRANS_TABLES模式root@localhost : wing 01:20:51&gt; set sql_mode=&apos;NO_ZERO_DATE,STRICT_TRANS_TABLES&apos;;Query OK, 0 rows affected, 1 warning (0.00 sec)Warning (Code 1681): &apos;NO_ZERO_DATE&apos; is deprecated and will be removed in a future release.root@localhost : wing 01:21:29&gt; root@localhost : wing 01:21:30&gt; insert into sql_mode_test values(&apos;0000-00-00 00:00:00&apos;);ERROR 1292 (22007): Incorrect datetime value: &apos;0000-00-00 00:00:00&apos; for column &apos;ts&apos; at row 1root@localhost : wing 01:21:37&gt; select * from sql_mode_test;+---------------------+| ts |+---------------------+| 0000-00-00 00:00:00 || 0000-00-00 00:00:00 |+---------------------+2 rows in set (0.00 sec)# 只启用STRICT_TRANS_TABLES模式root@localhost : wing 01:21:42&gt; set sql_mode=&apos;STRICT_TRANS_TABLES&apos;;Query OK, 0 rows affected (0.00 sec)root@localhost : wing 01:32:37&gt; insert into sql_mode_test values(&apos;0000-00-00 00:00:00&apos;);Query OK, 1 row affected (0.00 sec)root@localhost : wing 01:32:42&gt; select * from sql_mode_test;+---------------------+| ts |+---------------------+| 0000-00-00 00:00:00 || 0000-00-00 00:00:00 || 0000-00-00 00:00:00 |+---------------------+3 rows in set (0.00 sec) NO_ZERO_IN_DATE声明：该参数将会被弃用，对于这类模式均生成一个warning。与NO_ZERO_DATE模式相差不大。 ONLY_FULL_GROUP_BY未启ONLY_FULL_GROUP_BY模式时，group by跟数据表中的任何字段均可，启用ONLY_FULL_GROUP_BY模式时，select的字段必须出现在group by中，group by字段可以不出现在select字段中。 root@localhost : wing 02:42:49&gt; show create table sql_mode_test;+---------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+---------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+| sql_mode_test | CREATE TABLE `sql_mode_test` ( `id` int(11) DEFAULT NULL, `name` varchar(16) DEFAULT NULL, `score` int(11) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8 |+---------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec)# 未启用ONLY_FULL_GROUP_BY模式root@localhost : wing 02:42:57&gt; set sql_mode=&apos;&apos;;Query OK, 0 rows affected (0.00 sec)root@localhost : wing 02:43:03&gt; select id from sql_mode_test group by name;Empty set (0.00 sec)# 启用ONLY_FULL_GROUP_BY模式root@localhost : wing 02:43:18&gt; set sql_mode=&apos;ONLY_FULL_GROUP_BY&apos;;Query OK, 0 rows affected (0.00 sec)root@localhost : wing 02:43:36&gt; select id from sql_mode_test group by name;ERROR 1055 (42000): &apos;wing.sql_mode_test.id&apos; isn&apos;t in GROUP BYroot@localhost : wing 02:43:37&gt; select id from sql_mode_test group by id,name;Empty set (0.00 sec) PAD_CHAR_TO_FULL_LENGTH未启用PAD_CHAR_TO_FULL_LENGTH模式，对于CHAR类型会截断尾部空格，启用PAD_CHAR_TO_FULL_LENGTH模式，对于CHAR类型不会截断尾部空格。该模式对VARCHAR类型不起作用。 root@localhost : wing 02:55:07&gt; create table sql_mode_test(name char(10));Query OK, 0 rows affected (1.39 sec)root@localhost : wing 02:58:30&gt; insert into sql_mode_test values(&apos;wing&apos;);Query OK, 1 row affected (0.00 sec)# 未启用PAD_CHAR_TO_FULL_LENGTH模式root@localhost : wing 02:59:18&gt; set sql_mode=&apos;&apos;;Query OK, 0 rows affected (0.00 sec)root@localhost : wing 02:59:50&gt; select name,char_length(name) from sql_mode_test; +------+-------------------+| name | char_length(name) |+------+-------------------+| wing | 4 |+------+-------------------+1 row in set (0.00 sec)启用PAD_CHAR_TO_FULL_LENGTH模式root@localhost : wing 02:59:55&gt; set sql_mode=&apos;PAD_CHAR_TO_FULL_LENGTH&apos;;Query OK, 0 rows affected (0.00 sec)root@localhost : wing 03:00:09&gt; select name,char_length(name) from sql_mode_test;+------------+-------------------+| name | char_length(name) |+------------+-------------------+| wing | 10 |+------------+-------------------+1 row in set (0.00 sec) PIPES_AS_CONCAT未启用PIPES_AS_CONCAT模式，将’||’视为’或’的功能，启用PIPES_AS_CONCAT模式，将’||’视为CONCAT()函数。 root@localhost : wing 03:05:46&gt; show create table sql_mode_test\G*************************** 1. row *************************** Table: sql_mode_testCreate Table: CREATE TABLE `sql_mode_test` ( `name` char(10) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf81 row in set (0.00 sec)# 未启用PIPES_AS_CONCAT模式root@localhost : wing 03:00:11&gt; set sql_mode=&apos;&apos;;Query OK, 0 rows affected (0.00 sec)root@localhost : wing 03:03:47&gt; select concat(name,&apos; hello&apos;) from sql_mode_test;+-----------------------+| concat(name,&apos; hello&apos;) |+-----------------------+| wing hello |+-----------------------+1 row in set (0.00 sec)root@localhost : wing 03:04:19&gt; select name||&apos; hello&apos; from sql_mode_test;+----------------+| name||&apos; hello&apos; |+----------------+| 0 |+----------------+1 row in set, 2 warnings (0.00 sec)Warning (Code 1292): Truncated incorrect DOUBLE value: &apos;wing &apos;Warning (Code 1292): Truncated incorrect DOUBLE value: &apos; hello&apos;# 启用PIPES_AS_CONCAT模式root@localhost : wing 03:05:12&gt; set sql_mode=&apos;PIPES_AS_CONCAT&apos;; Query OK, 0 rows affected (0.00 sec)root@localhost : wing 03:05:18&gt; select concat(name,&apos; hello&apos;) from sql_mode_test;+-----------------------+| concat(name,&apos; hello&apos;) |+-----------------------+| wing hello |+-----------------------+1 row in set (0.00 sec)root@localhost : wing 03:05:21&gt; select name||&apos; hello&apos; from sql_mode_test;+----------------+| name||&apos; hello&apos; |+----------------+| wing hello |+----------------+1 row in set (0.00 sec) REAL_AS_FLOAT未启用REAL_AS_FLOAT模式时，REAL类型相当于DOUBLE类型，启用REAL_AS_FLOAT模式时，REAL类型相当于FLOAT类型。 # 未启用REAL_AS_FLOAT模式root@localhost : wing 07:02:50&gt; set sql_mode=&apos;&apos;;Query OK, 0 rows affected (0.00 sec)root@localhost : wing 07:03:13&gt; create table sql_mode_test(salary real);Query OK, 0 rows affected (0.01 sec)root@localhost : wing 07:06:06&gt; show create table sql_mode_test;+---------------+----------------------------------------------------------------------------------------------------+| Table | Create Table |+---------------+----------------------------------------------------------------------------------------------------+| sql_mode_test | CREATE TABLE `sql_mode_test` ( `salary` double DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8 |+---------------+----------------------------------------------------------------------------------------------------+1 row in set (0.00 sec)# 启用REAL_AS_FLOAT模式root@localhost : wing 07:06:17&gt; set sql_mode=&apos;REAL_AS_FLOAT&apos;;Query OK, 0 rows affected (0.00 sec)root@localhost : wing 07:06:34&gt; show create table sql_mode_test;+---------------+----------------------------------------------------------------------------------------------------+| Table | Create Table |+---------------+----------------------------------------------------------------------------------------------------+| sql_mode_test | CREATE TABLE `sql_mode_test` ( `salary` double DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8 |+---------------+----------------------------------------------------------------------------------------------------+1 row in set (0.00 sec)root@localhost : wing 07:06:36&gt; drop table sql_mode_test;Query OK, 0 rows affected (0.09 sec)root@localhost : wing 07:06:48&gt; create table sql_mode_test(salary real);Query OK, 0 rows affected (0.10 sec)root@localhost : wing 07:06:57&gt; show create table sql_mode_test;+---------------+---------------------------------------------------------------------------------------------------+| Table | Create Table |+---------------+---------------------------------------------------------------------------------------------------+| sql_mode_test | CREATE TABLE `sql_mode_test` ( `salary` float DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8 |+---------------+---------------------------------------------------------------------------------------------------+1 row in set (0.00 sec) STRICT_ALL_TABLES对所有的表都启用严格模式。 STRICT_TRANS_TABLES对事务型的表启用严格模式。 组合的sql_mode ANSIREAL_AS_FLOAT + PIPES_AS_CONCAT + ANSI_QUOTES + IGNORE_SPACE DB2PIPES_AS_CONCAT + ANSI_QUOTES + IGNORE_SPACE + NO_KEY_OPTIONS + NO_TABLE_OPTIONS + NO_FIELD_OPTIONS MAXDBPIPES_AS_CONCAT + ANSI_QUOTES + IGNORE_SPACE + NO_KEY_OPTIONS + NO_TABLE_OPTIONS + NO_FIELD_OPTIONS + NO_AUTO_CREATE_USER MSSQLPIPES_AS_CONCAT + ANSI_QUOTES + IGNORE_SPACE + NO_KEY_OPTIONS + NO_TABLE_OPTIONS + NO_FIELD_OPTIONS MYSQL323MYSQL323 + HIGH_NOT_PRECEDENCE MYSQL40MYSQL40 + HIGH_NOT_PRECEDENCE ORACLEPIPES_AS_CONCAT + ANSI_QUOTES + IGNORE_SPACE + NO_KEY_OPTIONS + NO_TABLE_OPTIONS +NO_FIELD_OPTIONS, NO_AUTO_CREATE_USER POSTGRESQLPIPES_AS_CONCAT + ANSI_QUOTES + IGNORE_SPACE + NO_KEY_OPTIONS + NO_TABLE_OPTIONS, + NO_FIELD_OPTIONS TRADITIONALSTRICT_TRANS_TABLES + STRICT_ALL_TABLES + NO_ZERO_IN_DATE + NO_ZERO_DATE + ERROR_FOR_DIVISION_BY_ZERO + NO_AUTO_CREATE_USER + NO_ENGINE_SUBSTITUTION 严格的sql_mode模式严格处理MySQL的INSERT/UPDATE的无效的值。对于查询，无效的值将会返回一个warning,而不是error。严格模式包括：STRICT_ALL_TABLES/STRICT_TRANS_TABLES。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Nginx安装模块]]></title>
      <url>%2F2015%2F10%2F30%2FNginx%E5%AE%89%E8%A3%85%E6%A8%A1%E5%9D%97%2F</url>
      <content type="text"><![CDATA[记一次Nginx出现的错误，登录浏览器，出现如下错误：SSL received a record that exceeded the maximum permissible length. 出现问题如上，所以开始google造成这个问题的原因，然后google告诉我需要安装http_ssl_module模块，可是作为Nginx小白的我连模块都不会安装，于是继续google，找到了如何安装模块，就是需要重新编译。。 # http_ssl_module模块需要以来于OpenSSL,所以找到OpenSSL的压缩包，然后解压到OpenSSL目录下即可。./configure --with-http_ssl_module --with-openssl=/usr/local/openssl/openssl-1.0.2dmakemake install# 通过-V参数可看到安装了相应的模块[root@wing nginx]# ./sbin/nginx -Vnginx version: nginx/1.8.0built by gcc 4.8.3 20140911 (Red Hat 4.8.3-9) (GCC) built with OpenSSL 1.0.2d 9 Jul 2015TLS SNI support enabledconfigure arguments: --with-http_ssl_module --with-openssl=/usr/local/openssl/openssl-1.0.2d 于是问题解决了。。。。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[mysql.event中诡异的execute_at&last_executed]]></title>
      <url>%2F2015%2F10%2F30%2Fmysql-event%E4%B8%AD%E8%AF%A1%E5%BC%82%E7%9A%84execute-at-last-executed%2F</url>
      <content type="text"><![CDATA[你发现了吗？对于不是UTC时区的MySQL，mysql.event与information_schema.events中execute_at、last_executed、starts、ends的时间是不一样的。你以为是Bug吗？NONONO,这才不是一个Bug呢。这是MySQL的精心设计之处。 问题发现一次意外的发现创建完Event之后，information_schema.events和mysql.event表中execute_at和last_executed字段均相差8个小时，mysqld实例的时区为： 测试过程 创建测试数据库 创建测试数据表 创建Event 执行SQL语句查询mysqld实例的时区和时间 执行SQL语句查询information_schema.events表和mysql.event表中execute_at和last_executed字段值 问题原因为了确保Event的执行不受时区的影响，使得Event可以准确执行，MySQL将mysql.event表的事件调度时间（execute_at和last_executed）转换成UTC时间。 文档描述：Times in the ON SCHEDULE clause are interpreted using the current session time_zone value. This becomes the event time zone; that is, the time zone that is used for event scheduling and is in effect within the event as it executes. These times are converted to UTC and stored along with the event time zone in the mysql.event table. This enables event execution to proceed as defined regardless of any subsequent changes to the server time zone or daylight saving time effects. 文档链接：http://dev.mysql.com/doc/refman/5.6/en/create-event.html 注意mysql.event表中除了execute_at和last_execute诡异外，starts和ends也是同样原因引起的诡异。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux系统命令之screen命令]]></title>
      <url>%2F2015%2F10%2F29%2FLinux%E7%B3%BB%E7%BB%9F%E5%91%BD%E4%BB%A4%E4%B9%8Bscreen%E5%91%BD%E4%BB%A4%2F</url>
      <content type="text"><![CDATA[对于执行长时间命令（如大文件运输）而言，为了不让它终止，都会开启一个远程登陆会话窗口来单独运行这样的命令。在此期间，不能关闭远程登录会话窗口或者断开连接，否则前功尽弃。这时候screen命令就派上用场了。screen就是可以在同一个远程登陆会话窗口中创建多个screen会话窗口，当会话暂时可以不需要的时候，可以放至后台运行，虽然LINUX下提供&amp;命令将程序放置后台运行，但是对于人机交互的任务（比如在MySQL中运行一个长任务），还是screen比较胜任一点. 下面来介绍下我最常用的screen命令（仅限于我）： 首先新建screen会话窗口，可以在这个窗口中执行任何命令，如同在远程登录会话窗口下一样。[root@wing ~]# screen 接下来我想登陆我的MySQL进行人机交互，当然是在screen会话窗口中。[root@wing ~]# mysql -uroot -p --socket=/data/mysqldata3306/sock/mysql.sock 然后在我的MySQL中执行如下命令，customer表中有150万条数据，执行起来需要一点时间（对于我这种烂虚拟机来说而已），所以我现在又不想克隆一个远程登陆会话窗口（太懒），所以我要将当前的这个screen会话抛到后台运行。root@localhost : tcph 12:37:39&gt; SELECT * FROM customer; 下面这个命令就是将screen会话抛到后台了，当然MySQL里面的任务还在运行。Ctrl+A D（按下ctrl+A ,松开再按下D） 然而现在我又想去看看我的语句跑完没（我是个麻烦的人），执行命令如下：screen -ls 此时就可以查看刚刚丢到后台的任务了，见下图： 既然找到了任务，也看到了任务在screen里面的代号（红框前部分的数字）了，那赶紧运行吧。[root@wing ~]# screen -r 6158 这时候我们就能看到了前面在MySQL中的交互，此时发现我任务跑完了。如果我还是按Ctrl+A D的话，它还是会存在于screen -ls的任务清单中，作为强迫症的我，怎么能忍受=_=，那么怎么办呢？Ctrl+A K 这样子就可以杀掉这个任务了，并且在screen -ls的任务清单中也就不存在了。 更深入的命令带我研究研究再来。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Nginx的配置文件]]></title>
      <url>%2F2015%2F10%2F27%2FNginx%E7%9A%84%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%2F</url>
      <content type="text"><![CDATA[前面写了一个Nginx的quickstart,现在来江江Nginx的配置文件nginx.conf文件，一个程序没了配置文件是多么坑爹的事情，还好Nginx是个友好的程序。 配置文件结构 配置文件中的指令包括简单指令和指令块简单指令： # 名称 参数；worker_processes 1; 指令块：# 有&#123;&#125;的一系列简单指令 location / &#123; root /usr/local/nginx/html/host1; index index.html index.htm; &#125; events和http在main指令块内，server在http指令块内，location在server指令块内 #是注释符号]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[MySQL5.6之XA事务]]></title>
      <url>%2F2015%2F10%2F23%2FMySQL5-6%E4%B9%8BXA%E4%BA%8B%E5%8A%A1%2F</url>
      <content type="text"><![CDATA[MySQL5.6新特性之XA事务，XA事务也即是分布式事务。 XA事务SQL语句XA &#123;START|BEGIN&#125; xid [JOIN|RESUME]XA END xid [SUSPEND [FOR MIGRATE]]XA PREPARE xidXA COMMIT xid [ONE PHASE]XA ROLLBACK xidXA RECOVER 特别说明 对于XA START语句,JOIN/RESUME语句不支持;对于XA END语句,SUSPEND [FOR MIGRATE]语句并不支持; 每一个XA语句都是以XA开头的; xid的值是有客户提供的或者有MySQL服务器生成; xid的组成如下 xid: gtrid [, bqual [, formatID ]] gtrid是全局事务ID,64字节的字符串;bqual是分支(branch)的修饰符,64字节的字符串;formatID是用于标识gtrid和bqual的格式,无符号的整数数值;xid是由transaction manager管理;每一个XA事务都必须拥有其唯一的xid值; XA事务state XA事务的处理过程如下1) 使用XA START开启一个xa事务,并将其置为active状态;2) 对于active状态的xa事务,处理完事务内的SQL语句后,使用XA END语句,XA END语句将事务置为IDLE状态;3) 对于IDLE状态的xa事务,可以使用XA PREPARE或XA XOMMIT … ONE PHASE语句;XA PREPARE将xa事务置于prepared状态;XA RECOVER将prepare状态的xa事务全部列出来;XA COMMIT … ONE PHASE准备并提交事务,此时的xid值将不会出现在XA RECOVER语句中;4) 对于prepared状态的XA事务,可以使用XA COMMIT或者XA ROLLBACK来提交或回滚XA事务。实战演练 #开启新的xa事务,将xa事务置于active状态root@localhost : wing 02:34:24&gt; xa start 'xatest';Query OK, 0 rows affected (0.00 sec)root@localhost : wing 02:34:42&gt; insert into xa_test values (1);Query OK, 1 row affected (0.00 sec)# 结束xa事务,将xa事务置于idle状态,此时事务并未提交root@localhost : wing 02:34:48&gt; xa end 'xatest';Query OK, 0 rows affected (0.00 sec)root@localhost : wing 02:35:06&gt; xa recover 'xatest';ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ''xatest'' at line 1#由于xa事务没有置于prepare状态,故xa recover没有显示结果root@localhost : wing 02:35:26&gt; xa recover;Empty set (0.00 sec)#将xa事务置于prepared状态root@localhost : wing 02:35:55&gt; xa prepare 'xatest';Query OK, 0 rows affected (0.00 sec)#xa事务进入prepared状态之后,可以在xa recover中查看到prepare的xa事务root@localhost : wing 02:35:59&gt; xa recover;+----------+--------------+--------------+--------+| formatID | gtrid_length | bqual_length | data |+----------+--------------+--------------+--------+| 1 | 6 | 0 | xatest |+----------+--------------+--------------+--------+1 row in set (0.00 sec)#提交xa事务,此时可开启另一个会话,提交xa事务,事务只有在进入prepared状态以后才可以提交root@localhost : wing 02:36:05&gt; xa commit 'xatest';Query OK, 0 rows affected (0.00 sec) 同一个会话中,不允许并行开启两个XA事务,除非前一个xa事务必须提交,常规的START TRANSACTION也一样 root@localhost : wing 02:48:58&gt; xa start 'xa1';Query OK, 0 rows affected (0.00 sec)root@localhost : wing 02:51:51&gt; xa start 'xa2';ERROR 1399 (XAE07): XAER_RMFAIL: The command cannot be executed when global transaction is in the ACTIVE stateroot@localhost : wing 02:53:49&gt; xa end 'xa1';Query OK, 0 rows affected (0.00 sec)root@localhost : wing 02:54:01&gt; xa start 'xa2';ERROR 1399 (XAE07): XAER_RMFAIL: The command cannot be executed when global transaction is in the IDLE stateroot@localhost : wing 02:54:05&gt; xa prepare 'xa1';Query OK, 0 rows affected (0.00 sec)root@localhost : wing 02:54:11&gt; xa start 'xa2';ERROR 1399 (XAE07): XAER_RMFAIL: The command cannot be executed when global transaction is in the PREPARED stateroot@localhost : wing 02:54:15&gt; xa commit 'xa1';Query OK, 0 rows affected (0.00 sec)# 直到'xa1'提交后,方可创建新的xa事务'xa2'root@localhost : wing 02:54:24&gt; xa start 'xa2';Query OK, 0 rows affected (0.00 sec) 当事务处于active时,不能发出任何导致隐式提交的语句。(此处隐式提交的语句与常规事务的隐式提交语句一致) XA事务的限制 XA事务仅适用于Innodb存储引擎; XA START语句不支持JOIN/RESUME语句; XA END语句不支持SUSPEND [FOR MIGRATE]语句; 当XA事务在prepared阶段时,此时MySQL Server宕机，该事物可以在服务器重启后继续执行，然而客户端重连后，即使事务已经提交，但是它并不会记录到binlog中，这将导致主从复制不一致。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[无效的grant语句导致主从复制断开]]></title>
      <url>%2F2015%2F10%2F21%2F%E6%97%A0%E6%95%88%E7%9A%84grant%E8%AF%AD%E5%8F%A5%E5%AF%BC%E8%87%B4%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E6%96%AD%E5%BC%80%2F</url>
      <content type="text"><![CDATA[执行无效的grant语句，会导致主从复制断开以及主库二进制日志重新创建，从库报错：The incident LOST_EVENTS occured on the master. Message: error writing to the binary log，以及二进制日志会有如下记录：# Incident: LOST_EVENTSRELOAD DATABASE; # Shall generate syntax error #。 场景重现测试版本：MySQL 5.6.11首先主从复制正常：主库：执行grant file on test.* to test@127.0.0.1语句从库： 查看从库状态show slave status\G 查看二进制日志（注意：此时查看的二进制日志是show master status的上一个日志，因为在无效的grant语句执行后，主库会创建新的二进制日志），会有如下记录 错误原因分析因为file权限不能只赋予某个库，需要赋予所有库，即grant file on . to user@host才是有效的grant语句。 解决方法 使用sql_slave_skip_counter跳过事件，但此方法只适用于基于二进制日志原理的复制，不适用于基于GTID原理的复制。 使用slave_skip_errors跳过错误。 在从库上做change master操作，重新切换master_log_file和master_log_pos。（由于无效的grant语句执行后会创建新的二进制日志，所以可以指定主库show master status的master_log_file和master_log_pos） 规避方案避免无效的grant语句的执行。例如，file,process,super权限都要赋予所有数据库上，而不是赋予某个库或者某几个库。 Bug地址MySQL Bug#68892：http://bugs.mysql.com/bug.php?id=68892]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[MySQL5.6之特别的RAND()函数]]></title>
      <url>%2F2015%2F10%2F20%2FMySQL5-6%E4%B9%8B%E7%89%B9%E5%88%AB%E7%9A%84RAND-%E5%87%BD%E6%95%B0%2F</url>
      <content type="text"><![CDATA[RAND()函数特别之处在于它在主从复制下是安全的函数，即使是基于binlog_format=STATEMENT模式下也不会导致主从复制数据不一致。 约定1.在没有特殊说明的情况下，默认binlog_format=STATEMENT,由于binlog_format=ROW/MIXED模式下，RAND()函数以行格式记录，所以此处不做讨论。2.测试表结构如下： CREATE TABLE `t` ( `id` decimal(5,3) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8 测试过程1.RAND()函数不在同一个事务中① 测试原理：在autocommit=ON的情况下，多次执行INSERT INTO t VALUES(RAND());语句② binlog_format=STATEMENT模式下的。二进制日志记录如下：③ 主从复制的数据MASTER:SLAVE:2.RAND()函数在同一个事务中① 测试原理：在START TRANSACTION;和COMMIT;之间执行多次INSERT INTO t VALUES(RAND());语句② binlog_format=STATEMENT模式下的。二进制日志记录如下：③ 主从复制的数据MASTER:SLAVE: 结论由测试过程可得,不论是同一个事务中还是非同一个事务中,包含RAND()函数的SQL语句之前都会记录两个会话级的参数RAND_SEED1和RAND_SEED2，由这两个参数根据RAND()产生随机数的算法便可得到一个确定的数值，所以即使在binlog_format=STATEMENT模式下，主从复制之间使用RAND()函数可以确保数据一致。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Nginx的quickstart]]></title>
      <url>%2F2015%2F10%2F18%2FNginx%E7%9A%84quickstart%2F</url>
      <content type="text"><![CDATA[众所周知，IT这个行业什么都要懂一点才能成长起来，嗯，所以我就学学Nginx了，主要的目标还是通过Nginx实现方向代理。最直观的需求就是，我可以通过电脑（Windows）的浏览器，敲入虚拟机(Linux)地址，从而访问该虚拟机内容。下面仅介绍如何在Linux上安装Nginx,并学会Nginx的简单使用。 Nginx的安装 通过Nginx官网下载想要的版本（以nginx1.8.0为例）http://nginx.org/ 解压Nginx安装包tar -zxvf nginx-1.8.0.tar.gz 进入解压后的Nginx目录cd nginx-1.8.0 安装编译相关的工具yum -y install gcc gcc-c++ autoconf automake 安装需要的依赖包 yum -y install pcre pcre-devel如不安装该依赖包，会报如下错误：./configure: error: the HTTP rewrite module requires the PCRE library.You can either disable the module by using –without-http_rewrite_moduleoption, or install the PCRE library into the system, or build the PCRE librarystatically from the source with nginx by using –with-pcre= option. yum install -y zlib zlib-devel如不安装该依赖包，会报如下错误：./configure: error: the HTTP rewrite module requires the zlib library.You can either disable the module by using –without-http_rewrite_moduleoption, or install the PCRE library into the system, or build the PCRE librarystatically from the source with nginx by using –with-pcre= option. 进行编译安装./configuremakemake install至此Nginx安装完成，进入/usr/local可查看是否存在nginx目录来确定Nginx是否安装成功。 Nginx的启动 启动命令如下 /usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf# Nginx执行命令 -c Nginx配置文件#可通过如下命令查看Nginx是否启动[root@mysql local]# ps -ef |grep nginxroot 31672 1 0 21:03 ? 00:00:00 nginx: master process /usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.confnobody 31673 31672 0 21:03 ? 00:00:00 nginx: worker process root 31675 26425 0 21:04 pts/2 00:00:00 grep nginx#如上为Nginx的启动成功的显示结果 控制一旦Nginx启动后，可以使用-s参数来控制Nginx的可执行文件。 nginx -s signal# signal可为以下值stop:快速关闭quit:缓慢关闭reload:重新加载配置文件reopen:重新打开日志文件 Nginx的停止 缓慢停止kill -QUIT Nginx主进程号 快速停止kill -TERM Nginx主进程号或者kill -INT Nginx主进程号 强制停止pkill -9 nginx #缓慢停止的实例[root@mysql local]# ps -ef | grep nginxroot 31679 1 0 21:10 ? 00:00:00 nginx: master process /usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.confnobody 31680 31679 0 21:10 ? 00:00:00 nginx: worker process root 31694 26425 0 21:12 pts/2 00:00:00 grep nginx[root@mysql local]# kill -QUIT 31679[root@mysql local]# ps -ef | grep nginxroot 31696 26425 0 21:12 pts/2 00:00:00 grep nginx Nginx验证配置文件正确性 方法一 [root@mysql local]# cd /usr/local/nginx/sbin/# 使用-t参数，可看到ok,即配置文件是正确的[root@mysql sbin]# ./nginx -tnginx: the configuration file /usr/local/nginx/conf/nginx.conf syntax is oknginx: configuration file /usr/local/nginx/conf/nginx.conf test is successful 方法二 # 注意此处-t参数需要在-c参数前面[root@mysql sbin]# /usr/local/nginx/sbin/nginx -t -c /usr/local/nginx/conf/nginx.confnginx: the configuration file /usr/local/nginx/conf/nginx.conf syntax is oknginx: configuration file /usr/local/nginx/conf/nginx.conf test is successful Nginx的重启 方法一 [root@mysql sbin]# cd /usr/local/nginx/sbin/[root@mysql sbin]# ./nginx -s reload 方法二 [root@mysql sbin]# ps -ef | grep nginxroot 31711 1 0 21:20 ? 00:00:00 nginx: master process /usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.confnobody 31716 31711 0 21:21 ? 00:00:00 nginx: worker process root 31718 26425 0 21:22 pts/2 00:00:00 grep nginx# 通过发送信号量的方式，kill -HUP Nginx主进程号[root@mysql sbin]# kill -HUP 31711 说明若想通过浏览器访问Nginx,需要考虑将端口添加到防火墙或者关闭防火墙。 关闭防火墙 centos7之前service iptables stop centos7之后service firewalld stop或者systemctl stop firewalld.service禁止firewalld开机自启systemctl disable firewalld.service]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[MySQL5.6新特性Index conditontion pushdow]]></title>
      <url>%2F2015%2F10%2F18%2FMySQL5-6%E6%96%B0%E7%89%B9%E6%80%A7Index-conditontion-pushdow%2F</url>
      <content type="text"><![CDATA[index condition pushdown是MySQL5.6的新特性，主要是对MySQL索引使用的优化。 Index condition push简称ICP，索引条件下推，将索引条件从server层下推到storage engine层。此时出现ICP现象，但ICP到底是个什么现象呢。1、ICP的开启和关闭方法：set optimizer_switch=’index_condition_pushdown=on|off’2、ICP使用限制：ICP适用于MyISAM和InnoDB表，对于InnoDB表只能用于二级索引。MySQL5.6版本的分区表还没有ICP功能，官网文档介绍将于5.7版本中解决。3、ICP的目的在于减少完整读取一条记录的次数，从而减少IO的操作。4、当优化器没有使用ICP的时候，数据的访问和提取：当storage engine层读取记录的时候，首先使用索引定位并读取整条记录，server层再根据where条件决定该条记录是否使用。当优化器使用ICP的时候，数据的访问和提取：当storage engine层读取记录的时候，首先测试where条件中索引列（由server层下推到storage engine层的索引条件）决定该条记录是否可用，如果可用，则索引定位的记录完整读取出来，让server层根据where条件决定该条记录是否使用。 5、实例演示：首先有如下表结构： CREATE TABLE `icp_test` ( `id` int(11) NOT NULL AUTO_INCREMENT, `code` int(11) DEFAULT NULL, `name` varchar(16) DEFAULT NULL, `address` varchar(16) DEFAULT NULL, `age` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `idx_c_n_a` (`code`,`name`,`address`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 分析如下操作： select * from icp_test where code=18 and name like '%A%' ;select * from icp_test where code=18 and name like '%A%' ; 做如下分析：优化器在没有ICP的条件下：select * from icp_test where code=18 and name like ‘%A%’ ;优化器在有ICP的条件下： 分析：在没有ICP的条件下，虽然有idx_c_n_a(code,name.address)索引，但由于where条件中name like ‘%A%’不使用索引，所以storage engine层只能使用code=18过滤得到4条记录返回到server层，此时server层根据name like ‘%A%’得到两条符合条件的记录返回客户端。 在有ICP的条件下，有idx_c_n_a(code,name,address)索引，where条件中name like ‘%A%’不使用索引，所以storage engine层只能使用code=18过滤得到4条记录，但此时不立即返回server层，由于MySQL5.6版本的索引条件下推（ICP）的特性，stora engine层还会根据索引中的name列name like ‘%A%’过滤得到2条记录返回客户端，此时没有非索引列的where条件，最终返回客户端。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[基于CentOS6.5安装Inception]]></title>
      <url>%2F2015%2F10%2F17%2F%E5%9F%BA%E4%BA%8ECentOS6-5%E5%AE%89%E8%A3%85Inception%2F</url>
      <content type="text"><![CDATA[Inception是去哪儿网开源的一款SQL审核工具，功能丰富，便于使用。Inception官方文档：http://mysql-inception.github.io/inception-document/由于官方文档中没有对该工具的安装写详细教程，故自己写一个便于不了解编译安装的人参考。 inception编译安装1. 至github中获取inception源码 github地址：https://github.com/mysql-inception/inception 最简便的方法是点击该页面的Dowload ZIP,然后在CentOS安装unzip工具，以下安装过程均基于获得源码的zip压缩包。 2. 安装unzip工具 yum install -y unzip3. 解压inception源码包 unzip inception-master.zip -d /4. 安装相关依赖包和编译工具 yum install -y openssl-develyum install -y gcc-c++yum install -y ncurses-develyum install -y bisonyum install -y cmake5. 编译安装 sh inception_build.sh inception [linux]#上述命令中inception为编译安装的目录，[linux]为当前操作系统平台6. 此时等待一大堆密密麻麻的英文数字跑完以后，亲测是完成安装了。 关于运行和使用，请移步inception官方文档。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux如何将eth1改为eth0]]></title>
      <url>%2F2015%2F10%2F16%2FLinux%E5%A6%82%E4%BD%95%E5%B0%86eth1%E6%94%B9%E4%B8%BAeth0%2F</url>
      <content type="text"><![CDATA[在工作中，新开启一个Linux服务器，发现我居然找不到eth0,使用ifconfig居然没有发现我认识的eth0，然后通过查询资料，找到原因及解决方法，故记录之。 现象ifconfig时，并未发现和ethN相关的信息，ifconfig -a时，发现eth1,并未发现eth0。 原因很多Linux distribution使用udev动态管理设备文件，并根据设备的信息对其进行持久化命名。udev会在系统引导的过程中识别网卡，将mac地址和网卡名称对应起来记录在udev的规则脚本中。而对于新的虚拟机，VMware会自动为虚拟机的网卡生成MAC地址，当你克隆或者重装虚拟机软件时，由于你使用的是以前系统虚拟硬盘的信息，而该系统中已经有eth0的信息，对于这个新的网卡，udev会自动将其命名为eth1（累加的原则），所以在你的系统启动后，你使用ifconfig看到的网卡名为eth1。（摘自参考链接） 解决方案vi /etc/udev/rules.d/70-persistent-net.rules注释掉其中的eth0行，将原来的eth1名称修改为eth0 参考链接http://www.cnblogs.com/swblog/p/3251935.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LOCK TABLES & TRIGGER那些事]]></title>
      <url>%2F2015%2F10%2F15%2FLOCK-TABLES-TRIGGER%E9%82%A3%E4%BA%9B%E4%BA%8B%2F</url>
      <content type="text"><![CDATA[LOCK TABLES和TRIGGER之间会产生哪些奇妙的锁呢？ 测试用例 创建测试相关的表与触发器 root@localhost : sp_test_db 02:55:50&gt; create table t1(id int);Query OK, 0 rows affected (0.04 sec)root@localhost : sp_test_db 02:56:50&gt; create table t2(id int);Query OK, 0 rows affected (0.02 sec)root@localhost : sp_test_db 02:56:53&gt; create table t3(id int);Query OK, 0 rows affected (0.04 sec)root@localhost : sp_test_db 02:56:56&gt; create table t4(id int);DELIMITER //CREATE TRIGGER t1_after_ins AFTER INSERT ON t1 FOR EACH ROWBEGINUPDATE t4 SET id=id+1WHERE id=NEW.id AND EXISTS (SELECT id FROM t3);INSERT INTO t2 VALUES(100);END//DELIMITER ; 执行LOCK TABLES 语句 LOCK TABLES t1 WRITE, t2 READ; 测试相关表的锁情况，为了测试方便，将参数innodb_lock_wait_timeout设置为2 root@localhost : sp_test_db 03:18:00&gt; set session innodb_lock_wait_timeout=2;Query OK, 0 rows affected (0.02 sec)root@localhost : sp_test_db 03:18:29&gt; select * from t1;ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transactionroot@localhost : sp_test_db 03:18:38&gt; select * from t2;ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transactionroot@localhost : sp_test_db 03:18:47&gt; select * from t3;Empty set (0.00 sec)root@localhost : sp_test_db 03:19:19&gt; insert into t3 values(1);ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transactionroot@localhost : sp_test_db 03:19:36&gt; select * from t4;ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction 分析过程 t1表持有写锁，因为LOCK TABLES显示指定; t2表持有写锁，虽然LOCK TABLES显示指定读锁，但是触发器中需要做INSERT操作; t3表持有读锁，因为触发器中只需要做SELECT操作; t4表持有写锁，因为触发器中需要UPDATE操作。 结论如果显示指定了LOCK TABLES,那么与它相关的触发器中的所有的表都会被隐式的指定LOCK TABLE。 使用LOCK TABLES指定的表被相应的锁锁住,如果在触发器中没有该表相关的语句需要更高范围的锁; 触发器里的表持有的锁取决于其SQL语句,如果只需要读,则持有读锁,否则为写锁; 如果一个表被LOCK TABLES显示指定读锁,但在触发器中需要写锁,那么该表将持有写锁。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[glob路径下查找文件]]></title>
      <url>%2F2015%2F10%2F14%2Fglob%E8%B7%AF%E5%BE%84%E4%B8%8B%E6%9F%A5%E6%89%BE%E6%96%87%E4%BB%B6%2F</url>
      <content type="text"><![CDATA[Python中通过输入路径和通配符查找到相关的文件或目录的库。 通配符* ： 匹配无限个字符? ： 匹配单个字符[] ： 匹配指定范围内的数字 glob.glob返回的是一个列表，所有在路径下匹配的文件或目录。 # 返回/root目录下所有以MySQL-开头，以.rpm结尾的文件或目录&gt;&gt;&gt; import glob&gt;&gt;&gt; glob.glob('/root/MySQL-*.rpm')['/root/MySQL-client-5.6.26-1.el6.x86_64.rpm', '/root/MySQL-devel-5.6.26-1.el6.x86_64.rpm', '/root/MySQL-server-5.6.26-1.el6.x86_64.rpm', '/root/MySQL-shared-5.6.26-1.el6.x86_64.rpm', '/root/MySQL-shared-compat-5.6.26-1.el6.x86_64.rpm']# 验证glob.glob返回的是列表list&gt;&gt;&gt; type(glob.glob('/root/MySQL-*.rpm'))&lt;type 'list'&gt; glob.iglob功能与glob.glob相似，但glob.glob输出为列表，而glob.iglob为各个输出。 &gt;&gt;&gt; import glob&gt;&gt;&gt; f = glob.iglob('/root/MySQL-*.rpm')&gt;&gt;&gt; f&lt;generator object iglob at 0x7f7206ab63c0&gt;&gt;&gt;&gt; for i in f:... print i... /root/MySQL-client-5.6.26-1.el6.x86_64.rpm/root/MySQL-devel-5.6.26-1.el6.x86_64.rpm/root/MySQL-server-5.6.26-1.el6.x86_64.rpm/root/MySQL-shared-5.6.26-1.el6.x86_64.rpm/root/MySQL-shared-compat-5.6.26-1.el6.x86_64.rpm]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[oak-online-alter-table工具]]></title>
      <url>%2F2015%2F10%2F13%2Foak-online-alter-table%E5%B7%A5%E5%85%B7%2F</url>
      <content type="text"><![CDATA[oak-online-alter-table作为一款实现非阻塞的在线ALTER TABLE操作的工具，与online DDL的区别在于其最后不需要应用online log，故最后一部分不需要进行锁表。 使用限制 表至少含有一个单列的唯一索引; 修改的表和源表之间共享单列的唯一索引; 表上没有‘AFTER’触发器(因为该工具会在执行过程中创建AFTER触发器); 表没有外键; 表名不超过57个字符。 程序运行时,源表上只能有INSERT/UPDATE/DELETE/REPLACE操作,不允许源表上有TRUNCATE/ALTER/REPAIR/OPTIMIZER等操作,且运行的表必须有表级别锁。 基本功能 非阻塞的ALTER TABLE操作存在限制如下：ADD COLUMN：新列必须有一个默认值;DROP COLUMN：源表和修改的表必须有共享的单列唯一索引;MODIFY COLUMN：无;ADD KEY：无;DROP KEY：源表和修改的表必须有共享的单列唯一索引;CHANGE ENGINE：无。 空的ALTER操作,将会重建表重组数据并释放无用的磁盘空间。 创建一个源表的镜像表,可以在数据库中看到。限制：ALTER TABLE不操作在源表上;源表上不能有TRUNCATE操作;源表上没有load data infile操作;源表上没有optimize操作; 原理通过在oak-online-alter-table工具运行期间创建源表的镜像表,而后与源表进行缓慢的同步,当同步完成后,该镜像表就取代了源表。镜像表取代源表的过程中,存在一个将源表数据复制到镜像表的过程,该过程中需要将源表的数据放进chunk中,将源表数据读入到chunck中,对于MyISAM表需要表级读锁,对于InnoDB表需要对读取到chunk的数据上加上行级读锁,所以此处需要锁。chunk的大小可以有参数-c设置;chunk较小时读锁持有时间较小且允许更多的并发,但数据复制过程可能需要更久;chunk较大时,读锁持有时间较长且允许并发更少,但数据复制过程时间短。 工具选项-h,–help 显示help页面-u,–user MySQL连接用户-H,–host MySQL连接主机(默认localhost)–ask-pass MySQL连接密码-P,–port MySQL连接端口,默认为3306端口-S,–socket MySQL连接socket文件–defaults-file 指定MySQL配置文件-d,–database 指定需要做online-alter-table对应的数据库-t,–table 指定需要做online-alter-table对应的表-g,–ghost 显示创建一个镜像表,与源表保持同步-a,–alter alter-table的语句,该工具自身会执行alter table table_name,故alter-table语句只需alter table table_name后的语句-l,–lock-chunks 每个chunk被复制之前使用lock tables语句-N,–skip-binlog 不记录到binlog中-r,–max-lock-retries 当出现死锁或者超过lock_wait_timeout时,重试的次数,默认为10,0代表无穷大–skip-delete-pass 不执行DELETE语句–sleep 两个chunk复制数据之间的间隔时间,单位为：微秒,默认为0–sleep-ratio 睡眠时间与执行时间的比例,睡眠时间将与每部分的执行时间成比例,而不是一成不变的–sleep时间,默认为0–cleanup 删除自定义的触发器,如果程序意外终止,但没有该选项,那么没有删除的自定义触发器并不会得到删除。-v,–verbose 输出更详细的信息-q,–quiet 输出简要信息 实战演练测试添加普通索引以及删除索引的过程。 # 源表表结构root@localhost : wing 11:19:23&gt; show create table oak;+-------+--------------------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+-------+--------------------------------------------------------------------------------------------------------------------------------------------------------+| oak | CREATE TABLE `oak` ( `id` int(11) NOT NULL DEFAULT '0', `name` varchar(16) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 |+-------+--------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec)# oak-online-alter-table添加普通索引执行语句[root@oracle01 ~]# oak-online-alter-table --user=root --socket=/data/mysqldata3306/sock/mysql.sock --database=wing --table=oak --alter='add key idx_test(name)';# oak-online-alter-table添加普通索引输出信息-- Connecting to MySQL-- Table wing.oak is of engine innodb-- Checking for UNIQUE columns on wing.oak, by which to chunk-- Possible UNIQUE KEY column names in wing.oak:-- - id-- Table wing.__oak_oak has been created-- Table wing.__oak_oak has been altered-- Checking for UNIQUE columns on wing.__oak_oak, by which to chunk-- Possible UNIQUE KEY column names in wing.__oak_oak:-- - id-- Checking for UNIQUE columns on wing.oak, by which to chunk-- - Found following possible unique keys:-- - id (int)-- Chosen unique key is 'id'-- Shared columns: id, name-- Created AD trigger-- Created AU trigger-- Created AI trigger-- Attempting to lock tables-- Tables locked WRITE-- id (min, max) values: ([1L], [3L])-- Tables unlocked-- - Reminder: altering wing.oak: add key idx_test(name)...-- Copying range (1), (3), progress: 0%-- Copying range 100% complete. Number of rows: 3-- - Reminder: altering wing.oak: add key idx_test(name)...-- Deleting range (1), (3), progress: 0%-- Deleting range 100% complete. Number of rows: 0-- Table wing.oak has been renamed to wing.__arc_oak,-- and table wing.__oak_oak has been renamed to wing.oak-- Table wing.__arc_oak was found and dropped-- ALTER TABLE completed# oak-online-alter-table添加普通索引成功root@localhost : wing 11:26:22&gt; show create table oak;+-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| oak | CREATE TABLE `oak` ( `id` int(11) NOT NULL DEFAULT '0', `name` varchar(16) DEFAULT NULL, PRIMARY KEY (`id`), KEY `idx_test` (`name`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 |+-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec)# oak-online-alter-table删除普通索引执行语句[root@oracle01 ~]# oak-online-alter-table --user=root --socket=/data/mysqldata3306/sock/mysql.sock --database=wing --table=oak --alter='drop key idx_test'# oak-online-alter-table删除普通索引输出信息-- Connecting to MySQL-- Table wing.oak is of engine innodb-- Checking for UNIQUE columns on wing.oak, by which to chunk-- Possible UNIQUE KEY column names in wing.oak:-- - id-- Table wing.__oak_oak has been created-- Table wing.__oak_oak has been altered-- Checking for UNIQUE columns on wing.__oak_oak, by which to chunk-- Possible UNIQUE KEY column names in wing.__oak_oak:-- - id-- Checking for UNIQUE columns on wing.oak, by which to chunk-- - Found following possible unique keys:-- - id (int)-- Chosen unique key is 'id'-- Shared columns: id, name-- Created AD trigger-- Created AU trigger-- Created AI trigger-- Attempting to lock tables-- Tables locked WRITE-- id (min, max) values: ([1L], [3L])-- Tables unlocked-- - Reminder: altering wing.oak: drop key idx_test...-- Copying range (1), (3), progress: 0%-- Copying range 100% complete. Number of rows: 3-- - Reminder: altering wing.oak: drop key idx_test...-- Deleting range (1), (3), progress: 0%-- Deleting range 100% complete. Number of rows: 0-- Table wing.oak has been renamed to wing.__arc_oak,-- and table wing.__oak_oak has been renamed to wing.oak-- Table wing.__arc_oak was found and dropped-- ALTER TABLE completed# oak-online-alter-table删除普通索引成功root@localhost : wing 11:28:48&gt; show create table oak;+-------+--------------------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+-------+--------------------------------------------------------------------------------------------------------------------------------------------------------+| oak | CREATE TABLE `oak` ( `id` int(11) NOT NULL DEFAULT '0', `name` varchar(16) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 |+-------+--------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec) 特别说明oak-online-alter-table工具与online DDL的区别：online DDL在操作执行完毕后,需要一次锁表将online log应用到当前表中,而oak-online-alter-table则在操作执行完毕即可,无需再次锁表。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openark-kit安装]]></title>
      <url>%2F2015%2F10%2F12%2Fopenark-kit%E5%AE%89%E8%A3%85%2F</url>
      <content type="text"><![CDATA[作为MySQL另一个工具集openark-kit，内部包含很多小工具，会在将来一一介绍~ 官方网站 http://code.openark.org/forge/openark-kit 本文以CentOS为操作系统，且默认操作系统中已经安装Python环境。 安装Python模块包之MySQL-python,用于使用Python连接操作MySQL使用。yum install -y MySQL-python 安装openark-kit工具包1) RPM安装方式获得RPM包 https://code.google.com/p/openarkkit/downloads/detail?name=openark-kit-196-1.noarch.rpm执行命令 rpm -ivh openark-kit-196-1.noarch.rpm2) tar包安装方式获取tar包 https://code.google.com/p/openarkkit/downloads/detail?name=openark-kit-196.tar.gz解压tar包 tar -zxvf openark-kit-196.tar.gz -C /usr/local/openark-kit/安装openark-kit工具 python setup.py install 特别说明openark-kit工具的前缀为oak-,相关工具有如下，详细介绍后续将会使用并记录。oak-apply-rioak-get-slave-lagoak-modify-charsetoak-purge-master-logsoak-show-limitsoak-block-accountoak-hook-general-logoak-online-alter-tableoak-repeat-queryoak-show-replication-statusoak-chunk-updateoak-kill-slow-queriesoak-prepare-shutdownoak-security-audit]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[mysqldump备份工具]]></title>
      <url>%2F2015%2F10%2F09%2Fmysqldump%E5%A4%87%E4%BB%BD%E5%B7%A5%E5%85%B7%2F</url>
      <content type="text"><![CDATA[作为一名DBA，备份是件很重要的工作，如果备份没有做好，那就大事不好啦。。mysqldump作为MySQL自带一款备份工具，还是大有用途的。经常与mysqldump工具做比较的另一款Percona Xtrabackup，详情请见：http://wing324.github.io/tags/Percona-Xtrabackup/ 简介mysqldump是一款逻辑备份工具，针对原库中的定义和数据产生一系列可执行的SQL语句。 权限 select:备份表需要的权限 show view:备份视图需要的权限 trigger:备份触发器需要的权限 lock tables:如果没有使用–single-transaction时需要的权限 导入表时，需要有相应的权限执行SQL语句，例如CREATE权限等 其他权限视选项而定注意mysqldump中可能会含有ALTER DATABASE语句,这种情况是发生在使用–routines选项备份存储过程时（此处存储过程一定要存在才会发生这种现象），在存储过程写下后，又更改了数据库的字符集，此时才会发现mysqldump中会包含ALTER DATABASE语句,此处在导入表时，需要有ALTER 权限。 # 原来数据库字符集为utf8root@localhost : wing 10:44:01&gt; show create database wing;+-----------+--------------------------------------------------------------------+| Database | Create Database |+-----------+--------------------------------------------------------------------+| wing | CREATE DATABASE `wing` /*!40100 DEFAULT CHARACTER SET utf8 */ |+-----------+--------------------------------------------------------------------+#内部包含存储过程，此时执行如下命令[root@mysql tmpdir]# mysqldump -uroot --socket=/data/mysqldata3306/sock/mysql.sock -p --routines --single-transaction wing &gt; dumpfile.sql[root@mysql tmpdir]# less dumpfile.sql此时会发现如下这段信息，此处不存在ALTER DATABASES语句：---- Dumping routines for database &apos;wing&apos;--/*!50003 DROP PROCEDURE IF EXISTS `sp_in` */;/*!50003 SET @saved_cs_client = @@character_set_client */ ;/*!50003 SET @saved_cs_results = @@character_set_results */ ;/*!50003 SET @saved_col_connection = @@collation_connection */ ;/*!50003 SET character_set_client = utf8 */ ;/*!50003 SET character_set_results = utf8 */ ;/*!50003 SET collation_connection = utf8_general_ci */ ;/*!50003 SET @saved_sql_mode = @@sql_mode */ ;/*!50003 SET sql_mode = &apos;STRICT_TRANS_TABLES,NO_ENGINE_SUBSTITUTION&apos; */ ;DELIMITER ;;CREATE DEFINER=`root`@`localhost` PROCEDURE `sp_in`(in id int)BEGINSELECT id AS id_init;IF (id is not null) THENset id = id + 1;END IF;SELECT id AS id_out;END ;;DELIMITER ; # 此时更改数据库字符集root@localhost : wing 11:03:20&gt; alter database wing character set = latin1;[root@mysql tmpdir]# mysqldump -uroot --socket=/data/mysqldata3306/sock/mysql.sock -p --routines --single-transaction wing &gt; dumpfile1.sql[root@mysql tmpdir]# less dumpfile1.sql此时会发现如下这段信息，此处存在ALTER DATABASES语句：---- Dumping routines for database &apos;wing&apos;--/*!50003 DROP PROCEDURE IF EXISTS `sp_in` */;ALTER DATABASE `wing` CHARACTER SET utf8 COLLATE utf8_general_ci ;/*!50003 SET @saved_cs_client = @@character_set_client */ ;/*!50003 SET @saved_cs_results = @@character_set_results */ ;/*!50003 SET @saved_col_connection = @@collation_connection */ ;/*!50003 SET character_set_client = utf8 */ ;/*!50003 SET character_set_results = utf8 */ ;/*!50003 SET collation_connection = utf8_general_ci */ ;/*!50003 SET @saved_sql_mode = @@sql_mode */ ;/*!50003 SET sql_mode = &apos;STRICT_TRANS_TABLES,NO_ENGINE_SUBSTITUTION&apos; */ ;DELIMITER ;;CREATE DEFINER=`root`@`localhost` PROCEDURE `sp_in`(in id int)BEGINSELECT id AS id_init;IF (id is not null) THENset id = id + 1;END IF;SELECT id AS id_out;END ;;DELIMITER ; 语法mysqldump [options] db_name [tbl_name ...]mysqldump [options] --databases db_name ...mysqldump [options] --all-databases 参数详解此处仅列出个人认为还是比较重要的参数，毕竟参数选项太多了，更多的参数可通过mysqldump –help查看。 连接参数 –compress,-C压缩所有的信息在服务器和客户端之间传输 –host.-h连接MySQL服务器的主机名 –login-path连接MySQL服务器的一种方式，详情请见：http://wing324.github.io/2015/09/28/mysql-config-editor%E5%B7%A5%E5%85%B7/ –password,-p连接MySQL服务器的密码 –port，-P连接MySQL服务器的端口 –socket,-S连接MySQL服务器的socket文件 –user,-u连接MySQL服务器的用户名 –max_allowed_packet服务器/客户端之间传输包的最大数量 –net_buffer_length用于建立连接时的连接缓冲和结果缓冲 设置文件参数 –print-defaults输出mysqldump默认选项 # 只是一个例子啦[root@mysql log]# mysqldump --login-path=root3306 --print-defaultsmysqldump would have been started with the following arguments:--quick --max_allowed_packet=2G --default-character-set=utf8 --user=root --password=root --socket=/data/mysqldata3306/sock/mysql.sock –defaults-file指定配置文件 DDL参数 –add-drop-database在每个CREATE DATABASE前面添加DROP DATABASE语句 –add-drop-table在每个CREATE TABLE前面添加DROP TABLE语句 –add-drop-trigger在每个CREATE TRIGGER前面添加DROP TRIGGER语句 –all-tablespaces导出所有的表空间，不会有表空间的相关信息出现在mysqldump的输出文件中，该参数只适用于MySQL集群表 –no-create-db,-n使用–databases或者–all-databases不使用CREATE DATABASE语句 # 不添加-n参数可发现存在CREATE DATABASE语句[root@mysql tmpdir]# mysqldump --login-path=root3306 --single-transaction --databases test &gt; t1.sql[root@mysql tmpdir]# less t1.sql-- -------------------------------------------------------- Server version 5.6.26-log/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;/*!40101 SET NAMES utf8 */;/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;/*!40103 SET TIME_ZONE=&apos;+00:00&apos; */;/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE=&apos;NO_AUTO_VALUE_ON_ZERO&apos; */;/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;---- Current Database: `test`--CREATE DATABASE /*!32312 IF NOT EXISTS*/ `test` /*!40100 DEFAULT CHARACTER SET utf8 */;USE `test`;---- Table structure for table `t1`--# 添加-n则不存在CREATE DATABASE语句[root@mysql tmpdir]# mysqldump --login-path=root3306 --single-transaction -n --databases test &gt; t2.sql[root@mysql tmpdir]# less t2.sql-- -------------------------------------------------------- Server version 5.6.26-log/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;/*!40101 SET NAMES utf8 */;/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;/*!40103 SET TIME_ZONE=&apos;+00:00&apos; */;/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE=&apos;NO_AUTO_VALUE_ON_ZERO&apos; */;/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;---- Current Database: `test`--USE `test`;---- Table structure for table `t1`-- –no-create-info,-t在备份表时，不写CREATE TABLE语句，即该参数不导出表结构，只导出数据 –no-tablespances,-ymysqldump输出文件中不包含CREATE LOGFILE GROUP/CREATE TABLESPACE语句 –replace使用REPLACE语句代替INSERT语句 # 未使用--replacemysqldump --login-path=root3306 --single-transaction --databases test &gt; t2.sql[root@mysql tmpdir]# less t2.sql -- 部分输出如下LOCK TABLES `t1` WRITE;/*!40000 ALTER TABLE `t1` DISABLE KEYS */;INSERT INTO `t1` VALUES (1,&apos;FMmer58&apos;,15),(2,&apos;INLGVwn&apos;,19),(3,&apos;AVJTHCw&apos;,31),(4,&apos;WsA2Oae&apos;,39),(5,&apos;gTnLLnW&apos;,99),(6,&apos;95D4ooj&apos;,12),(7,&apos;ZnyNFxz&apos;,59),(8,&apos;8ajAUfF&apos;,24),(9,&apos;hsAx4YD&apos;,69),(10,&apos;huG3rP1&apos;,18);/*!40000 ALTER TABLE `t1` ENABLE KEYS */;UNLOCK TABLES;#使用--replace[root@mysql tmpdir]# mysqldump --login-path=root3306 --single-transaction --replace --databases test &gt; t3.sql[root@mysql tmpdir]# less t3.sql-- 部分输出如下LOCK TABLES `t1` WRITE;/*!40000 ALTER TABLE `t1` DISABLE KEYS */;REPLACE INTO `t1` VALUES (1,&apos;FMmer58&apos;,15),(2,&apos;INLGVwn&apos;,19),(3,&apos;AVJTHCw&apos;,31),(4,&apos;WsA2Oae&apos;,39),(5,&apos;gTnLLnW&apos;,99),(6,&apos;95D4ooj&apos;,12),(7,&apos;ZnyNFxz&apos;,59),(8,&apos;8ajAUfF&apos;,24),(9,&apos;hsAx4YD&apos;,69),(10,&apos;huG3rP1&apos;,18);/*!40000 ALTER TABLE `t1` ENABLE KEYS */;UNLOCK TABLES;# 观察发现REPLACE语句取代了INSERT语句 –allow-keywords允许列名为关键字 –comment,-i附加注释信息 –force,-f即使在备份表时出现SQL错误，依旧继续备份。即使一个view的基表被删除，那么该view将会无效，但如果使用–force，会出现报错信息，但mysqldump以及继续工作。 –log-error错误信息和警告信息都将追加到该文件中 –skip-comments不备份注释信息 复制选项 –apply-slave-statements使用–dump-slave选项，在CHANGE MASTER TO之前添加STOP SLAVE,在备份文件末尾添加START SLAVE –delete-master-logs该选项会在备份时，像服务器发送PURGE BINARY LOGS语句删除二进制日志，该选项会自动开启–master-data选项。 –dump-slave=0/1/2与参数–master-slave相似，用来备份一个slave机器的数据文件可以直接用于建立另一个slave作为备份文件的master的从机。使用该参数的备份文件中存在master机器的CHANGE MASTER TO的MASTER_LOG_FILE和MASTER_LOG_POS参数，用于为该master新建一个slave机器使用。当–dump-slave和–master-data一起使用时，–master-data参数会被忽略。值为0：并没有CHANGE MASTER TO的信息； # --dump-slave=0/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;---- Table structure for table `company`-- 值为1：存在CHANGE MASTER TO的信息，并且为可执行的；--dump-slave=1---- Position to start replication or point-in-time recovery from (the master of this slave)--CHANGE MASTER TO MASTER_LOG_FILE=&apos;mysql-bin.000003&apos;, MASTER_LOG_POS=70606941;---- Table structure for table `company`-- 值为2：存在CHANGE MASTER TO的信息，以注释方式出现。# --dump-slave=2---- Position to start replication or point-in-time recovery from (the master of this slave)---- CHANGE MASTER TO MASTER_LOG_FILE=&apos;mysql-bin.000003&apos;, MASTER_LOG_POS=70607833;---- Table structure for table `company`-- 注意# 使用--dump-slave参数，会输出备份机器的master机器的如下CHANGE MASTER TO信息，注意该信息并非是备份机器的信息，而是备份机器的master机器信息。---- Position to start replication or point-in-time recovery from (the master of this slave)--CHANGE MASTER TO MASTER_LOG_FILE=&apos;mysql-bin.000003&apos;, MASTER_LOG_POS=70575052; –include-master-host-port对于–dump-slave参数产生的CHANGE MASTER TO语句，会增加MASTER_HOST和MASTER_PORT参数。拓展如果master机器已经存在一个或多个slave,其实想给master机器增加更多的slave,那么要做的事情就是对其slave从机使用–dump-slave做备份文件。 # 使用命令mysqldump -uroot --socket=/data/mysql/mysqldata3306/sock/mysql.sock --single-transaction --dump-slave=1 --apply-slave-statements --include-master-host-port -p --all-databases &gt; 备份文件名称Enter password: # 和复制相关信息---- stop slave statement to make a recovery dump)--STOP SLAVE;---- Position to start replication or point-in-time recovery from (the master of this slave)--CHANGE MASTER TO MASTER_HOST=&apos;192.168.200.158&apos;, MASTER_PORT=&apos;3306&apos;, MASTER_LOG_FILE=&apos;mysql-bin.000003&apos;, MASTER_LOG_POS=70729145;---- Table structure for table `company`--...（备份数据完毕后）---- start slave statement to make a recovery dump)--START SLAVE;/*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */;/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;-- Dump completed on 2015-10-24 10:41:21 –master-data该参数用于master机器的备份文件用于设置另一个机器为该master机器的slave从机，它会产生备份机器的CHANGE MASTER TO的MASTER_LOG_FILE和MASTER_LOG_POS参数。值为0：备份文件中并不会存在CHANGE MASTER TO的参数信息； # --master-data=0/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;---- Table structure for table `company`-- 值为1：CHANGE MASTER TO产生的参数以语句的形式出现在备份文件中，导入文件时起作用；# --master-data=1---- Position to start replication or point-in-time recovery from--CHANGE MASTER TO MASTER_LOG_FILE=&apos;mysql-bin.000007&apos;, MASTER_LOG_POS=74942041;---- Table structure for table `company`-- 值为2：CHANGE MASTER TO产生的参数以注释形式出现在备份文件中，导入文件时并不起作用。# --master-data=2---- Position to start replication or point-in-time recovery from---- CHANGE MASTER TO MASTER_LOG_FILE=&apos;mysql-bin.000007&apos;, MASTER_LOG_POS=74942749;---- Table structure for table `company`-- 注意 该参数的开启需要RELOAD权限，且需要开启二进制日志； 该参数会自动关闭–lock-tables,在没有使用参数–single-transaction时也会自动开启–lock-all-tables。 –set-gtid-purged=OFF/ON/AUTO该参数表示是否在备份文件中添加SET @@global.gtid_purged。值为OFF:不添加SET @@global.gtid_purged至备份文件中；值为ON：添加SET @@global.gtid_purged至备份文件中，但需要开启GTID参数，否则会报错；值为AUTO:如果服务器开启了GTID，则添加SET @@global.gtid_purged至备份文件中，否则不添加。 格式参数 –compact开启–skip-add-drop-table,–skip-add-locks,–skip-comments,–skip-disable-keys,–skip-set-charset参数，以达到压缩的目的。 # 使用--compact[root@host-192-168-200-154 tmpdir]# du -h t0.sql 4.0K t0.sql# 其备份文件如下[root@host-192-168-200-154 tmpdir]# less t0.sql /*!40101 SET @saved_cs_client = @@character_set_client */;/*!40101 SET character_set_client = utf8 */;CREATE TABLE `company` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `name` varchar(64) NOT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=6 DEFAULT CHARSET=utf8;/*!40101 SET character_set_client = @saved_cs_client */;/*!40101 SET @saved_cs_client = @@character_set_client */;/*!40101 SET character_set_client = utf8 */;CREATE TABLE `company_address` ( `id` bigint(20) unsigned NOT NULL, `address` varchar(256) NOT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;/*!40101 SET character_set_client = @saved_cs_client */;/*!40101 SET @saved_cs_client = @@character_set_client */;/*!40101 SET character_set_client = utf8 */;CREATE TABLE `createtest` ( `id` varchar(1) DEFAULT NULL...# 未使用--compact[root@host-192-168-200-154 tmpdir]# du -h t1.sql 8.0K t1.sql#其备份文件如下[root@host-192-168-200-154 tmpdir]# less t1.sql -- MySQL dump 10.13 Distrib 5.6.11, for Linux (x86_64)---- Host: localhost Database: hotdb_01-- -------------------------------------------------------- Server version 5.6.11-log/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;/*!40101 SET NAMES utf8 */;/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;/*!40103 SET TIME_ZONE=&apos;+00:00&apos; */;/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE=&apos;NO_AUTO_VALUE_ON_ZERO&apos; */;/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;---- Table structure for table `company`--DROP TABLE IF EXISTS `company`;/*!40101 SET @saved_cs_client = @@character_set_client */;/*!40101 SET character_set_client = utf8 */;CREATE TABLE `company` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `name` varchar(64) NOT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=6 DEFAULT CHARSET=utf8;/*!40101 SET character_set_client = @saved_cs_client */;---- Table structure for table `company_address`--DROP TABLE IF EXISTS `company_address`;/*!40101 SET @saved_cs_client = @@character_set_client */;/*!40101 SET character_set_client = utf8 */;CREATE TABLE `company_address` ( `id` bigint(20) unsigned NOT NULL, `address` varchar(256) NOT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;/*!40101 SET character_set_client = @saved_cs_client */;... –compatible=name告诉MySQL,导出的备份文件尽量与name兼容;name可以为ansi, mysql323, mysql40, postgresql, oracle, mssql, db2, maxdb, no_key_options, no_table_options, no_field_options这些字段。 –complete-insertINSERT语句包括完整的列名。 –create-optionsCREATE TABLE语句中包括所有的MySQL表选项。 –tab对备份的数据库中每个表都会产生两个文件，一份是table_name.sql，记录的是CREATE TABLE创建的表结构，一份是table_name.txt，记录的是该表的数据，并非包含INSERT语句。 # 使用--tab参数mysqldump --login-path=root3306 --single-transaction --tab=/data/mysqldata3306/tmpdir/ hotdb# 结果-rw-r--r-- 1 root root 1391 Oct 25 21:10 company_address.sql-rw-rw-rw- 1 mysql mysql 120 Oct 25 21:10 company_address.txt-rw-r--r-- 1 root root 1395 Oct 25 21:10 company.sql-rw-rw-rw- 1 mysql mysql 16 Oct 25 21:10 company.txt-rw-r--r-- 1 root root 3755 Oct 25 21:10 createtest.sql-rw-rw-rw- 1 mysql mysql 0 Oct 25 21:10 createtest.txt-rw-r--r-- 1 root root 1973 Oct 25 21:10 customer.sql-rw-rw-rw- 1 mysql mysql 6251 Oct 25 21:10 customer.txt-rw-r--r-- 1 root root 1829 Oct 25 21:10 orderlist.sql-rw-rw-rw- 1 mysql mysql 28054 Oct 25 21:10 orderlist.txt-rw-r--r-- 1 root root 1711 Oct 25 21:10 product.sql-rw-rw-rw- 1 mysql mysql 329 Oct 25 21:10 product.txt# 查看.sql和.txt文件[root@mysql tmpdir]# less company_address.sql -- MySQL dump 10.13 Distrib 5.6.26, for Linux (x86_64)---- Host: localhost Database: hotdb-- -------------------------------------------------------- Server version 5.6.26-log/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;/*!40101 SET NAMES utf8 */;/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;/*!40103 SET TIME_ZONE=&apos;+00:00&apos; */;/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE=&apos;&apos; */;/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;---- Table structure for table `company_address`--DROP TABLE IF EXISTS `company_address`;/*!40101 SET @saved_cs_client = @@character_set_client */;/*!40101 SET character_set_client = utf8 */;CREATE TABLE `company_address` ( `id` bigint(20) unsigned NOT NULL, `address` varchar(256) NOT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;/*!40101 SET character_set_client = @saved_cs_client */;/*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */;/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;-- Dump completed on 2015-10-25 21:17:58[root@mysql tmpdir]# less company_address.txt 1 北京省北京市某某区2 上海省上海市某某区3 江苏省南京市某某区4 广东省广州市某某区``` **特别说明** 1. 使用该参数需要FILE权限。 2. 服务器必须拥有写入到目录的权限。 - --xml,-X 备份文件为xml文件。 使用–xml参数mysqldump –login-path=root3306 –single-transaction –xml hotdb&gt; hotdb.sql[root@mysql tmpdir]# lltotal 168-rw-r–r– 1 root root 170151 Oct 25 21:26 hotdb.sql[root@mysql tmpdir]# less hotdb.sql&lt;?xml version=”1.0”?&gt; 1 A 2 B …#### 过滤选项- --all-databases,-A 备份所有数据库。 - --database,-B 备份指定的数据库，备份几个数据库。 - --events,-E 备份数据库的event,需要EVENT权限。 - --ignore-table=db_name.table_name 不备份指定的表。 - --no-data,-d 不备份表数据。 - --routines,-R 备份数据库的stored routines(存储过程+函数)，需要对mysql.proc表的SELECT权限。 - --tables 只备份指定的表,会忽略--databases/-B参数。 使用–tables参数，其后接的第一个参数是数据库名，从第二个参数开始均为表名mysqldump –login-path=root3306 –tables hotdb company customer &gt; hotdb.sql- --triggers 备份数据库的触发器。 - --where=&apos;where条件&apos;，-w &apos;where条件&apos; 只备份指定条件的数据。 使用–where参数mysqldump –login-path=root3306 -w ‘id=1’ –tables hotdb customer &gt; hotdb.sql [root@mysql tmpdir]# less hotdb.sql …– Dumping data for table customer– WHERE: id=1 LOCK TABLES customer WRITE;/!40000 ALTER TABLE customer DISABLE KEYS /;INSERT INTO customer VALUES (1,’赵合肥’,’13912340001’,1,’Anhui’,’合肥’,’某某街某某号’);/!40000 ALTER TABLE customer ENABLE KEYS /;UNLOCK TABLES;… mysqldump –login-path=root3306 –single-transaction -w “city=’ 上海’” –tables hotdb customer &gt; hotdb.sql [root@mysql tmpdir]# mysqldump –login-path=root3306 –single-transaction -w “city=’ 上海’” –tables hotdb customer &gt; hotdb.sql …– Dumping data for table customer– WHERE: city=’上海’ LOCK TABLES customer WRITE;/!40000 ALTER TABLE customer DISABLE KEYS /;INSERT INTO customer VALUES (75,’罗上海’,’13912340075’,25,’Shanghai’,’上海’,’某某街某某号’),(76,’毕上海’,’13912340076’,25,’Shanghai’,’上海’,’某某街某某号’),(77,’郝上海’,’13912340077’,25,’Shanghai’,’上海’,’某某街某某号’),(78,’邬上海’,’13912340078’,25,’Shanghai’,’上海’,’某某街某某号’),(79,’安上海’,’13912340079’,25,’Shanghai’,’上海’,’某某街某某号’),(80,’常上海’,’13912340080’,25,’Shanghai’,’上海’,’某某街某某号’),(81,’乐上海’,’13912340081’,25,’Shanghai’,’上海’,’某某街某某号’),(82,’于上海’,’13912340082’,25,’Shanghai’,’上海’,’某某街某某号’),(83,’时上海’,’13912340083’,25,’Shanghai’,’上海’,’某某街某某号’),(84,’傅上海’,’13912340084’,25,’Shanghai’,’上海’,’某某街某某号’),(85,’皮上海’,’13912340085’,25,’Shanghai’,’上海’,’某某街某某号’),(86,’卞上海’,’13912340086’,25,’Shanghai’,’上海’,’某某街某某号’),(87,’齐上海’,’13912340087’,25,’Shanghai’,’上海’,’某某街某某号’),(88,’康上海’,’13912340088’,25,’Shanghai’,’上海’,’某某街某某号’),(89,’伍上海’,’13912340089’,25,’Shanghai’,’上海’,’某某街某某号’),(90,’余上海’,’13912340090’,25,’Shanghai’,’上海’,’某某街某某号’);/!40000 ALTER TABLE customer ENABLE KEYS /;UNLOCK TABLES;…#### 性能选项- --extended-insert,-e 产生批量插入的SQL语句，可以使用--skip-extended-insert产生单次插入语句。 - --insert-ignore 使用INSERT IGNORE INTO代替INSERT INTO语句。 - --opt 该参数选项是默认开启的，是 --add-drop-table，--add-locks，--create-options，--disable-keys，--extended-insert，--lock-tables，--quick，--set-charset的集合。 - --quick,-q 该参数适用于备份大表，没有该参数，则mysqldump在导出结果前将整个结果集装载到内存中，如果使用该参数，则mysqldump将直接导出结果集至文件中，不经过内存。 - --skip-opt 跳过--opt参数。 #### 事务选项- --add-locks 每张表备份前后添加LOCK TABLES和UNLOCK TABLES语句。 - --flush-logs,-F 备份前FLUSH数据库日志，这个参数要求RELOAD权限。 **注意** 如果--flush-logs与--all-databases一起使用，则日志将会在每备份一个库时FLUSH一次； 如果--flush-logs与--lock-all-tables/--master-data/--single-transaction一起使用，日志将仅仅被刷新一次。 - --flush-privileges 备份mysql(指的是系统库)库完毕后，添加FLUSH PRIVILEGES语句。 - --lock-all-tables,-x 在整个备份的过程中，将有一个全局的读锁锁住所有的数据库，该参数将会自动关闭--single-transaction和--lock-tables参数。 - --lcok-tables,-l 备份每个库之前对该库的每张表添加读锁，对于事务型存储引擎而言，--single-trnsaction比--lock-tables更适合，因为--single-transaction根本不需要锁。 - --no-autocommit 为每张表的INSERT语句添加SET autocommit = 0和COMMIT语句。 – – Dumping data for table t1LOCK TABLES t1 WRITE;/!40000 ALTER TABLE t1 DISABLE KEYS /;set autocommit=0;INSERT INTO t1 VALUES (1,’fzqX83OfCJ3eLs6a’,9),(2,’TeijyzUju26j1AmY’,34),(3,’By4SYpW2MYzGtaIq’,59),(4,’8DepDafmEtpARRgp’,42),(5,’OUVJWkV5Z2J1mW1M’,97),(6,’gzQyudMSB4v86uKZ’,84),(7,’J1IdGyYhvkQQy0mO’,90),(8,’WBSR3OW4QTwKz8HT’,2),(9,’YG6pAUHdDOdkCcrt’,19),(10,’3lEboOZkvhgW8C80’,12);/!40000 ALTER TABLE t1 ENABLE KEYS /;UNLOCK TABLES;commit;/!40103 SET TIME_ZONE=@OLD_TIME_ZONE /;- --order-by-primary 备份每张表的数据按主键或者第一个唯一索引排序。 - --single-transaction 该参数将事务隔离级别设置为REPEATABLE READ,并且在开始备份前发送START TRANSACTION语句至MySQL数据库中，这样可以保证备份的数据具有一致性。但是该参数仅适用于Innodb表，此时备份数据时，不阻塞任何英语程序。 **注意** 1. 当使用--single-transaction参数备份时，为了保证一个有效的备份文件，禁止执行以下语句： ALTER TABLE; CREATE TABLE; DROP TABLES; RENAME TABLE; TRUNCATE TABLE; 2. --single-transaction和--lock-tables两个参数是互斥的，因为前者不阻塞应用，后者会阻塞应用。 3. 当备份InnoDB类型的大表时，--single-transaction和--quick一起使用更佳。 推荐mysqldump使用语句--------------------以下均只适用于事务型存储引擎，如InnoDB #### 常规备份``` mysqldump --login-path=root3306 --single-transaction --master-data=2 --quick --no-autocommit --extended-insert --complete-insert --events --routines --comments test &gt; test.sql 压缩备份mysqldump --login-path=root3306 --single-transaction --master-data=2 --quick --no-autocommit --events --routines --compact test &gt; test.sql 表结构与数据分开mysqldump --login-path=root3306 --single-transaction --quick --comments --tab=/data/mysql/mysqldata3306/tmpdir/ test 备份slave数据库mysqldump --login-path=root3306 --single-transaction --dump-slave=2 --include-master-host-port --quick --no-autocommit --compact --events --routines test &gt; test.sql mysqldump限制 mysqldump默认不会备份INFORMATION_SCHEMA和performance_schema数据库，备份这些数据库时，需要在命令行指定这两个数据库。 mysqldump不会备份MySQL集群的ndbnfo数据库。 在mysql5.6.6之前，mysqldump不会备份general_log和slow_log表。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[mysqladmin管理MySQL服务器的客户端]]></title>
      <url>%2F2015%2F10%2F08%2Fmysqladmin%E7%AE%A1%E7%90%86MySQL%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E5%AE%A2%E6%88%B7%E7%AB%AF%2F</url>
      <content type="text"><![CDATA[mysqladhmin是一款作为管理MySQL的客户端工具，你可以用它来检查服务器的配置和当前状态等等。 语法# 语法mysqladmin [options] command [command-arg]# 示例[root@mysql ~]# mysqladmin --user=root --socket=/data/mysqldata3306/sock/mysql.sock status -pEnter password: Uptime: 86415 Threads: 3 Questions: 8545 Slow queries: 2 Opens: 79 Flush tables: 1 Open tables: 68 Queries per second avg: 0.098 Command create db_name创建新的数据库 debug使服务器将debug信息写到error log里 drop db_name删除该数据库及所有表 extended-status输出服务器的状态变量，相当于show global status; flush-hosts刷新host cache中的所有信息 flus-status清除所有的状态变量 flush-tables刷新所有的表 flush-threads刷新所有的线程缓存 kill id,id,…杀死服务器线程，id为thread id old-password new-password修改旧密码 password new-password设置新密码 ping检查服务器是否可用 [root@mysql ~]# mysqladmin --login-path=root3306 pingmysqld is alive processlist相当于show processlist [root@mysql ~]# mysqladmin --login-path=root3306 processlist+----+-----------------+-----------+----+---------+-------+------------------------+------------------+| Id | User | Host | db | Command | Time | State | Info |+----+-----------------+-----------+----+---------+-------+------------------------+------------------+| 1 | event_scheduler | localhost | | Daemon | 88456 | Waiting on empty queue | || 39 | root | localhost | | Query | 0 | init | show processlist |+----+-----------------+-----------+----+---------+-------+------------------------+------------------+ reload加载权限表（grant table） refresh刷新所有表并且先关闭当前binlog,重新开启新的binlog shutdown关闭MySQL服务器 start-slave开启slave,相当于start slave stop-slave关闭slave,相当于stop slave status输出当前服务器的状态 [root@mysql ~]# mysqladmin --login-path=root3306 statusUptime: 89174 Threads: 2 Questions: 8594 Slow queries: 2 Opens: 79 Flush tables: 3 Open tables: 0 Queries per second avg: 0.096# 对上述的输出名词解释Uptime 当前服务器运行了多长时间Threads 活动线程（active threads）的数量Questions 服务器运行以来执行的总查询数，不论是正确的SQL还是错误的SQLSlow queries 慢查询数量Opens 服务器开启的表数量Flush tables 服务器执行flush-*/refresh/reload的数量Open tables 服务器当前打开的表的数量 variables输出当前服务器的系统变量，相当于show global variables; Option以下选项可在配置文件中的[mysqladmin]组下配置。（以下仅列出一些个人觉得有用的选项，具体可通过mysqladmin –help查看） –compress 压缩所有的信息在服务器与客户端之间传输 –force 即使SQL语句出错，仍然继续执行 –login-path 一种连接MySQL的方式，详情请参考：http://wing324.github.io/2015/09/28/mysql-config-editor%E5%B7%A5%E5%85%B7/ –print-default 输出默认选项 [root@mysql ~]# mysqladmin --login-path=root3306 --print-defaultsmysqladmin would have been started with the following arguments:--user=root --password=root --socket=/data/mysqldata3306/sock/mysql.sock –sleep=N 每N分钟重复执行相同命令 [root@mysql ~]# mysqladmin --login-path=root3306 status --sleep=5Uptime: 101052 Threads: 3 Questions: 8633 Slow queries: 2 Opens: 79 Flush tables: 3 Open tables: 0 Queries per second avg: 0.085Uptime: 101057 Threads: 3 Questions: 8634 Slow queries: 2 Opens: 79 Flush tables: 3 Open tables: 0 Queries per second avg: 0.085Uptime: 101062 Threads: 3 Questions: 8635 Slow queries: 2 Opens: 79 Flush tables: 3 Open tables: 0 Queries per second avg: 0.085 –relative 与–sleep一起使用，显示当前状态值和之前状态值的差，仅能用于extended-status]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[mysql_install_db的Warning解决方法]]></title>
      <url>%2F2015%2F10%2F08%2Fmysql-install-db%E7%9A%84Warning%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95%2F</url>
      <content type="text"><![CDATA[每次在使用mysql_install_db的时候都会有一个恼人的Waring:WARNING: The host ‘mysql’ could not be looked up with /usr/bin/resolveip.在强迫症的眼里，Warning就是个Error啊！ 参考链接：http://yangrong083.blog.163.com/blog/static/113406097201526101243232/ Warning如下：WARNING: The host ‘mysql’ could not be looked up with /usr/bin/resolveip.This probably means that your libc libraries are not 100 % compatiblewith this binary MySQL version. The MySQL daemon, mysqld, should worknormally with the exception that host name resolving will not work.This means that you should use IP addresses instead of hostnameswhen specifying MySQL privileges ! Warning的原因是主机名host不能被解析。 解决方法： cat /etc/hosts此处可以发现并没有对应的ip：主机名 [root@mysql mysqldata3308]# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 vi /etc/hosts添加对应的ip:主机名 [root@mysql mysqldata3308]# vi /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.200.143 mysql 使用resoveip解析下主机名 [root@mysql mysqldata3308]# /usr/bin/resolveip mysqlIP address of mysql is 192.168.200.143# 此时看到完美解析 此时再次使用mysql_install_db将不再出现该Warning，至少我已经成功解决了~ mysql_install_db工具的使用：http://wing324.github.io/2015/10/08/mysql-install-db%E5%88%9D%E5%A7%8B%E5%8C%96%E6%95%B0%E6%8D%AE%E5%BA%93%E5%B7%A5%E5%85%B7/]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[mysql_install_db初始化数据库工具]]></title>
      <url>%2F2015%2F10%2F08%2Fmysql-install-db%E5%88%9D%E5%A7%8B%E5%8C%96%E6%95%B0%E6%8D%AE%E5%BA%93%E5%B7%A5%E5%85%B7%2F</url>
      <content type="text"><![CDATA[编译安装MySQL的时候，我们可以编译指定MySQL各种数据目录，比如datadir，binlog等等，那么在RPM安装MySQL时怎么办呢？别担心，我们有mysql_install_db神器，哈哈哈哈哈~ mysql_install_db对MySQL做什么 初始化MySQL的数据目录，该数据目录在–defaults-file指定的配置文件中配置，或者不指定时默认为/etc/my.cnf（需要说明的是，这些数据目录需要执行自行创建好，并修改目录的user&amp;group为适当权限的用户，如mysql:mysql）； 创建MySQL的系统表，即mysql和information_schema库和表（此处可能包含performance_schema,还没有研究过performance_schema,研究之后再来确定）； 初始化系统表空间和需要管理Innodb表的数据结构； mysql_install_db使用 语法mysql_install_db [options] 使用说明 由于mysql_install_db在运行期间mysqld需要进入到数据目录中，所以要使用与运行mysqld相同账号运行，所以可能需要指定–user。也许还需要指定–basedir和–datadir,当不使用默认的安装目录和数据目录。 mysql_install_db --user=mysql --datadir=/data/mysqldata3308/mydata/ 注意mysql_install_db读取my.cnf配置文件时，只读取[mysqld]组，所以请在运行该工具之前，将[mysqlN]修改为[mysqld],运行完mysql_install_db之后，再将[mysqld]修改回[mysqldN]。 参数选项 –basedir MySQL安装目录 –builddir –cross_bootstrap内核参数，暂不做解释 –datadir MySQL数据目录 –defaults-extra-file额外的配置文件，在读取配置文件之后读取该额外的配置文件 –defaults-file指定配置文件 –force运行mysql_install_db即使在DNS不工作的情况下 –keep-my-cnf运行mysql_install_db时，不创建配置文件模板my.cnf –ldata=pathMySQL数据库目录，相当于–datadir –no-defaults不读取配置文件中的任意选项 –random-passwords为MySQL的所有root账号创建一个随机密码并设置过期时间 –rpm内核参数，暂不做解释 –skip-name-resolve创建权限表时，使用IP地址取代主机名，用于DNS不工作的情况下 –srcdirMySQL源码目录 –user运行mysqld的用户名 –windows内核参数，暂不做解释除以上参数外，还支持mysqld参数。 mysql_install_db由于主机解析不了导致的Warning的解决方法：http://wing324.github.io/2015/10/08/mysql-install-db%E7%9A%84Warning%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Liunx系统命令之tar命令]]></title>
      <url>%2F2015%2F10%2F07%2FLiunx%E7%B3%BB%E7%BB%9F%E5%91%BD%E4%BB%A4%E4%B9%8Btar%E5%91%BD%E4%BB%A4%2F</url>
      <content type="text"><![CDATA[我经常混淆tar命令的各种选项，为了不用每次使用tar命令的时候，去google搜索tar的选项详解，主要还是FQ，所以我还是默默的把tar选项整理一遍，便于日后自己查找方便吧~ tar选项详解-c 建立压缩文件-x 解压一个压缩文件-t 查看tarfile里面的文件-z 是否具有gzip属性？或者是否要用gzip属性压缩-j 是否具有bzip2属性？或者是否需要用bzip2属性压缩-v 压缩的过程中显示文件夹-f 使用f后紧跟的文件名-p 使用原文件属性，属性不会根据使用者而改变-P 使用绝对路径来压缩 实战演练实战演练一：压缩tar -cf new_file.tar old_file 仅打包文件不压缩tar -zcvf new_file.tar.gz old_file 打包文件且以gzip属性压缩tar -jcvf new_file.tar.bz2 old_file 打包文件且以bzip2属性压缩 实战演练二：查看tarfile中包含哪些文件tar -ztvf file.tar.gz 查看gzip属性的压缩文件 实战演练三：解压缩文件tar -zxvf new_file old_file.tar.gz 解压缩gzip属性的文件tar -jxvf new_file old_file.tar.bz2 解压缩bzip2属性的文件]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[MySQL字符集]]></title>
      <url>%2F2015%2F10%2F06%2FMySQL%E5%AD%97%E7%AC%A6%E9%9B%86%2F</url>
      <content type="text"><![CDATA[啊啊啊啊啊啊啊，乱码真的是个太恼人的东西了，而且一乱码就心慌……好吧，好好学习下字符集吧。。 字符集简介 字符集不仅影响到数据的存储,也影响到客户端和MySQL服务器之间的通信,如果你需要客户端和服务器之间通信不同于默认的字符集,则需要指定字符集通过以下的方式：set names charset_name;特别说明set names修改字符集,修改的参数如下：character_set_clientcharacter_set_connectioncharacter_set_resultscollation_connection 通过SHOW CHARACTER SET查看MySQL支持的字符集(SHOW CHARACTER SET LIKE ‘utf8’);通过SHOW COLLATION查看MySQL支持的排序规则(SHOW COLLATION LIKE ‘utf8%’)。 排序规则的特点 不同的字符集有不同的排序规则 每一个字符集都有一个默认的排序规则 排序规则中,_ci(case insensitive)大小写不敏感,_cs(case sensitive)大小写敏感,_bin(binary)二进制# _ciroot@localhost : character_set_test 01:36:24&gt; show create table t1;+-------+-------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+-------+-------------------------------------------------------------------------------------------------------------------------------------------+| t1 | CREATE TABLE `t1` ( `id` int(11) DEFAULT NULL, `name` varchar(16) CHARACTER SET gbk DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8 |+-------+-------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec)root@localhost : character_set_test 01:37:04&gt; select * from t1;+------+------+| id | name |+------+------+| 1 | ABd |+------+------+1 row in set (0.01 sec)root@localhost : character_set_test 01:37:10&gt; select * from t1 where name like 'A%';+------+------+| id | name |+------+------+| 1 | ABd |+------+------+1 row in set (0.00 sec)root@localhost : character_set_test 01:37:21&gt; select * from t1 where name like 'a%';+------+------+| id | name |+------+------+| 1 | ABd |+------+------+1 row in set (0.01 sec) # _csroot@localhost : character_set_test 01:37:24&gt; show create table t2;+-------+-------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+-------+-------------------------------------------------------------------------------------------------------------------------------------------+| t2 | CREATE TABLE `t2` ( `name` varchar(16) CHARACTER SET latin1 COLLATE latin1_general_cs DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8 |+-------+-------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.01 sec)root@localhost : character_set_test 01:37:56&gt; select * from t2;+------+| name |+------+| ABd |+------+1 row in set (0.00 sec)root@localhost : character_set_test 01:38:06&gt; select * from t2 where name like 'A%';+------+| name |+------+| ABd |+------+1 row in set (0.00 sec)root@localhost : character_set_test 01:38:14&gt; select * from t2 where name like 'a%';Empty set (0.00 sec) # _binroot@localhost : character_set_test 01:40:09&gt; show create table t3;+-------+------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+-------+------------------------------------------------------------------------------------------------------------------------------------+| t3 | CREATE TABLE `t3` ( `name` varchar(16) CHARACTER SET latin1 COLLATE latin1_bin DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8 |+-------+------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec)root@localhost : character_set_test 01:40:17&gt; select * from t3 ;+------+| name |+------+| ABd |+------+1 row in set (0.00 sec)root@localhost : character_set_test 01:40:37&gt; select * from t3 where name like 'A%';+------+| name |+------+| ABd |+------+1 row in set (0.00 sec)root@localhost : character_set_test 01:40:47&gt; select * from t3 where name like 'a%';Empty set (0.00 sec) 一个字符集会有多个排序规则,具体如何排序,请参照 http://www.collation-charts.org/ 字符集&amp;排序规则 字符集和排序规则可以设置4个级别：server,database,table,column CHARACTRE SET=CHARSET Server character set &amp; collationMySQL服务器的字符集和排序规则character_set_servercollation_server可在线修改参数 Database character set &amp; collation 数据库的字符集和排序规则character_set_databasecollation_database可在线修改用于默认的数据库字符集和排序规则 所有的数据库选项被保存在数据库目录下的db.opt文件里 [root@wing character_set_test]# cat db.opt default-character-set=utf8default-collation=utf8_general_ci 如果没有指定表的字符集和排序规则,那么表的字符集和排序规则默认使用character_set_database和collation_database。 对于LOAD DATA语句,如果没有指定character set,默认使用character_set_database。 对于存储过程和函数来说,如果没有指定character set和collation,默认使用character_set_database和collation_database。 Table character set &amp; collation 在CREATE TABLE 和 ALTER TABLE中指定character set &amp; collation。 表的字符集和排序规则被使用在列上,当列没有指定的字符集和排序规则。 Column character set &amp; collation 在CREATE TABLE和ALTER TABLE中指定character set &amp; collation。 如果列没有指定的字符集和排序规则,那么将使用表的字符集和排序规则。 如果列指定了字符集,那么排序规则将使用字符集默认的排序规则,而不是表的字符集。 字符串的character set &amp; collation[_charset_name]'string' [COLLATE collation_name] EXAMPLEroot@localhost : character_set_test 01:59:42&gt; select '中文';+--------+| 中文 |+--------+| 中文 |+--------+1 row in set (0.01 sec)root@localhost : character_set_test 02:01:29&gt; select _gbk'中文';+---------+| 涓?枃 |+---------+| 涓?枃 |+---------+1 row in set (0.00 sec)root@localhost : character_set_test 02:01:34&gt; select _gbk'中文' collate gbk_bin;+------------------------------+| _gbk'中文' collate gbk_bin |+------------------------------+| 涓?枃 |+------------------------------+1 row in set (0.00 sec) 特别说明 相关参数character_set_connectioncollation_connection _charset_name表达式旨在告诉解析器：该字符串使用指定的charset Connection character set &amp; collation 相关参数character_set_client：客户端发来的字符集,如client端发来的语句使用该字符集到server端character_set_connection collation_connection：server端接收该语句后使用的字符集和排序规则,server端将语句从character_set_client转换到character_set_connection(除了使用_charset_name指定的字符串)character_set_database collation_database：指定数据库的字符集和排序规则character_set_filesystemcharacter_set_results：服务器返回结果给客户端的字符集character_set_server collation_server：server端默认的内部操作字符集character_set_system 设置字符集语句1)set names ‘charset_name‘ collate ‘collate_name‘指明了客户端发来语句时使用的字符集以及客户端收到语句时使用的字符集set names修改字符集,修改的参数如下:character_set_client = character_namecharacter_set_connection = character_namecharacter_set_results = character_namecollation_connection = collation_name2)set character name ‘charset_name‘修改的参数如下:character_set_client = character_namecharacter_set_results = character_namecollation_connection = collation_database由于collation_connection的改变会引起character_set_connection的改变3)使用mysql客户端时,可以直接使用charset ‘charset_name‘,与set names一样root@localhost : (none) 04:13:54&gt; charset latin1Charset changedroot@localhost : (none) 04:14:02&gt; show variables like 'character%';+--------------------------+----------------------------+| Variable_name | Value |+--------------------------+----------------------------+| character_set_client | latin1 || character_set_connection | latin1 || character_set_database | utf8 || character_set_filesystem | binary || character_set_results | latin1 || character_set_server | utf8 || character_set_system | utf8 || character_sets_dir | /usr/share/mysql/charsets/ |+--------------------------+----------------------------+8 rows in set (0.00 sec) 应用程序的character set &amp; collection 应用程序使用的MySQL默认charset&amp;collation(latin1,latin1_swedish_ci),如果需要不同的charset&amp;collection,可使用如下几种方式1) 为每个数据库指定一个charset&amp;collation2) 服务器启动时指定charset&amp;collation3) 在配置文件中指定charset&amp;collation 错误信息的charset &amp; collation描述服务器如何使用字符集构建错误日志以及返回客户端 MySQL使用utf8构建错误信息,以character_set_results返回到客户端 错误信息的模板为utf8错误信息模板中参数被特定值取代时：标识值如表名/列名在内部使用utf8;字符串值(非二进制形式)则将它们从charset转换成utf8;二进制字符串在 (0x20,0x7E)范围内使用十六进制,其他的使用十进制 返回错误信息时,MySQL服务器会将错误信息从utf8转换到character_set_results设置的字符集中。 字符集排序规则(collation)_ci：大小写不敏感_cs：大小写敏感_bin：字符基于二进制编码,大小写敏感 MySQL元数据的字符集 MySQL默认元数据的字符集为UTF-8。 参数character_set_system设置的便是MySQL元数据字符集。 字符集转换1) 元数据与数据之间字符集转换SELECT * FROM t1 WHERE USER() = latin1_column;# latin1_column自动转换成utf8字符集 INSERT INTO t1 (latin1_column) SELECT USER();# user()自动转换成latin1字符集 2) 列的字符集转换 如果列为二进制的数据类型(BINARY,VARBINARY,BLOB),所有的值必须使用同一个字符集,如果使用多个字符集,MySQL没法知道哪些值使用了那些字符集导致MySQL不能正确转换数据。 如果列为非二进制的数据类型(CHAR,VARCHAR,TEXT),列值将会被列的字符集编码,如果列值在不同的字符集里,可先将列值转换成二进制,然后将二进制列转换成需要的列。 如果某列由于字符集原因导致乱码,可将该列转换为二进制模式(binary/varbinary/blob) # 一个乱码小实验root@localhost : ym 02:14:53&gt; show variables like 'character%';+--------------------------+----------------------------+| Variable_name | Value |+--------------------------+----------------------------+| character_set_client | utf8 || character_set_connection | utf8 || character_set_database | utf8 || character_set_filesystem | binary || character_set_results | utf8 || character_set_server | utf8 || character_set_system | utf8 || character_sets_dir | /usr/share/mysql/charsets/ |+--------------------------+----------------------------+8 rows in set (0.00 sec)root@localhost : ym 02:15:01&gt; show create table t;+-------+------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+-------+------------------------------------------------------------------------------------------------------------------------+| t | CREATE TABLE `t` ( `id` int(11) DEFAULT NULL, `name` varchar(16) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8 |+-------+------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec)root@localhost : ym 02:15:16&gt; select * from t;+------+--------+| id | name |+------+--------+| 1 | 中国 |+------+--------+1 row in set (0.00 sec)root@localhost : ym 02:15:21&gt; set names gbk;Query OK, 0 rows affected (0.00 sec)root@localhost : ym 02:15:26&gt; show variables like 'character%';+--------------------------+----------------------------+| Variable_name | Value |+--------------------------+----------------------------+| character_set_client | gbk || character_set_connection | gbk || character_set_database | utf8 || character_set_filesystem | binary || character_set_results | gbk || character_set_server | utf8 || character_set_system | utf8 || character_sets_dir | /usr/share/mysql/charsets/ |+--------------------------+----------------------------+8 rows in set (0.00 sec)root@localhost : ym 02:15:30&gt; show create table t;+-------+------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+-------+------------------------------------------------------------------------------------------------------------------------+| t | CREATE TABLE `t` ( `id` int(11) DEFAULT NULL, `name` varchar(16) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8 |+-------+------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec)root@localhost : ym 02:15:36&gt; select * from t;+------+------+| id | name |+------+------+| 1 | |+------+------+1 row in set (0.00 sec)root@localhost : ym 02:15:41&gt; alter table t modify name varbinary(16);Query OK, 1 row affected (0.02 sec)Records: 1 Duplicates: 0 Warnings: 0root@localhost : ym 02:16:00&gt; select * from t;+------+--------+| id | name |+------+--------+| 1 | 中国 |+------+--------+1 row in set (0.00 sec)root@localhost : ym 02:16:04&gt; show create table t;+-------+--------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+-------+--------------------------------------------------------------------------------------------------------------------------+| t | CREATE TABLE `t` ( `id` int(11) DEFAULT NULL, `name` varbinary(16) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8 |+-------+--------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec) 对于binary类型转换为char类型,末尾会有0x00结尾符,体现在结果集中为异常的空格,可使用TRIM()函数去除EXAMPLE root@localhost : ym 02:21:40&gt; create table t(id int,name binary(50));Query OK, 0 rows affected (0.01 sec)root@localhost : ym 02:22:04&gt; insert into t values(1,'中国');Query OK, 1 row affected (0.00 sec)root@localhost : ym 02:22:15&gt; select * from t;+------+----------------------------------------------------+| id | name |+------+----------------------------------------------------+| 1 | 中国 |+------+----------------------------------------------------+1 row in set (0.00 sec)root@localhost : ym 02:22:19&gt; alter table t modify name char(50);Query OK, 1 row affected (0.02 sec)Records: 1 Duplicates: 0 Warnings: 0root@localhost : ym 02:22:45&gt; select * from t;+------+----------------------------------------------------+| id | name |+------+----------------------------------------------------+| 1 | 中国 |+------+----------------------------------------------------+1 row in set (0.00 sec)# 中国后面有异常的空格# 正常非转换过的结果集如下# root@localhost : ym 02:24:02&gt; create table t1 (id int,name char(50));# Query OK, 0 rows affected (0.01 sec)# root@localhost : ym 02:22:46&gt; update t set name =trim(trailing 0x00 from name);# Query OK, 1 row affected (0.00 sec)# Rows matched: 1 Changed: 1 Warnings: 0# root@localhost : ym 02:24:16&gt; insert into t1 values(1,'中国');# Query OK, 1 row affected (0.00 sec)# root@localhost : ym 02:24:35&gt; select * from t1;# +------+--------+# | id | name |# +------+--------+# | 1 | 中国 |# +------+--------+# 1 row in set (0.00 sec)# root@localhost : ym 02:23:25&gt; select * from t;# +------+--------+# | id | name |# +------+--------+# | 1 | 中国 |# +------+--------+# 1 row in set (0.00 sec)# 使用trim()函数解决该情况root@localhost : ym 02:22:46&gt; update t set name =trim(trailing 0x00 from name);Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0root@localhost : ym 02:23:25&gt; select * from t;+------+--------+| id | name |+------+--------+| 1 | 中国 |+------+--------+1 row in set (0.00 sec) 两个字符集的切换,保险起见,还是应该先将字符集切换为二进制类型(binary/varbinary/blob),而后转换成自己想要的字符集。 如果需要将表中所有列的字符集转换成另外的字符集,可以使用语句ALTER TABLE table_name CONVERT TO CHARACTER SET charset_name; 总结一个完整的用户请求的字符集转换过程 特别说明 相关变量character_set_client：client端发出请求使用的字符集character_set_connection：Server端接受到Client端的请求后,使用的字符集character_set_database：数据库默认字符集,如果没有默认数据库,那就使用character_set_server指定的字符集character_set_filesystem：文件系统的字符集，把操作系统上文件名转化成此字符集,即把character_set_client转换character_set_filesystem,该参数默认字符集为binary则不需做任何转换character_set_results：返回结果集的字符集character_set_server：数据库服务器的默认字符集character_set_system：MySQL服务器的元数据字符集,默认为utf8]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Docker的快速使用]]></title>
      <url>%2F2015%2F10%2F06%2FDocker%E7%9A%84%E5%BF%AB%E9%80%9F%E4%BD%BF%E7%94%A8%2F</url>
      <content type="text"><![CDATA[介绍完Docker的安装和基本使用之后，那就进入quick start吧。Docker的安装和基本使用：http://wing324.github.io/2015/10/06/Docker%E7%9A%84%E5%AE%89%E8%A3%85%E5%8F%8A%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/ 推荐课程：http://www.jikexueyuan.com/path/docker/ Docker的配置文件docker的配置文件位于/etc/default/docker使用后续介绍 Docker的基本操作 启动容器：docker run IMAGE启动交互式容器：docker run -i -t IMAGE /bin/bash 查看容器：docker ps -a -l 无参数时,返回的是当前运行时的容器-a 代表列出所有的容器-l 代表列出最新创建的容器 返回容器相关配置信息 docker inspect CONTAINER ID/NAMES 自定义容器名 docker run --name=自定义容器名 -i -t IMAGE EXAMPLE [root@localhost ~]# docker run --name=mydocker -i -t hello-worldUsage of loopback devices is strongly discouraged for production use. Either use `--storage-opt dm.thinpooldev` or use `--storage-opt dm.no_warn_on_loop_devices=true` to suppress this warning.Hello from Docker.This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub. 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker Hub account: https://hub.docker.comFor more examples and ideas, visit: https://docs.docker.com/userguide/[root@localhost ~]# docker -aflag provided but not defined: -aSee &apos;docker --help&apos;.[root@localhost ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES4b1a0607bc88 hello-world &quot;/hello&quot; 12 seconds ago Exited (0) 10 seconds ago mydocker # ,名字已换成自定义的mydocker 1e98f9067f6b docker.io/hello-world &quot;/hello&quot; 10 minutes ago Exited (0) 10 minutes ago jovial_pasteur 1d2c288dcde8 hello-world &quot;/bin/bash&quot; 28 minutes ago sharp_hoover 855231f306a4 hello-world &quot;/hello&quot; 28 minutes ago Exited (0) 28 minutes ago desperate_kilby 0ab13a92ed4b hello-world &quot;/hello&quot; 32 minutes ago Exited (0) 32 minutes ago clever_sammet e1fa10d7a55f hello-world &quot;/bin/bash&quot; 35 minutes ago drunk_albattani dad5054db548 hello-world &quot;/hello&quot; 42 minutes ago Exited (0) 42 minutes ago compassionate_turing 16ae30c8b4d0 hello-world &quot;/hello&quot; 45 minutes ago Exited (0) 45 minutes ago cranky_goldstine f1ba689673d3 hello-world &quot;/hello&quot; 21 hours ago Exited (0) 21 hours ago silly_almeida 8f200d44ba00 hello-world &quot;/hello&quot; 21 hours ago Exited (0) 21 hours ago loving_mayer 重新启动停止的容器 docker start [-i] 容器名 删除停止的容器 docker rm 容器名# 不能用于删除运行中的容器 Docker运行守护式容器 以守护形式运行容器 docker run -i -t IMAGECtrl+P Ctrl+Q #将当前容器置于后台运行 附加到运行中的容器 #调出后台运行的容器至前台运行 docker attach 容器名 启动守护式容器 docker run -d IMAGE 循环脚本之类的等,让docker一直运行-d 将docker后台运行,若没有循环脚本之类等,在命令执行结束后即停止,若想让docker一直运行,需在命令后使用-c添加脚本 查看容器日志 docker logs [-f] [-t] [--tail] 容器名-f 表示动态加载日志,相当于LINUX命令里面的tail -f的参数-t 表示显示时间timestamp--tail N 表示只显示最后N条记录 EXAMPLE [root@localhost ~]# docker logs mydockerHello from Docker.This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub. 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker Hub account: https://hub.docker.comFor more examples and ideas, visit: https://docs.docker.com/userguide/ # docker logs -t示例[root@localhost ~]# docker logs -t mydocker2015-09-04T13:06:04.469114324Z 2015-09-04T13:06:04.469184154Z Hello from Docker.2015-09-04T13:06:04.469191726Z This message shows that your installation appears to be working correctly.2015-09-04T13:06:04.469196921Z 2015-09-04T13:06:04.469201788Z To generate this message, Docker took the following steps:2015-09-04T13:06:04.469206498Z 1. The Docker client contacted the Docker daemon.2015-09-04T13:06:04.469211437Z 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub.2015-09-04T13:06:04.469216436Z 3. The Docker daemon created a new container from that image which runs the2015-09-04T13:06:04.469221194Z executable that produces the output you are currently reading.2015-09-04T13:06:04.469225733Z 4. The Docker daemon streamed that output to the Docker client, which sent it2015-09-04T13:06:04.469230539Z to your terminal.2015-09-04T13:06:04.469235191Z 2015-09-04T13:06:04.469239605Z To try something more ambitious, you can run an Ubuntu container with:2015-09-04T13:06:04.469244274Z $ docker run -it ubuntu bash2015-09-04T13:06:04.469248764Z 2015-09-04T13:06:04.469253159Z Share images, automate workflows, and more with a free Docker Hub account:2015-09-04T13:06:04.469257784Z https://hub.docker.com2015-09-04T13:06:04.469262189Z 2015-09-04T13:06:04.469267096Z For more examples and ideas, visit:2015-09-04T13:06:04.469271622Z https://docs.docker.com/userguide/2015-09-04T13:06:04.469276135Z [root@localhost ~]# docker logs -t -f mydocker2015-09-04T13:06:04.469114324Z 2015-09-04T13:06:04.469184154Z Hello from Docker.2015-09-04T13:06:04.469191726Z This message shows that your installation appears to be working correctly.2015-09-04T13:06:04.469196921Z 2015-09-04T13:06:04.469201788Z To generate this message, Docker took the following steps:2015-09-04T13:06:04.469206498Z 1. The Docker client contacted the Docker daemon.2015-09-04T13:06:04.469211437Z 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub.2015-09-04T13:06:04.469216436Z 3. The Docker daemon created a new container from that image which runs the2015-09-04T13:06:04.469221194Z executable that produces the output you are currently reading.2015-09-04T13:06:04.469225733Z 4. The Docker daemon streamed that output to the Docker client, which sent it2015-09-04T13:06:04.469230539Z to your terminal.2015-09-04T13:06:04.469235191Z 2015-09-04T13:06:04.469239605Z To try something more ambitious, you can run an Ubuntu container with:2015-09-04T13:06:04.469244274Z $ docker run -it ubuntu bash2015-09-04T13:06:04.469248764Z 2015-09-04T13:06:04.469253159Z Share images, automate workflows, and more with a free Docker Hub account:2015-09-04T13:06:04.469257784Z https://hub.docker.com2015-09-04T13:06:04.469262189Z 2015-09-04T13:06:04.469267096Z For more examples and ideas, visit:2015-09-04T13:06:04.469271622Z https://docs.docker.com/userguide/2015-09-04T13:06:04.469276135Z # docker logs --tail演示[root@localhost ~]# docker logs -t -f --tail 10 mydocker2015-09-04T13:06:04.469235191Z 2015-09-04T13:06:04.469239605Z To try something more ambitious, you can run an Ubuntu container with:2015-09-04T13:06:04.469244274Z $ docker run -it ubuntu bash2015-09-04T13:06:04.469248764Z 2015-09-04T13:06:04.469253159Z Share images, automate workflows, and more with a free Docker Hub account:2015-09-04T13:06:04.469257784Z https://hub.docker.com2015-09-04T13:06:04.469262189Z 2015-09-04T13:06:04.469267096Z For more examples and ideas, visit:2015-09-04T13:06:04.469271622Z https://docs.docker.com/userguide/2015-09-04T13:06:04.469276135Z 查看容器进程 docker top 容器名 在运行中的容器内启动新的进程 docker exec [-d] [-i] [-t] 容器名 停止守护式容器 docker stop 容器名docker kill 容器名 Docker镜像与仓库 查看镜像 docker images 无参数时,显示所有当前安装的镜像-a 显示所有的镜像,包括中间层镜像（仓库为&lt;none&gt;,TAG为&lt;none&gt;）-f 使用过滤条件--no-trunc 不截断镜像的ID-q,--quiet 只显示镜像的ID 删除镜像 docker rmi 镜像名# 注意 &apos;docker rm 容器名&apos;&apos; 的区别-f,--false 强制删除镜像--no-prune 不删除被打标签的父镜像 查看镜像的详细信息 docker inspect 容器名/镜像名 -f,--format=&apos;&apos; 查找镜像Docker Hub官网查找镜像或者docker search TERM–automated 只显示完成的镜像–no-trunc 不截断IMAGE ID-s,–stars 获取镜像 docker pull NAME:TAG-a,--all-tags 下载仓库中所有的镜像文件# 当网络缓慢时,可以使用--registry-mirror选项在配置文件中添加国内的镜像地址修改/etc/default/docker添加DOCKER_OPTS=&apos;--registry-mirror=添加URL&apos; 推送镜像 docker push 用户名/镜像名 EXAMPLE doxker push wing324/mysql 构建镜像 # 通过容器构建docker commit 容器名 [REPOSITORY[:TAG]]-a,--author 作者-m,--message 提交消息-p,--pause 提交阶段暂停容器# 通过Dockerfile构建镜像docker build-t,--tag 为镜像去一个名字 Dockerfile介绍 Dockerfile关键字 FROMFROMFROM &lt;image&gt;FROM &lt;image&gt;:&lt;tag&gt; 特别说明FROM后的镜像特性：已经存在的镜像基础镜像必须是第一条非注释指令 MAINTAINER指定镜像的作者信息 MAINTAINER &lt;name&gt; RUN指定当前镜像中的运行命令 # shell模式RUN &lt;command&gt;/bin/sh -c commandeg.RUN echo hello # exec模式RUN [&apos;executable&apos;,&apos;参数1&apos;,&apos;参数2&apos;]RUN [&apos;/bin/bash&apos;,&apos;-c&apos;,&apos;echo hello&apos;] EXPOSE指定该镜像容器使用的端口 EXPOSE &lt;port&gt; CMD在容器中指定运行的命令RUN是在镜像构建中执行的命令CMD是在容器运行中执行的命令CMD是指定容器运行指令的默认行为,当RUN中指定后,CMD指令将会被RUN指令覆盖 # shell模式CMD command 参数1 参数2 # exec模式CMD [&apos;executable&apos;,&apos;参数1&apos;,&apos;参数2&apos;] # 与ENTRYPOINT搭配使用CMD [&apos;参数1&apos;,&apos;参数2&apos;]# 为ENRYPOINT提供默认参数 ENTRYPOINT与CMD功能相同,不同的是ENTRYPOINT指令不会被RUN指令覆盖可以使用docker run –entrypoint强制使用RUN指令覆盖ENTRYPOINT指令 # shell模式ENTRYPOINT command 参数1 参数2 # exec模式ENTRYPOINT [&apos;executable&apos;,&apos;参数1&apos;,&apos;参数2&apos;] ADD &amp; COPY将其后的使用文件复制到Dockerfile镜像文件中ADD vs COPYADD包含tar解压功能,如果只是单纯的复制文件,Docker推荐使用COPY VOLUME [‘/data’]提供卷的功能,后续讲解 WORKDIR在容器内部设置执行目录,即CMD,ENTRYPOINT工作目录 ENV设置环境变量 ENV &lt;key&gt;&lt;value&gt;ENV &lt;key&gt;=&lt;value&gt; USER指定镜像是什么用户运行 USER daemon EXAMPLE USER mysql# 基于该镜像的指令使用mysql用户运行 ONBUILD镜像触发器]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Docker的安装及基本操作]]></title>
      <url>%2F2015%2F10%2F06%2FDocker%E7%9A%84%E5%AE%89%E8%A3%85%E5%8F%8A%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
      <content type="text"><![CDATA[如今的Docker容器越来越活，之前趁着周末对其进行了肤浅的学习，以下是其安装和基本使用。 Docker的组成Docker Client 客户端Docker Daemon 守护进程/服务器Docker Image 镜像：存放用于启动容器中的各种条件，相当于容器的源代码Docker Container 容器：通过镜像启动，容器的执行来源Docker Registry 仓库：保存用户构建的镜像，公有(如docker hub)仓库和私有仓库 Docker依赖的Linux内核特性Namespace 命名空间 PID（Process ID）进程隔离 NET（Network）管理网络接口 IPC（InterProcess Communication）管理跨进程通信的访问 MNT（Mount）管理挂载点 UTS（Unix Timesharing System）隔离内核和版本标识 Control Groups(cgroups) 控制组资源限制优先级设定资源计量资源控制 Docker容器的能力 文件系统隔离每个容器都有自己的root文件系统 进程隔离每个容器都运行在自己的进程环境中 网络隔离荣期间的虚拟网络接口个IP地址都是分开的 资源的隔离和分组使用cgroups将CPU和内存之类的资源独立分配给每个Docker容器 Docker在Linux下的安装操作系统：CentOS7Docker版本：1.7.1 安装Docker命令 yum install -y docker-io Docker守护进程的配置和操作service docker start：启动docker守护进程service docker stop：关闭docker守护进程service docker restart：重启docker守护进程]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[if __name__ == '__main__是什么']]></title>
      <url>%2F2015%2F10%2F05%2Fif%20__name__%20%3D%3D%20'__main__'%E6%98%AF%E4%BB%80%E4%B9%88%2F</url>
      <content type="text"><![CDATA[在实验楼学习Python的时候，被一个奇怪的代码弄得很不知所措，if name == ‘main‘代表啥意思呢？通过google,终于在stackoverflow找到了我能理解的答案，通过实验让自己更深入的理解和牢记。参考链接：http://stackoverflow.com/questions/419163/what-does-if-name-main-do 实验开始吧~ 首先准备一段代码： #! /usr/bin/env python# coding:utf-8# A.pydef t(): print 'I AM IN FUNCTION!'print 'I AM I A!'if __name__ == '__main__': print 'FIRST TIME IN A!'else: print 'SECOND TIME IN A!' # B.pyimport AA.t()print 'I AM IN B!'if __name__ == '__main__': print 'FIRST TIME IN B!'else: print 'SECOND TIME IN B' 看看代码的执行结果吧 [root@localhost python]# python A.py I AM I A!FIRST TIME IN A![root@localhost python]# python B.py I AM I A!SECOND TIME IN A!I AM IN FUNCTION!I AM IN B!FIRST TIME IN B! 执行结果能理解吗？还是先说下if name == ‘main‘的含义吧当包含if name == ‘main‘的程序直接被执行时，如python A.py，此时name与main相等，即if name == ‘main‘成立，执行的是if成立的代码块；当包含if name == ‘main‘的程序被import到其他的程序中，如B.py程序中的import A，此时A中的name与main不相等，即if name == ‘main‘不成立，执行的是if不成立的代码块，即else的代码块。 好了，原理知道了，那我们来分析下代码的执行结果吧 # A程序的执行结果[root@localhost python]# python A.py I AM I A! #这是程序中if语句之外的语句块print 'I AM I A!'输出的FIRST TIME IN A! #这是程序自己执行时，__name__==__main__时，if成立的语句块输出的[root@localhost python]# python B.py I AM I A! #这是import A时，if语句之外的语句块print 'I AM I A!'输出的SECOND TIME IN A! #这是import A时，__name__！=__main__时，if不成立的语句块else语句输出的I AM IN FUNCTION! #这是B程序调用A程序中的函数A.t()输出的I AM IN B! #这是程序中if语句之外的语句块print 'I AM I B!'输出的FIRST TIME IN B! #这是程序自己执行时，__name__==__main__时，if成立的语句块输出的]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux系统命令之ulimit命令]]></title>
      <url>%2F2015%2F10%2F04%2FLinux%E7%B3%BB%E7%BB%9F%E5%91%BD%E4%BB%A4%E4%B9%8Bulimit%E5%91%BD%E4%BB%A4%2F</url>
      <content type="text"><![CDATA[ulimit命令作为一款Linux内置的命令，可以限制资源的使用，利用鸟哥的例子来说明ulimit的作用吧：想像一个状況：我的 Linux 主机里面同时登陆了十个人，这十个人不知怎么搞的， 同时开启了 100 个文件，每个文件的大小约10MBytes ，请问一下， 我的 Linux 主机的内存要有多大才够？ 1010010 = 10000 MBytes = 10GBytes … 老天爷，这样，系統不挂掉才有鬼呢！为了要预防这个情况的发生，所以我们得『限制使用者的某些系统资源』才行，包括可以开启的文件数量， 可以使用的 CPU 时间，可以使用的内存总量等等。如何设置呢？用 ulimit 吧！(参考链接：http://linux.vbird.org/linux_basic/0320bash.php#variable_ulimit) 参数详解 -S 软限制，资源使用超过该值会出现警告-H 硬限制，资源使用不能超过该值-a 列出所有的资源使用限制情况-b socket文件的buffer大小-c 当某些程序发生错误时，系统会将内存中的信息写到文件中，该文件即为core file，该参数就是限制每个core file的最大容量-d 一个程序的数据段（data segement）最大限制-e 调度优先级的最大限制-f 使用shell建立文件的最大限制-i 等待信号的最大数量-l 程序可锁住内存的最大值-m 可使用内存的最大限制，单位为KB-n 打开文件的最大数量-p 管道缓冲区的大小-q POSIX消息队列的最大字节数-r 实时调度优先级的最大限制-s stack的最大限制-t 可使用的最大CPU时间（单位为秒）-u 每个用户可使用的最大程序数量-v 可使用虚拟内存的最大限制-x 锁定文件的最大数量（file locks并不确定为锁定文件，该参数实际含义不详） 实战演练# 查看当前用户的所有资源使用限制情况[test@localhost ~]$ ulimit -acore file size (blocks, -c) 0data seg size (kbytes, -d) unlimitedscheduling priority (-e) 0file size (blocks, -f) unlimitedpending signals (-i) 7230max locked memory (kbytes, -l) 64max memory size (kbytes, -m) unlimitedopen files (-n) 1024pipe size (512 bytes, -p) 8POSIX message queues (bytes, -q) 819200real-time priority (-r) 0stack size (kbytes, -s) 8192cpu time (seconds, -t) unlimitedmax user processes (-u) 4096virtual memory (kbytes, -v) unlimitedfile locks (-x) unlimited# 修改当前用户的file size的限制为512blocks,注意file size的大小有所变动，并且注意该方法的设置在当前用户登出系统时，设置失效，即此时的file size依旧会变为原来的unlimited[test@localhost ~]$ ulimit -f 512[test@localhost ~]$ ulimit -acore file size (blocks, -c) 0data seg size (kbytes, -d) unlimitedscheduling priority (-e) 0file size (blocks, -f) 512pending signals (-i) 7230max locked memory (kbytes, -l) 64max memory size (kbytes, -m) unlimitedopen files (-n) 1024pipe size (512 bytes, -p) 8POSIX message queues (bytes, -q) 819200real-time priority (-r) 0stack size (kbytes, -s) 8192cpu time (seconds, -t) unlimitedmax user processes (-u) 4096virtual memory (kbytes, -v) unlimitedfile locks (-x) unlimited#测试file size是否真的限制文件大小[test@localhost ~]$ dd if=/dev/zero of=test bs=4K count=1204文件大小超出限制[test@localhost ~]$ dd if=/dev/zero of=test bs=4K count=513文件大小超出限制[test@localhost ~]$ dd if=/dev/zero of=test bs=4K count=512文件大小超出限制[test@localhost ~]$ dd if=/dev/zero of=test bs=4K count=511文件大小超出限制[test@localhost ~]$ dd if=/dev/zero of=test bs=4K count=256文件大小超出限制[test@localhost ~]$ dd if=/dev/zero of=test bs=4K count=128记录了128+0 的读入记录了128+0 的写出524288字节(524 kB)已复制，0.000737911 秒，711 MB/秒]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Liunx系统命令之tree命令]]></title>
      <url>%2F2015%2F10%2F04%2FLiunx%E7%B3%BB%E7%BB%9F%E5%91%BD%E4%BB%A4%E4%B9%8Btree%E5%91%BD%E4%BB%A4%2F</url>
      <content type="text"><![CDATA[Linux下有一个树状图展示目录的命令，是一款在颜值上优先于ls的命令。 tree安装tar -zxvf tree-1.7.0.tgzcd tree-1.7.0makecp -af tree /usr/bin 或者 yum install -y tree tree命令详解 tree参数详解 -a 显示所有文件和目录。-A 使用ASNI绘图字符显示树状图而非以ASCII字符组合。 -C 在文件和目录清单加上色彩，便于区分各种类型。-d 显示目录名称而非内容。 [root@oracle01 /]# tree -d /data/mysqldata3306/data/mysqldata3306├── binlog├── innodb_log├── innodb_ts├── log├── mydata│ ├── mysql│ ├── performance_schema│ ├── test│ └── ym├── relaylog├── sock└── tmpdir -D 列出文件或目录的更改时间。-f 在每个文件或目录之前，显示完整的相对路径名称。-F 在执行文件，目录，Socket，符号连接，管道名称名称，各自加上”*”,”/“,”=”,”@”,”|”号。-g 列出文件或目录的所属群组名称，没有对应的名称时，则显示群组识别码。-i 不以阶梯状列出文件或目录名称。-I&lt;范本样式&gt; 不显示符合范本样式的文件或目录名称。-l 如遇到性质为符号连接的目录，直接列出该连接所指向的原始目录。-n 不在文件和目录清单加上色彩。-N 直接列出文件和目录名称，包括控制字符。-p 列出权限标示。 [root@oracle01 /]# tree -dCp /data/mysqldata3306/data/mysqldata3306├── [drwxr-xr-x] binlog├── [drwxr-xr-x] innodb_log├── [drwxr-xr-x] innodb_ts├── [drwxr-xr-x] log├── [drwxr-xr-x] mydata│ ├── [drwx------] mysql│ ├── [drwx------] performance_schema│ ├── [drwx------] test│ └── [drwx------] ym├── [drwxr-xr-x] relaylog├── [drwxr-xr-x] sock└── [drwxr-xr-x] tmpdir -P&lt;范本样式&gt; 只显示符合范本样式的文件或目录名称。-q 用”?”号取代控制字符，列出文件和目录名称。-s 列出文件或目录大小。 [root@oracle01 /]# tree -s /data/mysqldata3306/binlog//data/mysqldata3306/binlog/├── [ 65302] mysql-bin.000001├── [ 1046158] mysql-bin.000002├── [ 556] mysql-bin.000003├── [ 1125] mysql-bin.000004├── [ 1510] mysql-bin.000005├── [ 30019] mysql-bin.000006└── [ 264] mysql-bin.index0 directories, 7 files -t 用文件和目录的更改时间排序,从最新开始排序。 [root@oracle01 ym]# lltotal 220-rw-rw---- 1 mysql mysql 61 Aug 18 14:01 db.opt-rw-rw---- 1 mysql mysql 8556 Aug 18 17:04 s.frm-rw-rw---- 1 mysql mysql 98304 Aug 18 17:04 s.ibd-rw-rw---- 1 mysql mysql 8556 Aug 18 16:36 t.frm-rw-rw---- 1 mysql mysql 98304 Aug 18 16:36 t.ibd[root@oracle01 ym]# tree -t /data/mysqldata3306/mydata/ym//data/mysqldata3306/mydata/ym/├── s.ibd├── s.frm├── t.ibd├── t.frm└── db.opt -u 列出文件或目录的拥有者名称，没有对应的名称时，则显示用户识别码。-x 将范围局限在现行的文件系统中，若指定目录下的某些子目录，其存放于另一个文件系统上，则将该子目录予以排除在寻找范围外。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Percona-Xtrabackup之xtrabackup使用]]></title>
      <url>%2F2015%2F10%2F04%2FPercona-Xtrabackup%E4%B9%8Bxtrabackup%E4%BD%BF%E7%94%A8%2F</url>
      <content type="text"><![CDATA[xtrabackup为Percona Xtrabackup工具之一，文章从xtrabackup工具的工作原理与使用对其进行了介绍。Percona Xtrabackup的安装详见：http://wing324.github.io/2015/10/04/Percona-Xtrabackup%E5%AE%89%E8%A3%85/ 配置xtrabackupxtrabackup可以在my.cnf文件[xtrabackup]下配置，但注意[xtrabackup]的相关配置会取代[mysqld]相关配置。 [xtrabackup]target_dir = /data/backup xtrabackup完全备份演示创建完全备份xtrabackup备份时的两个主要任务 在后台运行log-copying线程，监控innodb重做日志文件，当日志文件发生改变，就将更改的部分复制到xtrabackup_logfile文件中，当备份完成后，该线程也将会停止。 将innodb数据文件复制到备份目录下，并不是简单的复制，而是从数据目录中一页一页的复制。 xtrabackup --user=percona --host=127.0.0.1 --password=percona --port=3306 --defaults-group=&apos;mysqld3306&apos; --backup --target-dir=/data/backup/x_1# 备份成功后的输出信息为：xtrabackup: Transaction log of lsn (1575937353) to (1584357142) was copied. 恢复准备阶段# 第一次恢复准备xtrabackup --prepare --target-dir=/data/backup/x_1/# 目的使数据文件保持一致，但没有创建新的InnoDB日志文件，如果在此时恢复数据并启动MySQL,此时MySQL需要创建新的日志文件，比较耗时。# 准备成功后的输出信息为：InnoDB: Shutdown completed; log sequence number 1584357152# 第二次恢复准备xtrabackup --prepare --target-dir=/data/backup/x_1/# 目的此时xtrbackup囧创建日志文件，在启动MySQL时，将更减少时间。# 成功后输出信息有点像下面这样[root@oracle01 backup]# xtrabackup --prepare --target-dir=/data/backup/x_1/xtrabackup version 2.2.12 based on MySQL server 5.6.24 Linux (x86_64) (revision id: 8726828)xtrabackup: cd to /data/backup/x_1/xtrabackup: This target seems to be already prepared.xtrabackup: notice: xtrabackup_logfile was already used to &apos;--prepare&apos;....xtrabackup: starting shutdown with innodb_fast_shutdown = 1InnoDB: FTS optimize thread exiting.InnoDB: Starting shutdown...InnoDB: Shutdown completed; log sequence number 1584357408 恢复备份rsync -avrP ibdata1 /data/mysqldata3307/innodb_ts/rsync -avrP ib_logfile* /data/mysqldata3307/innodb_log/rsync -avrP mysql/ wing/ xtrabackup_binlog_pos_innodb xtrabackup_checkpoints xtrabackup_logfile /data/mysqldata3307/mydata/# xtrabackup未提供任何用于恢复的参数# 修改权限chown -R mysql:mysql /data/mysqldata3307/ Xtrabackup增量备份演示创建增量备份# 由于增量备份时建立在完全备份的基础上，所以首先创建完全备份xtrabackup --user=percona --port=3306 --password=percona --host=192.168.200.143 --defaults-group=&apos;mysqld3306&apos; --backup --target-dir=/data/backup/full# 创建增量备份xtrabackup --user=percona --port=3306 --password=percona --host=192.168.200.143 --defaults-group=&apos;mysqld3306&apos; --backup --target-dir=/data/backup/inc1 --incremental-basedir=/data/backup/full/# --target-dir 增量备份存储目录# --incemental-base-dir 基于该目录做增量备份 增量备份恢复前准备阶段# 对完全备份进行准备xtrabackup --prepare --apply-log-only --target-dir=/data/backup/full/# 对增量备份进行准备xtrabackup --prepare --target-dir=/data/backup/full/ --incremental-dir=/data/backup/inc1/# --target-dir指的是全备份目录# --incremental-dir指的是增量备份目录 恢复增量备份恢复增量备份与完全备份相似，此处省略。 Xtrabackup部分演示创建部分备份备份单个表或者单个数据库。 # 使用-tables创建部分备份xtrabackup --user=percona --host=127.0.0.1 --port=3306 --password=percona --defaults-group=&apos;mysqld3306&apos; --backup --target-dir=/data/backup/t_full --tables=&apos;^wing[.]t&apos;# 使用--tables-file，--databases，--databases-file创建部分备份,具体参考innobackupex部分备份 恢复部分备份准备阶段xtrabackup --prepare --export --target-dir=/data/backup/t_full/ 恢复部分备份该部分与innobackupex部分备份恢复相似，请参考之。切记要首先创建恢复表的表结构。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Percona-Xtrabackup之innobackupex的使用]]></title>
      <url>%2F2015%2F10%2F04%2FPercona-Xtrabackup%E4%B9%8Binnobackupex%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
      <content type="text"><![CDATA[innobackupex为Percona Xtrabackup工具之一，文章从innobackupex工具的工作原理与使用对其进行了介绍。Percona Xtrabackup的安装详见：http://wing324.github.io/2015/10/04/Percona-Xtrabackup%E5%AE%89%E8%A3%85/ innobackupex工作原理备份 默认情况下,innobackupex会以xtrbackup –suspend-at-end启动复制InnoDB文件,当xtrbackup完成后,innobackupex检测到xtrabackup创建的xtrabackup_suspended_2文件,然后执行FTWRL复制非InnoDB文件。 当所有的文件都备份成功,重新开始ibbackup,等待完成复制备份过程中执行完成的事务,然后释放表锁,从库开启（在使用参数–safe-slave-backup的前提下）,关闭连接MySQL,删除xtrabackup_suspended_2文件,退出xtrabackup。备份目录下的文件backup-my.cnf文件备份时需要的一些my.cnf文件中的选项。 [root@oracle01 2015-09-08_14-29-08]# cat backup-my.cnf # This MySQL options file was generated by innobackupex.# The MySQL server[mysqld]innodb_checksum_algorithm=innodbinnodb_data_file_path=ibdata1:64M:autoextendinnodb_log_files_in_group=2innodb_log_file_size=67108864innodb_page_size=16384innodb_undo_directory=.innodb_undo_tablespaces=0 xtrabackup_binlog_info文件记录最后的binlog目录和位置 [root@oracle01 2015-09-08_14-29-08]# cat xtrabackup_binlog_info mysql-bin.000005 47391729 xtrabackup_checkpoints文件记录备份类型,开始和结束的LSN,是否为压缩备份 [root@oracle01 2015-09-08_14-29-08]# cat xtrabackup_checkpoints backup_type = full-backupedfrom_lsn = 0to_lsn = 545232808last_lsn = 545232808compact = 0 xtrabackup_info文件备份的相关信息 [root@oracle01 2015-09-08_14-29-08]# cat xtrabackup_info uuid = f19640f9-55f2-11e5-9676-fa163ee44368name = tool_name = innobackupextool_command = --user=percona --host=127.0.0.1 --password=... --port=3306 --defaults-group=mysqld3306 /data/backup/tool_version = 1.5.1-xtrabackupibbackup_version = xtrabackup version 2.2.12 based on MySQL server 5.6.24 Linux (x86_64) (revision id: 8726828)server_version = 5.6.26-logstart_time = 2015-09-08 14:29:08end_time = 2015-09-08 14:29:22lock_time = 1binlog_pos = filename &apos;mysql-bin.000005&apos;, position 47391729innodb_from_lsn = 0innodb_to_lsn = 545232808partial = Nincremental = Nformat = filecompact = Ncompressed = Nencrypted = N xtrabackup_binlog_pos_innodb文件当前正在备份的相关事务对应的binlog位置 xtrabackup_slave_info文件使用-slave-info参数时记录SHOW SLAVE STATUS\G的主库binlog位置 恢复 首先innobackup会读取my.cnf文件，检查 datadir, innodb_data_home_dir, innodb_data_file_path, innodb_log_group_home_dir 对应的目录是否存在 然后，最先复制非InnoDB数据和索引文件，然后复制InnoDB表和索引文件，最后复制日志文件。复制会保留文件属性，所以在启动恢复服务器之前要修改复制文件的用户和用户组为mysql innobackupex连接MySQL需要使用的参数–user：连接MySQL用户–port： 连接MySQL端口–host：连接MySQL的IP地址–socket：连接MySQL的socket文件–password：连接MySQL用户的密码–defaults-file：指定MySQL配置文件–defaults-group：指定MySQL配置文件读取的位置,如–defaults-group=‘mysqld’或者–defaults-group=‘mysqld3306’ innobackupex用户权限 RELOAD &amp; LOCK TABLES：为了在复制文件之前FLUSH TABLE WITH READ LOCK &amp; FLUSH ENGINE LOGS使用,但备份锁需要时,则还需要执行LOCK TABLE FOR BACKUP &amp; LOCK BINLOG FOR BACKUP REPLICATION CLIENT：获取二进制日志位置信息 CREATE TABLESPACE：为了导入表 PROECSS：为了查看服务器上哪些线程正在运行 SUPER：复制环境下start/stop从库线程 CREATE：创建PERCONA_SCHEMA.xtrabackup_history数据库和表 INSERT：向PERCONA_SCHEMA.xtrabackup_history表中插入历史记录 SELECT：在PERCONA_SCHEMA.xtrabackup_history表中查看innodb_to_lsn值 备份用户所需要的最小的权限为：RELOAD、LOCK TABLES、REPLICATION CLIENT恢复用户所需要的最小的权限为：CREATE TABLESPACE innobackupex完全备份使用演示创建完全备份# innobackupex备份工具innobackupex --user=percona --host=127.0.0.1 --Port=3306 --password=percona --defaults-group=&apos;mysqld3306&apos; /data/backup # 备份成功会有如下输出信息150906 13:16:53 innobackupex: Connection to database server closed150906 13:16:53 innobackupex: completed OK!# 进入备份目录cd /data/backup/2015-09-06_13-16-49# 查看备份目录下的文件ls -l# 备份目录下的文件如下-rw-r--r-- backup-my.cnf-rw-r------ ibdata1drwx------ mysqldrwxr-xr-x performance_schemadrwx------ testdrwx------ wing-rw-r--r-- xtrabackup_binlog_info-rw-r----- xtrabackup_checkpoints-rw-r--r-- xtrabackup_info-rw-r----- xtrabackup_logfile 完全备份准备阶段完全备份结束后,数据并不能立即被使用,因为此时的备份只是简单的复制文件,而存在未提交事务,需要进行prepare阶段方可使用数据。 # 准备阶段innobackupex --apply-log /data/backup/2015-09-06_13-16-49/# 准备成功会有如下输出信息InnoDB: Shutdown completed; log sequence number 1639446150906 13:18:16 innobackupex: completed OK! 注意该部分准备阶段不适合增量备份。原理该过程从备份目录下的backup-my.cnf开始读取配置,然后innobackupex重播已提交事务或者回滚未提交事务,一旦这样做,表空间的所有信息和日志文件都将被重建。其他参数–use-memory 指定该过程使用的内存,默认为100MB。 恢复完全备份# innobackupex备份工具innobackupex --defaults-group=&apos;mysqld3307&apos; --copy-back /data/backup/2015-09-06_13-16-49/# 恢复成功会有如下输出信息innobackupex: Finished copying back files.150906 15:27:22 innobackupex: completed OK! 特别说明 恢复前,需要停掉恢复使用的MySQL数据库实例 datadir、innodb_data_home_dir（即共享表空间目录）、innodb_log_group_home_dir（即InnoDB日志文件）目录均需要清空,因为表空间的所有信息和日志文件都需要被重建。 恢复完成后,实例启动前,需要将之前清空的目录内部文件的权限修改为mysql用户。 恢复备份的实例不需要恢复账号即可完成。 chown -R mysql:mysql 目录名 恢复中的常见错误错误1：innobackupex: Error: Cannot overwrite file: /data/mysqldata3307/innodb_ts/ibdata1 at /usr/bin/innobackupex line 2177解决方法：删除文件ibdata1错误2：innobackupex: Error: Cannot overwrite file: /data/mysqldata3307/innodb_log/ib_logfile0 at /usr/bin/innobackupex line 2177.解决方法：删除文件ib_logfile0错误3：innobackupex: Error: Original data directory ‘/data/mysqldata3307/mydata’ is not empty! at /usr/bin/innobackupex line 2162.解决方法：删除/data/mysqldata3307/mydata下的所有文件 innobackupex增量备份使用演示增量备份只适用于XtraDB和InnoDB存储引擎,对于其他的存储引擎会复制整个数据库。 创建增量备份# 指定上一次备份后的增量备份innobackupex --user=percona --host=192.168.200.143 --port=3306 --password=percona --defaults-group=&apos;mysqld3306&apos; --incremental /data/backup --incremental-basedir=/data/backup/2015-09-06_13-16-49/# 备份成功会有如下输出信息150906 16:34:11 innobackupex: Connection to database server closed150906 16:34:11 innobackupex: completed OK!# 指定lsn后的增量备份innobackupex --user=percona --host=192.168.200.143 --port=3306 --password=percona --incremental /data/backup --defaults-group=&apos;mysqld3306&apos; --incremental-lsn=1639125# 备份成功后会有如下输出信息150906 16:45:43 innobackupex: Connection to database server closed150906 16:45:43 innobackupex: completed OK! 特别说明–incremental 指定增量备份的存储位置,以及指定为增量备份–incremental-basedir 指定最近上一次备份的目录 原理基于LSN,增量备份是在上一次备份后的LSN后备份其后的修改数据,可通过 xtrabackup_checkpoints文件查看LSN的变化。 增量备份准备阶段特别说明 只有已提交的事务被使用,未提交的事务将会被回滚。 # 完全备份准备阶段innobackupex --apply-log --redo-only /data/backup/2015-09-06_13-16-49/注意此处存在--redo-only参数,如果在基础完全备份上对未提交事务进行回滚,此时将不能使用增量备份,故使用--redo-only只重播已提交事务解决该问题。# 第一次增量备份准备阶段innobackupex --apply-log --redo-only /data/backup/2015-09-06_13-16-49/ --incremental-dir=/data/backup/2015-09-06_16-34-08/--incremental-dir 表示增量备份的目录# 第二次增量备份准备阶段innobackupex --apply-log /data/backup/2015-09-06_13-16-49/ --incremental-dir=/data/backup/2015-09-06_16-45-40/注意此处没有--redo-only参数,该参数用于所有的增量备份准备阶段,除了最后一个增量备份此时可在基础的完全备份目录下查看xtrabackup_checkpoints文件中LSN位置已经备份之最后一个增量备份处。 增量备份恢复阶段与完全备份恢复相同,故省略。 部分备份使用演示创建部分备份仅备份部分表(要求表为独立表空间)或者部分数据库。注意：部分备份的恢复不建议使用–copy-back,因为–copy-back是复制所有之前备份的文件。 # 备份wing.percona_test表,使用--include参数 innobackupex --user=percona --host=127.0.0.1 --port=3306 --password=percona --defaults-group=&apos;mysqld3306&apos; --include=&apos;^wing[.]percona_test&apos; /data/backup/ # 备份成功输出如下信息150907 11:08:44 innobackupex: Connection to database server closed150907 11:08:44 innobackupex: completed OK!# 该备份还是会为每个数据库创建目录,但是没有备份表的数据库目录会没有文件,mysql和information_schema亦没有文件。 # 备份wing.t表,使用--tables_file参数innobackupex --user=percona --host=127.0.0.1 --port=3306 --password=percona --defaults-group=&apos;mysqld3306&apos; --tables-file=/data/mysqldata3306/tmpdir/innobackupex_table.txt /data/backup/# 备份成功输出如下信息150907 11:20:21 innobackupex: Connection to database server closed150907 11:20:21 innobackupex: completed OK!# 该备份不会为每个数据库创建目录,仅指定备份的表的数据库才会创建目录 部分备份恢复准备阶段该阶段将会为每个innodb表创建自己的.exp表空间文件。 # --include参数备份文件恢复准备innobackupex --apply-log --export 2015-09-07_11-08-40/--export 用户导入单独表到服务器使用# 恢复准备成功输出如下信息InnoDB: Shutdown completed; log sequence number 1659926150907 11:29:18 innobackupex: completed OK!# 此时可看到该数据库目录下,该表的文件格式如下几种[root@oracle01 wing]# lltotal 128-rw-r--r-- 1 root root 420 Sep 7 11:53 t.cfg-rw-r--r-- 1 root root 16384 Sep 7 11:53 t.exp-rw-r----- 1 root root 8586 Sep 7 11:20 t.frm-rw-r----- 1 root root 98304 Sep 7 11:20 t.ibd 特别说明对于上述四种文件：.ctg、.exp、.frm、.ibd,有三种文件(.cfg、.exp、.ibd)需导入到服务器中.cfg文件：该文件将表的数据字典存放为特殊的格式,严格来说,该文件不需要被导入到服务器中,但是如果没有该文件,会在导入表空间时存在如下警告信息； Warning (Code 1810): InnoDB: IO Read error: (2, No such file or directory) Error opening './wing/t.cfg', will attempt to import without schema verification .exp文件：.frm文件：innodb表的数据字典文件；.ibd文件：innodb表的数据+索引文件。 # 以导入t表为例# 首先需要在导入的服务器中添加相同的表结构root@localhost : wing 01:44:32&gt; create table t (id int,name varchar(16));Query OK, 0 rows affected (0.01 sec)# 然后discard该表的表空间root@localhost : wing 01:45:03&gt; alter table t discard tablespace;Query OK, 0 rows affected (0.00 sec)# 将备份文件中的.exp、.ibd文件复制到该数据库目录下,.cfg文件可选复制cp t.ibd t.exp /data/mysqldata3307/mydata/wing/# 修改复制文件的权限[root@oracle01 wing]# chown -R mysql:mysql t.*# 然后import该表空间root@localhost : wing 01:48:09&gt; alter table t import tablespace;Query OK, 0 rows affected, 1 warning (0.02 sec) 压缩备份使用演示压缩备份会跳过innodb表的二级索引页,所以在恢复准备阶段需要更长的时间用于重建二级索引。（只适用于独立表空间的表） 创建压缩备份innobackupex --user=percona --host=127.0.0.1 --port=3306 --password=percona --defaults-group=&apos;mysqld3306&apos; --compact /data/backup/ 是否为压缩备份,可从xtrabackup_checkpoints文件中查看,compact 选项为 1时,则为压缩备份。 恢复压缩备份准备阶段该过程需要重建二级索引,所以需要使用–rebuild-indexes参数。 innobackupex --apply-log --rebuild-indexes /data/backup/2015-09-07_14-17-50/ 恢复压缩备份与完全恢复备份相同,请参考之。 innobackupex --user=percona --host=127.0.0.1 --port=3307 --password=percona --defaults-group=&apos;mysqld3307&apos; --copy-back /data/backup/2015-09-07_14-17-50 加密备份个人不是特别感兴趣,若以后感兴趣再学习之。https://www.percona.com/doc/percona-xtrabackup/2.2/innobackupex/encrypted_backups_innobackupex.html 复制环境下的备份–slave-info 该选项用于备份从库,输出二进制位置以及主库服务器名称,也会将CHANGE MASTER语句写入到xtrabackup_slave_info文件中。–safe-slave-backup 该选项用于确保一致性的复制状态,该选项将停止从库的SQL线程,等到Slave_open_temp_tables的状态值为0时,开始备份,备份完成后会重启SQL线程。如果在–safe-slave-backup-timeout(默认为30s)之后,Slave_open_temp_tables的状态值未变为0,则备份将失败。percona建议使用pt-table-checksum工具检查主从备份是否正确,暂时本人还未使用,待使用后告知。 基于时间点恢复备份# 完全备份innobackupex --user=percona --host=127.0.0.1 --port=3306 --password=percona --defaults-group=&apos;mysqld3306&apos; --parallel=5 --rsync /data/backup/# 准备恢复阶段innobackupex --apply-log 2015-09-08_10-50-05/# 对数据库做一些更改,然后查看当前数据库二进制日志及位置root@localhost : wing 10:52:44&gt; show tables;+----------------+| Tables_in_wing |+----------------+| percona_test || t |+----------------+2 rows in set (0.00 sec)root@localhost : wing 10:52:46&gt; drop table t;Query OK, 0 rows affected (0.18 sec)root@localhost : wing 10:52:56&gt; drop table percona_test;Query OK, 0 rows affected (0.01 sec)root@localhost : wing 11:28:27&gt; create table wing(id int);Query OK, 0 rows affected (0.09 sec)root@localhost : wing 11:29:11&gt; insert into wing values(1),(2),(3),(4),(5),(6),(7),(8),(9);Query OK, 9 rows affected (0.01 sec)Records: 9 Duplicates: 0 Warnings: 0root@localhost : wing 11:29:18&gt; select * from wing;+------+| id |+------+| 1 || 2 || 3 || 4 || 5 || 6 || 7 || 8 || 9 |+------+9 rows in set (0.00 sec)root@localhost : wing 11:29:34&gt; show binary logs;+------------------+-----------+| Log_name | File_size |+------------------+-----------+| mysql-bin.000001 | 65456 || mysql-bin.000002 | 1175657 || mysql-bin.000003 | 2099 || mysql-bin.000004 | 600 || mysql-bin.000005 | 449 |+------------------+-----------+5 rows in set (0.00 sec)root@localhost : wing 11:29:40&gt; show master status;+------------------+----------+--------------+------------------+-------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+------------------+----------+--------------+------------------+-------------------+| mysql-bin.000005 | 449 | | | |+------------------+----------+--------------+------------------+-------------------+1 row in set (0.00 sec)# 查找备份中的二进制日志及位置[root@oracle01 2015-09-08_10-50-05]# less xtrabackup_binlog_info mysql-bin.000004 338# 恢复完全备份innobackupex --copy-back --defaults-group=&apos;mysqld3307&apos; /data/backup/2015-09-08_10-50-05/# 基于完全备份后的位置(338)重定向到另一个文件中(官网需要这一步,然而好像并没有什么用的样子)mysqlbinlog /data/mysqldata3306/binlog/mysql-bin.000004 /data/mysqldata3306/binlog/mysql-bin.000005 --start-position=338 &gt; /data/mysqldata3307/tmpdir/mybinlog.sql# 此时我想将备份恢复到时间为2015-09-08 11:00:00mysqlbinlog /data/mysqldata3306/binlog/mysql-bin.000004 /data/mysqldata3306/binlog/mysql-bin.000005 --start-position=338 --stop-datetime=&apos;15-09-08 11:00:00&apos; | mysql -uroot --socket=/data/mysqldata3307/sock/mysql.sock# 检查发现percona_test,t表均不存在,是我想要的结果root@localhost : (none) 11:37:48&gt; use wingDatabase changedroot@localhost : wing 11:37:50&gt; show tables;Empty set (0.00 sec) 加速innobackupex备份的几个选项 –parallel为备份添加多个并行线程,它是文件级别上的,只适用于独立表空间或者共享表空间有多个文件。 innobackupex --user=percona --host=127.0.0.1 --port=3306 --password=percona --defaults-group=&apos;mysqld3306&apos; --parallel=5 /data/backup/ –compress-threads对使用–stream=xbstream –compress的压缩备份时,–compress-threads代表压缩备份的并行线程数。 –rsync该选项是加速备份并且缩小flush tables with read lock,使用该选项可以对非innodb文件使用rsync代替cp复制。该参数不能与–remote-host、–stream一起使用。 提高FLUSH TABLES WITH READ LOCK处理备份时为了保证一致性,会在备份开始时使用FLUSH TABLES WITH READ LOCK,但此时如果存在很长的写操作,将会导致产生冲突,默认为所有操作执行完毕后,才开始执行FLUSH TABLES WITH READ LOCK. –lock-wait-timeout 当产生冲突时,innobackupex等待的时候,如果在–lock-wait-timeout时间内没有执行FLUSH TABLES WITH READ LOCK,那么将退出备份。 –lock-wait-query-type=all|update all表示执行等待所有长运行语句结束后执行FLUSH TABLES WITH READ LOCK,update表示等待UPDATE/ALTER/REPLACE/INSERT长运行语句结束后执行FLUSH TABLES WITH READ LOCK。 –lock-wait-threshold 执行长运行查询的时间,超过该值的语句均为长运行语句,该参数需要PROCESS&amp;SUPPER权限。 –kill-long-queries-timeout 杀死FTWRL（FLUSH TABLES WITH READ LOCK）开始后超过该时间的操作。 –kill-long-query-type=all|select all代表杀死FTWRL（FLUSH TABLES WITH READ LOCK）开始后超过–kill-long-queries-timeout 时间的所有操作,select表示杀死FTWRL（FLUSH TABLES WITH READ LOCK）开始后超过–kill-long-queries-timeout 时间的select操作。 在服务器上保存备份历史https://www.percona.com/doc/percona-xtrabackup/2.2/innobackupex/storing_history.html暂时搁置该部分的学习。 innobackex参数https://www.percona.com/doc/percona-xtrabackup/2.2/innobackupex/innobackupex_option_reference.html 推荐阅读http://www.cnblogs.com/Amaranthus/archive/2014/08/19/3922570.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Percona-Xtrabackup安装]]></title>
      <url>%2F2015%2F10%2F04%2FPercona-Xtrabackup%E5%AE%89%E8%A3%85%2F</url>
      <content type="text"><![CDATA[Percona-Xtrabackup为innodb和xtraDB的无加锁的热备份工具。 声明操作系统：CentOS6.5MySQL版本：MySQL5.6.26Percona-Xtrbackup版本：Percona-Xtrabackup2.2.12-1.el6 Percona-Xtrbackup的优点 更快更可靠的备份; 备份过程中不会打断事务的执行; 节省磁盘空间和网络带宽; 自动备份验证; 由于更快的恢复时间,所以提高了数据库正常的运行时间。 Percona-Xtrbackup安装此处选择YUM安装yum install http://www.percona.com/downloads/percona-release/redhat/0.1-3/percona-release-0.1-3.noarch.rpmyum list | grep percona-xtrabackup.x86_64 即可查找到对应的Xtrabackup安装包yum install -y percona-xtrabackup.x86_64 安装percona-xtrabackup工具 Percona-Xtrabackup相关工具 innobackupex备份整个MySQL数据库实例（包括myisam表,innodb表,XtraDB表） xtrabackup只复制InnoDB和XtraDB的数据 xbcrypt用于加密解密备份文件 xbstream形成或提取xbstram流文件 innobackupex和xtrabackup的详细使用,请参考该目录下的同名文件，xbcrypt和xbstream不做介绍。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux正确关机方式]]></title>
      <url>%2F2015%2F10%2F03%2FLinux%E6%AD%A3%E7%A1%AE%E5%85%B3%E6%9C%BA%E6%96%B9%E5%BC%8F%2F</url>
      <content type="text"><![CDATA[也许在平常使用过程中，关机的方式就是shutdown/poweroff,可是在多人协作的服务器上，也许此时还存在其他人工作，这样的方式很容易导致他人的数据丢失等，所以要学会正确的关闭Linux系统。 参考连接：&lt;鸟哥的私房菜&gt; 具体步骤 查看有谁在线：who 将资料同步写入硬盘中：sync 提醒其他用户即将关机：shutdown -k now ‘提示语’ 关机命令：shutdown]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[如何更新CentOS6.5的yum源]]></title>
      <url>%2F2015%2F10%2F03%2F%E5%A6%82%E4%BD%95%E6%9B%B4%E6%96%B0CentOS6-5%E7%9A%84yum%E6%BA%90%2F</url>
      <content type="text"><![CDATA[系统自带的Yum源太老了，很多东西都不是最新的。所以才会有更新yum源的需求。 版本：CentOS6.5163yum源：http://mirrors.163.com/.help/CentOS6-Base-163.repo 步骤 进入yum源配置目录 cd /etc/yum.repos.d/ 备份系统自带yum源 mv CentOS-Base.repo CentOS-Base.repo.bk 下载163yum源 wget http://mirrors.163.com/.help/CentOS6-Base-163.repo 然后标准化 mv CentOS-Base-163.repo CentOS-Base.repo 然后执行如下命令，生效操作 yum clean all （清除所有缓存）yum makecache （加载元数据缓存） 更新系统到最新 rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-*yum upgrade]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux文件权限解释]]></title>
      <url>%2F2015%2F10%2F03%2FLinux%E6%96%87%E4%BB%B6%E6%9D%83%E9%99%90%E8%A7%A3%E9%87%8A%2F</url>
      <content type="text"><![CDATA[Linux下如何修改权限及权限的含义，比起windows，Linux修改权限还真是方便，比windows方便多了，不过是我太笨了，不会用windows命令。。 查看文件：ls -al -rw-r--r--. 1 root root 0 9月 18 06:59 test.txt# 前9个字符则代表权限，user/group/others(3/3/3) 修改权限命令 chgrp 修改文件所属组注：group存在于/etc/group文件中 # 修改文件所属组[root@localhost ~]# ls -l | grep test.txt -rw-r--r--. 1 root root 0 9月 18 06:59 test.txt[root@localhost ~]# chgrp test test.txt [root@localhost ~]# ls -l | grep test.txt -rw-r--r--. 1 root test 0 9月 18 06:59 test.txt chown 修改文件拥有者注：用户存在于/etc/passwd文件中 # 修改文件用户[root@localhost ~]# ls -l | grep test.txt -rw-r--r--. 1 root test 0 9月 18 06:59 test.txt[root@localhost ~]# chown test test.txt [root@localhost ~]# ls -l | grep test.txt -rw-r--r--. 1 test test 0 9月 18 06:59 test.txt# 修改文件用户和用户组[root@localhost ~]# ls -l | grep test.txt -rw-r--r--. 1 test test 0 9月 18 06:59 test.txt[root@localhost ~]# chown root.root test.txt [root@localhost ~]# ls -l | grep test.txt -rw-r--r--. 1 root root 0 9月 18 06:59 test.txt chmod 修改文件权限 # 修改文件权限[root@localhost ~]# ls -l | grep test.txt -rw-r--r--. 1 root root 0 9月 18 06:59 test.txt[root@localhost ~]# chmod 777 test.txt [root@localhost ~]# ls -l | grep test.txt -rwxrwxrwx. 1 root root 0 9月 18 06:59 test.txt r/w/x权限含义r(4):可读w(2):可写x(1):可执行-：无权限 对于文件r4：可读，可读取该文件内容w2：可写，可编辑/增加/修改该文件内容，但不包含删除文件x1：可执行，该文件具有被系统执行的权限 对于目录r4:读取目录下的文件w2:增删改查该目录及该目录下的文件x1:拥有cd到该目录下的权限]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux更改字符集]]></title>
      <url>%2F2015%2F10%2F03%2FLinux%E6%9B%B4%E6%94%B9%E5%AD%97%E7%AC%A6%E9%9B%86%2F</url>
      <content type="text"><![CDATA[为了便于自己记忆，将其方法整理出来。 参考链接：http://linux.vbird.org/linux_basic/0160startlinux.php#cmd_cmd_lang 临时更改字符集# 修改字符集为en_US.utf8[root@localhost ~]# LANG=en_US.utf8# 更改某个显示值得字符集[root@localhost ~]# export LC_ALL=en_US.utf8# 更改所有显示值得字符集[root@localhost ~]# locale# 显示当前LINUX的字符集LANG=en_US.utf8LC_CTYPE=&quot;en_US.utf8&quot;LC_NUMERIC=&quot;en_US.utf8&quot;LC_TIME=&quot;en_US.utf8&quot;LC_COLLATE=&quot;en_US.utf8&quot;LC_MONETARY=&quot;en_US.utf8&quot;LC_MESSAGES=&quot;en_US.utf8&quot;LC_PAPER=&quot;en_US.utf8&quot;LC_NAME=&quot;en_US.utf8&quot;LC_ADDRESS=&quot;en_US.utf8&quot;LC_TELEPHONE=&quot;en_US.utf8&quot;LC_MEASUREMENT=&quot;en_US.utf8&quot;LC_IDENTIFICATION=&quot;en_US.utf8&quot;LC_ALL=en_US.utf8[root@localhost ~]# date# 使用date测试当前字符集Thu Sep 17 06:59:28 CST 2015# 修改字符集为zh_CN.UTF-8[root@localhost ~]# LANG=zh_CN.UTF-8[root@localhost ~]# LC_ALL=zh_CN.UTF-8[root@localhost ~]# localeLANG=zh_CN.UTF-8LC_CTYPE=&quot;zh_CN.UTF-8&quot;LC_NUMERIC=&quot;zh_CN.UTF-8&quot;LC_TIME=&quot;zh_CN.UTF-8&quot;LC_COLLATE=&quot;zh_CN.UTF-8&quot;LC_MONETARY=&quot;zh_CN.UTF-8&quot;LC_MESSAGES=&quot;zh_CN.UTF-8&quot;LC_PAPER=&quot;zh_CN.UTF-8&quot;LC_NAME=&quot;zh_CN.UTF-8&quot;LC_ADDRESS=&quot;zh_CN.UTF-8&quot;LC_TELEPHONE=&quot;zh_CN.UTF-8&quot;LC_MEASUREMENT=&quot;zh_CN.UTF-8&quot;LC_IDENTIFICATION=&quot;zh_CN.UTF-8&quot;LC_ALL=zh_CN.UTF-8[root@localhost ~]# date2015年 09月 17日 星期四 06:59:40 CST 永久更改字符集方式# 编辑/etc/sysconfig/i18n文件，如果没有该文件可自行创建[root@localhost /]# vi /etc/sysconfig/i18n修改为中文添加：LANG=&quot;zh_CN.UTF-8&quot;SYSFONT=&quot;latarcyrheb-sun16&quot;修改为英文添加：LANG=&quot;en_US.UTF-8&quot;SYSFONT=&quot;latarcyrheb-sun16&quot;# 修改完成后，保存下文件后,source命令加载后，即可永久更改字符集[root@localhost /] source /etc/sysconfig/i18n]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[关于MySQL异常捕捉那些事儿]]></title>
      <url>%2F2015%2F10%2F02%2F%E5%85%B3%E4%BA%8EMySQL%E5%BC%82%E5%B8%B8%E6%8D%95%E6%8D%89%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF%2F</url>
      <content type="text"><![CDATA[本系列文章主要介绍如何捕捉处理MySQL异常以及实战演练展示，参考于MySQL5.6官方文档(http://dev.mysql.com/doc/refman/5.6/en/index.html) 和 ZHDBA官网之MySQL数据库的例外处理测试(http://www.zhdba.com/mysqlops/2013/08/31/mysql-handler-2/)。 异常捕捉原因存储程序执行过程中对于一些异常需要特殊处理，如退出当前执行程序或者继续执行等，而这些异常需要先捕捉而后处理。 异常捕捉准备知识之错误信息的认识 当客户端线程向MySQL服务器提交命令后，执行是否成功，服务器端都会返回其相关判断性数据。若是执行过程中出错了，MySQL会返回两类信息：A. MySQL特殊的错误编码（如1146），是一个整数类型的代码，这些错误编码并不适用于其他的数据库产品；B. MySQL的SQL状态值，由5个字符组成（如42S02），该值支持ANSI SQL、ODBC或其他标准协议，并不是所有的MySQL返回的错误信息都有SQL状态值，对于没有SQL状态值使用‘HY000’代替。 对于执行过程中的错误返回的错误编码、SQL状态值以及更详细的错误描述信息，MySQL提供C API函数，如下：A. 返回MySQL错误编码的API函数：mysql_errno()；B. 返回MySQL SQL状态值的API函数：mysql_sqlstate()；C. 返回MySQL错误描述信息的API函数：mysql_error()。 对于预处理语句，相应的错误函数有mysql_stmt_errno()、mysql_stmt_sqlstate()、mysql_stmt_error()。 Errors/Warnings/Notes的数量可以通过调用mysql_warning_count()函数获得。 对于MySQL执行过程中的错误返回的SQL状态值前两个字符的含义如下：A. ‘00’代表MySQL执行成功；B. ‘01’代表MySQL执行成功，但是存在警告信息；C. ‘02’代表MySQL没有获得结果，游标移动到记录结果集的最后或者SELECT … INTO var_list没有取到数据，就会触发此类错误信息；D. 前两个字符大于‘02’的SQL状态值统称为异常，使用EXCEPTION标识。 MySQL的错误编码、SQL状态值及信息http://dev.mysql.com/doc/refman/5.6/en/error-messages-server.html 定义异常 语法 特别说明A. 异常的定义必须出现在游标或异常处理定义之前；B. 不必要处理mysql_error_code为0或者sqlstate_value以‘00’开头的情况，因为它们标识着MySQL执行成功。 定义异常处理 语法 特别说明A. statement可以是简单的SQL语句如SET val_name=value，也可以由BEGIN 和END组成的复合语句；B. 异常处理的定义必须出现在变量或异常定义之后；C. handler_action表明触发异常后如何处理：CONTINUE:触发异常后，待执行完处理异常部分的语句体之后，回到触发异常的下一个语句位置，继续向下执行；EXIT:触发异常后，待执行完处理异常部分的语句体之后，退出当前程序；UNDO:不支持，创建时会出现语法错误。D. condition_value表示需要处理的异常，包括如下异常值：MySQL错误编码或SQL状态值DECLARE … CONDITION定义的异常名称SQLWARNING，表示以‘01’开头的SQL状态值NOT FOUND,表示以‘02’开头的SQL状态值SQLEXCEPTION，表示以大于‘02’开头的SQL状态值 对于没有定义异常处理的异常，MySQL采取的措施如下：A. 对于SQLEXCEPTION的异常，则程序在触发异常的位置终止；B. 对于SQLWARNING的异常，则程序在触发异常之后继续执行；C. 对于NOT FOUND的异常，正常情况下程序在触发异常之后继续执行，当使用SIGNAL/RESIGNAL处理之后，程序则在触发异常的位置终止。 伪装错误信息之SIGNAL 语法 特别说明A. SIGNAL语句的执行不需要任何特殊权限；B. condition_value可以是SQL状态值或DECLARE … CONDITION定义的异常名称，注意此处不可使用错误编号，否则会导致ERROR 1646 (HY000): SIGNAL/RESIGNAL can only use a CONDITION defined with SQLSTATE； SIGNAL语句实战演练A. 存储过程示例B. 执行结果C. 结果分析当divisor=1时，根据其执行结果红框中的SQLSTATE值可知触发的是第一个my_error异常，是由最后一条SIGNAL my_error;语句触发的；当divisor=0时，根据其执行结果红框中的SQLSTATE值可知触发的是第二个my_error异常，是由第一个SIGNAL my_error;语句触发的； 伪装错误信息之RESIGNAL 语法 特别说明A. RESINGAL语句和SIGNAL语句作用相似；B. RESIGNAL语句的执行不需要任何特殊的权限；C. condition_value可以是SQL状态值或DECLARE … CONDITION定义的异常名称，注意此处不可使用错误编号，否则会导致ERROR 1646 (HY000): SIGNAL/RESIGNAL can only use a CONDITION defined with SQLSTATE； RESIGNAL的几种用法A. 仅使用RESIGNAL作为单独的SQL语句B. RESIGNAL+SET语句一起使用C. RESIGNAL+condition_value+SET语句一起使用 异常捕捉实战演练 特别说明A. 以定义异常处理中的三种处理方式CONTINUE/EXIT/UNDO为不同点，演示如何定义异常和异常处理；B. 实战演练以存储过程为示例；C. 示例中的异常触发条件为：SELECT * FROM t;在执行示例的数据库中并不存在t表。 实战演练 A. 异常处理方式为CONTINUE存储过程示例： 存储过程结果： 结果分析：call sp_test的结果中包含三条语句，第一条语句为SELECT b的输出；然后‘SELECT * FROM t’触发错误编号为1146的异常，此时存储过程中存在对错误编号为1146的异常处理，所以第二条语句为异常处理语句块中SELECT a的输出；由于错误编号为1146的异常处理为CONTINUE，故异常处理后，回到触发异常的下一个语句位置，继续执行，所以第三条语句为SELECT c的输出。 B. 异常处理方式为EXIT存储过程示例： 存储过程结果： 结果分析：call sp_test的结果中包含两条语句，第一条语句和第二条语句的输出与‘异常处理为CONTINUE’的第一条和第二条语句输出分析一致；由于错误编号为1146的异常处理为EXIT，故异常处理完毕后，跳出当前程序，即该示例中SELECT c并不会被执行。 C. 异常处理方式为UNDO存储过程示例： 存储过程结果： 结果分析：由于异常处理UNDO方式不被支持，故创建时会报错。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[VARCHAR(N) VS CHAR(N)]]></title>
      <url>%2F2015%2F10%2F01%2FVARCHAR-N-VS-CHAR-N%2F</url>
      <content type="text"><![CDATA[MySQL中CHAR和VARCHAR的区别并不仅仅是所占空间的大小，你知道吗？ MySQL和其他的数据库的一个区别在于，mysql的数据类型中的N指的是字符，其他数据库的数据类型的N指的是字节数。 那么字符和字节数的关系呢？Mysql默认为utf-8，那么在半拼模式下1个字符=1个字节，在全拼模式下1个字符=3个字节。Char(N)：最大为255个字符Varchar(N)：最大为64K字符，即2的64次方不管char还是varchar，当设置的字节数&gt;255时，即N&gt;255/3(85)或255时，存储10个字节占用空间为10+2（2个字节标记位）个字节，当设置的字节数&lt;255时，即N&lt;255/3(85)或255时，存储10个字节占用空间为10+1（1个字节标记位）个字节。并且当insert或update的时候，申请内存N*3个字节的内存，因为不知道是半拼还是全拼模式，所以mysql会申请最大空间，所以此时如果N设置的过大会使用过度内存，不能达到性能优化的目的。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[常用的MySQL内置函数]]></title>
      <url>%2F2015%2F09%2F30%2F%E5%B8%B8%E7%94%A8%E7%9A%84MySQL%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0%2F</url>
      <content type="text"><![CDATA[MySQL存在很多内置的函数，往往会给我们的工作带来很多方便，接下来就介绍下常用的MySQL内置函数。 比较函数COALESCE():返回参数中第一个不是NULL的值。 root@localhost : wing 11:23:22&gt; select coalesce(null,null,1,2,3,'test');+----------------------------------+| coalesce(null,null,1,2,3,'test') |+----------------------------------+| 1 |+----------------------------------+1 row in set (0.00 sec)root@localhost : wing 11:39:14&gt; select coalesce(null,'test',null,1,2);+--------------------------------+| coalesce(null,'test',null,1,2) |+--------------------------------+| test |+--------------------------------+1 row in set (0.00 sec)root@localhost : wing 11:42:54&gt; select coalesce('test',null,1,2);+---------------------------+| coalesce('test',null,1,2) |+---------------------------+| test |+---------------------------+1 row in set (0.00 sec) GREATEST():返回最大的参数,若参数中存在NULL值,则返回NULL值。 root@localhost : wing 11:45:24&gt; select greatest(1,2,null,3,4);+------------------------+| greatest(1,2,null,3,4) |+------------------------+| NULL |+------------------------+1 row in set (0.00 sec)root@localhost : wing 11:45:29&gt; select greatest(1,2,3,4);+-------------------+| greatest(1,2,3,4) |+-------------------+| 4 |+-------------------+1 row in set (0.00 sec)root@localhost : wing 11:45:33&gt; select greatest(1,2,3,4,'test');+--------------------------+| greatest(1,2,3,4,'test') |+--------------------------+| 4 |+--------------------------+1 row in set, 1 warning (0.00 sec)Warning (Code 1292): Truncated incorrect DOUBLE value: 'test' IN():检验某个值是否存在于IN的集合中。 INTERVAL():返回第一个比第一个参数小的索引值,其内部参数必须为数值型,如果第一个参数为NULL值,则返回-1。 root@localhost : wing 11:47:18&gt; select interval(null,1,2,3,4);+------------------------+| interval(null,1,2,3,4) |+------------------------+| -1 |+------------------------+1 row in set (0.00 sec)root@localhost : wing 11:54:34&gt; select interval(2,1,2,3,4);+---------------------+| interval(2,1,2,3,4) |+---------------------+| 2 |+---------------------+1 row in set (0.00 sec) ISNULL():判断其参数是否为NULL值。 root@localhost : wing 11:54:39&gt; select isnull(null);+--------------+| isnull(null) |+--------------+| 1 |+--------------+1 row in set (0.00 sec)root@localhost : wing 11:57:34&gt; select isnull(0);+-----------+| isnull(0) |+-----------+| 0 |+-----------+1 row in set (0.00 sec)root@localhost : wing 11:57:37&gt; select isnull('test');+----------------+| isnull('test') |+----------------+| 0 |+----------------+1 row in set (0.00 sec) LEAST():返回参数中最小的值,如果参数中存在NULL值,则返回NULL。 root@localhost : wing 11:57:44&gt; select least(null,1,2,3);+-------------------+| least(null,1,2,3) |+-------------------+| NULL |+-------------------+1 row in set (0.00 sec)root@localhost : wing 12:00:05&gt; select least(3,2,7);+--------------+| least(3,2,7) |+--------------+| 2 |+--------------+1 row in set (0.00 sec)root@localhost : wing 12:00:13&gt; select least(3,2,7,'test');+---------------------+| least(3,2,7,'test') |+---------------------+| 0 |+---------------------+1 row in set, 1 warning (0.00 sec)Warning (Code 1292): Truncated incorrect DOUBLE value: 'test'root@localhost : wing 12:00:22&gt; select least(3,2,7,null,'test');+--------------------------+| least(3,2,7,null,'test') |+--------------------------+| NULL |+--------------------------+1 row in set (0.00 sec) STRCMP(expr1,expr2)字符串比较函数expr1expr2返回 1 root@localhost : wing 12:08:48&gt; select strcmp('tast','test');+-----------------------+| strcmp('tast','test') |+-----------------------+| -1 |+-----------------------+1 row in set (0.00 sec)root@localhost : wing 12:08:52&gt; select strcmp('test','test');+-----------------------+| strcmp('test','test') |+-----------------------+| 0 |+-----------------------+1 row in set (0.00 sec)root@localhost : wing 12:08:54&gt; select strcmp('test','tast');+-----------------------+| strcmp('test','tast') |+-----------------------+| 1 |+-----------------------+1 row in set (0.00 sec) 赋值操作 ‘:=’不同于=之处在于,’:=’操作从来不解释为一个比较操作符,所以可以使用’:=’操作符将任何一个有效的SQL语句赋值到变量中。 ‘=’只能用于两个场景下：SET语句中’=’相当于’:=’EXAMPLE # 未赋值成功,=相当于比较操作 root@localhost : wing 01:10:02&gt; select @a=1; +------+| @a=1 |+------+| NULL |+------+1 row in set (0.00 sec)# 赋值成功root@localhost : wing 01:25:05&gt; select @a:=1;+-------+| @a:=1 |+-------+| 1 |+-------+1 row in set (0.00 sec) 控制流函数CASE语法CASE value WHEN [compare_value] THEN result [WHEN [compare_value] THEN result ...] [ELSE result] END CASE WHEN [condition] THEN result [WHEN [condition] THEN result ...] [ELSE result] END 实战演练 root@localhost : wing 02:07:15&gt; select * from fun_data;+------+--------+| id | name |+------+--------+| 1 | MySQL || 2 | Oracle || 3 | Python || NULL | SHELL || 5 | NULL |+------+--------+5 rows in set (0.00 sec)# CASE WHEN ...语句实战演练root@localhost : wing 02:07:14&gt; select case when id is null then 0 else id end from fun_data;+-----------------------------------------+| case when id is null then 0 else id end |+-----------------------------------------+| 1 || 2 || 3 || 0 || 5 |+-----------------------------------------+5 rows in set (0.00 sec)# CASE value WHEN ...语句实战演练root@localhost : wing 02:08:46&gt; select case name when 'SHELL' then 'LINUX' else name end from fun_data;+---------------------------------------------------+| case name when 'SHELL' then 'LINUX' else name end |+---------------------------------------------------+| MySQL || Oracle || Python || LINUX || NULL |+---------------------------------------------------+5 rows in set (0.00 sec) IF(expr1,expr2,expr3)如果expr1为TRUE,即expr1&lt;&gt;0以及expr1&lt;&gt;NULL,此时返回expr2;如果expr1为FALSE,即exor1 is null或expr1=0,此时返回expr3。 IFNULL(expr1,expr2)如果expr1为NULL值,此时返回expr2;如果expr1不为NULL值,此时返回expr1。 root@localhost : wing 04:15:44&gt; select ifnull(null,0);+----------------+| ifnull(null,0) |+----------------+| 0 |+----------------+1 row in set (0.00 sec)root@localhost : wing 04:34:18&gt; select ifnull(3,0);+-------------+| ifnull(3,0) |+-------------+| 3 |+-------------+1 row in set (0.00 sec)root@localhost : wing 04:34:23&gt; select ifnull('test',0);+------------------+| ifnull('test',0) |+------------------+| test |+------------------+1 row in set (0.00 sec)root@localhost : wing 04:34:30&gt; select ifnull(0,1);+-------------+| ifnull(0,1) |+-------------+| 0 |+-------------+1 row in set (0.00 sec) NULLIF(expr1,expr2)如果expr1=exor2,则返回NULL;如果expr1&lt;&gt;expr2,则返回expr1。 root@localhost : wing 04:37:56&gt; select nullif(null,null);+-------------------+| nullif(null,null) |+-------------------+| NULL |+-------------------+1 row in set (0.00 sec)root@localhost : wing 04:39:06&gt; select nullif(1024,1024);+-------------------+| nullif(1024,1024) |+-------------------+| NULL |+-------------------+1 row in set (0.00 sec)root@localhost : wing 04:39:18&gt; select nullif(1024,512);+------------------+| nullif(1024,512) |+------------------+| 1024 |+------------------+1 row in set (0.00 sec)root@localhost : wing 04:39:24&gt; select nullif(512,1024);+------------------+| nullif(512,1024) |+------------------+| 512 |+------------------+1 row in set (0.00 sec) 字符串函数LENGTH()返回字符串的字节数 CHAR_LENGTH()返回字符串的字符数 BIT_LENGTH()返回字符串的位数 root@localhost : wing 04:48:06&gt; select char_length('MySQL数据库');+-------------------------------+| char_length('MySQL数据库') |+-------------------------------+| 8 |+-------------------------------+1 row in set (0.00 sec)root@localhost : wing 04:48:21&gt; select length('MySQL数据库');+--------------------------+| length('MySQL数据库') |+--------------------------+| 14 |+--------------------------+1 row in set (0.00 sec)root@localhost : wing 04:48:25&gt; select bit_length('MySQL数据库');+------------------------------+| bit_length('MySQL数据库') |+------------------------------+| 112 |+------------------------------+1 row in set (0.00 sec) CONCAT()对字符串进行合并 root@localhost : wing 04:50:15&gt; select concat('MySQL','数据库');+-----------------------------+| concat('MySQL','数据库') |+-----------------------------+| MySQL数据库 |+-----------------------------+1 row in set (0.00 sec) ELT(expr1,expr2….)根据expr1选择expr2….对应的数据 root@localhost : wing 05:23:19&gt; select elt(1,'MySQL','Oracle','Python');+----------------------------------+| elt(1,'MySQL','Oracle','Python') |+----------------------------------+| MySQL |+----------------------------------+1 row in set (0.00 sec)root@localhost : wing 05:23:41&gt; select elt(2,'MySQL','Oracle','Python');+----------------------------------+| elt(2,'MySQL','Oracle','Python') |+----------------------------------+| Oracle |+----------------------------------+1 row in set (0.00 sec)root@localhost : wing 05:23:45&gt; select elt(0,'MySQL','Oracle','Python');+----------------------------------+| elt(0,'MySQL','Oracle','Python') |+----------------------------------+| NULL |+----------------------------------+1 row in set (0.00 sec)root@localhost : wing 05:23:49&gt; select elt(5,'MySQL','Oracle','Python');+----------------------------------+| elt(5,'MySQL','Oracle','Python') |+----------------------------------+| NULL |+----------------------------------+1 row in set (0.00 sec)root@localhost : wing 05:23:54&gt; select elt(,'MySQL','Oracle','Python');ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ''MySQL','Oracle','Python')' at line 1root@localhost : wing 05:23:59&gt; select elt(NULL,'MySQL','Oracle','Python');+-------------------------------------+| elt(NULL,'MySQL','Oracle','Python') |+-------------------------------------+| NULL |+-------------------------------------+1 row in set (0.00 sec) FORMAT(X,D)数值的输出结果格式,X类似于‘#,###,###.##’,D为小数点输出的精度。 root@localhost : wing 05:37:31&gt; select format(837485938.3,4);+-----------------------+| format(837485938.3,4) |+-----------------------+| 837,485,938.3000 |+-----------------------+1 row in set (0.00 sec)root@localhost : wing 05:37:48&gt; select format(837485938.3);ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ')' at line 1root@localhost : wing 05:37:54&gt; select format(837485938.3,0);+-----------------------+| format(837485938.3,0) |+-----------------------+| 837,485,938 |+-----------------------+1 row in set (0.00 sec) INSERT(str,x,y,instr)将字符串str从第x位置开始，y个字符长的子串替换为instrmysql&gt; select * from t;+------------+| vc |+------------+| vcvcvcvcvc || wingwing |+------------+2 rows in set (0.00 sec)mysql&gt; select insert(vc,3,5,'fianna') from t;+-------------------------+| insert(vc,3,5,'fianna') |+-------------------------+| vcfiannacvc || wifiannag |+-------------------------+2 rows in set (0.04 sec) LCASE() &amp;&amp; LOWER()LCASE()与LOWER()功能相同,将字符串所有字母小写 root@localhost : wing 05:37:58&gt; select lower('MySQL');+----------------+| lower('MySQL') |+----------------+| mysql |+----------------+1 row in set (0.00 sec) LEFT(str,len) &amp;&amp; RIGHT(str,len)分别为返回从左边开始返回str第len个字母,从右边开始返回str第len个字母root@localhost : wing 05:41:08&gt; select left('MySQL',1),RIGHT('MySQL',1);+-----------------+------------------+| left('MySQL',1) | RIGHT('MySQL',1) |+-----------------+------------------+| M | L |+-----------------+------------------+1 row in set (0.00 sec) LOCATE(substr,str),LOCATE(substr,str,pos)LOCATE(substr,str)返回substr在str中第一次出现的位置LOCATE(substr,str,pos)返回sunstr在str中pos位置后第一次出现的位置类似函数有POSITION()root@localhost : wing 05:55:13&gt; select locate('sql','MySQLSQL');+--------------------------+| locate('sql','MySQLSQL') |+--------------------------+| 3 |+--------------------------+1 row in set (0.00 sec)root@localhost : wing 05:55:18&gt; select locate('SQL','MySQLSQL');+--------------------------+| locate('SQL','MySQLSQL') |+--------------------------+| 3 |+--------------------------+1 row in set (0.00 sec)root@localhost : wing 05:55:26&gt; select locate('SQL','MySQLSQL',4);+----------------------------+| locate('SQL','MySQLSQL',4) |+----------------------------+| 6 |+----------------------------+1 row in set (0.00 sec) LPAD(str,n,pad)用字符串pad对str最左边进行填充，直到长度为n个字符长度mysql&gt; select * from t;+------+| vc |+------+| vc || wing |+------+2 rows in set (0.01 sec)mysql&gt; select lpad(vc,5,'pad') from t;+------------------+| lpad(vc,5,'pad') |+------------------+| padvc || pwing |+------------------+2 rows in set (0.00 sec) RPAD(str,n,pad)用字符串pad对str最右边进行填充，直到长度为n个字符长度mysql&gt; select * from t;+------+| vc |+------+| vc || wing |+------+2 rows in set (0.01 sec)mysql&gt; select rpad(vc,5,'mysql') from t; +--------------------+| rpad(vc,5,'mysql') |+--------------------+| vcmys || wingm |+--------------------+2 rows in set (0.00 sec) LTRIM() &amp;&amp; RTRIM() &amp;&amp; TRIM()LTRIM()返回删除左边空格的字符串RTRIM()返回删除右边空格的字符串TRIM()返回删除左右空格的字符串root@localhost : wing 06:12:24&gt; select ltrim(' MySQL ');+----------------------+| ltrim(' MySQL ') |+----------------------+| MySQL |+----------------------+1 row in set (0.00 sec)root@localhost : wing 06:12:31&gt; select rtrim(' MySQL ');+----------------------+| rtrim(' MySQL ') |+----------------------+| MySQL |+----------------------+1 row in set (0.00 sec)root@localhost : wing 06:12:34&gt; select trim(' MySQL ');+---------------------+| trim(' MySQL ') |+---------------------+| MySQL |+---------------------+1 row in set (0.00 sec) repeat(str,count)重复count次strroot@localhost : wing 06:04:49&gt; select repeat('MySQL',2);+-------------------+| repeat('MySQL',2) |+-------------------+| MySQLMySQL |+-------------------+1 row in set (0.00 sec)root@localhost : wing 06:04:56&gt; select repeat(null,2);+----------------+| repeat(null,2) |+----------------+| NULL |+----------------+1 row in set (0.00 sec replace(str,from_str,to_str)将str中的from_str替换为to_str root@localhost : wing 06:07:13&gt; Select replace('www.mysql.com','mysql','MySQL');+------------------------------------------+| replace('www.mysql.com','mysql','MySQL') |+------------------------------------------+| www.MySQL.com |+------------------------------------------+1 row in set (0.00 sec) reverse(str)将str逆序输出 root@localhost : wing 06:08:26&gt; select reverse('MySQL');+------------------+| reverse('MySQL') |+------------------+| LQSyM |+------------------+1 row in set (0.00 sec) SPACE(n)返回n个空格 root@localhost : wing 06:12:38&gt; select space(6);+----------+| space(6) |+----------+| |+----------+1 row in set (0.00 sec)root@localhost : wing 06:15:52&gt; select space(100);+------------------------------------------------------------------------------------------------------+| space(100) |+------------------------------------------------------------------------------------------------------+| |+------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec) SUBSTRING(str,m,n)返回从字符串str的m位置起的n个字符长度的字符串mysql&gt; select * from t;+------------+| vc |+------------+| vcvcvcvcvc || wingwing |+------------+2 rows in set (0.00 sec)mysql&gt; select substring(vc,3,5) from t;+-------------------+| substring(vc,3,5) |+-------------------+| vcvcv || ngwin |+-------------------+2 rows in set (0.00 sec) UCASE() &amp;&amp; UPPER()将str大写 root@localhost : wing 06:16:11&gt; select UCASE('python'),UPPER('python');+-----------------+-----------------+| UCASE('python') | UPPER('python') |+-----------------+-----------------+| PYTHON | PYTHON |+-----------------+-----------------+1 row in set (0.00 sec) 数值函数CEIL() &amp; CEILING()两者功能相同,返回比给出的参数大的最小的整数。 root@localhost : wing 04:18:06&gt; select ceil(9.99);+------------+| ceil(9.99) |+------------+| 10 |+------------+1 row in set (0.00 sec)root@localhost : wing 04:18:20&gt; select ceiling(9.99);+---------------+| ceiling(9.99) |+---------------+| 10 |+---------------+1 row in set (0.00 sec)root@localhost : wing 04:18:24&gt; select ceil(null);+------------+| ceil(null) |+------------+| NULL |+------------+1 row in set (0.00 sec)root@localhost : wing 04:18:29&gt; select ceiling(null);+---------------+| ceiling(null) |+---------------+| NULL |+---------------+1 row in set (0.00 sec)root@localhost : wing 04:18:32&gt; select ceil(0);+---------+| ceil(0) |+---------+| 0 |+---------+1 row in set (0.00 sec)root@localhost : wing 04:18:36&gt; select ceiling(-9.99);+----------------+| ceiling(-9.99) |+----------------+| -9 |+----------------+1 row in set (0.00 sec) CRC32()用于循环冗余校验,返回一个32位无符号数值,是对一个传送的数据块进行校验,是一种高效的差错控制方法。 root@localhost : wing 04:21:43&gt; select crc32('MySQL');+----------------+| crc32('MySQL') |+----------------+| 3259397556 |+----------------+1 row in set (0.00 sec)root@localhost : wing 04:23:41&gt; select crc32('mysql');+----------------+| crc32('mysql') |+----------------+| 2501908538 |+----------------+1 row in set (0.00 sec) FLOOR()返回比参数小的最大的整数。 root@localhost : wing 04:41:32&gt; select floor(1.23);+-------------+| floor(1.23) |+-------------+| 1 |+-------------+1 row in set (0.00 sec)root@localhost : wing 04:41:39&gt; select floor(0);+----------+| floor(0) |+----------+| 0 |+----------+1 row in set (0.00 sec)root@localhost : wing 04:41:42&gt; select floor(null);+-------------+| floor(null) |+-------------+| NULL |+-------------+1 row in set (0.00 sec)root@localhost : wing 04:41:47&gt; select floor(-1.23);+--------------+| floor(-1.23) |+--------------+| -2 |+--------------+1 row in set (0.00 sec) POW(x,y) &amp;&amp; POWER(x,y)两者功能相同,求x的y次方 root@localhost : wing 04:49:54&gt; select power(2,4);+------------+| power(2,4) |+------------+| 16 |+------------+1 row in set (0.00 sec) RAND() &amp;&amp; RAND(N) 不论是同一个事务中还是非同一个事务中,包含RAND()函数的SQL语句之前都会记录两个会话级的参数RAND_SEED1和RAND_SEED2,由这两个参数根据RAND()产生随机数的算法便可得到一个确定的数值,所以即使在binlog_format=STATEMENT模式下,主从复制之间使用RAND()函数也可以确保数据一致; RAND()函数为产生随机数在[0,1)之间; 对于RAND(N),参数N用作种子值,种子值即binlog日志中RAND_SEED1和RAND_SEED2,产生相同的随机数; root@localhost : wing 05:08:51&gt; select * from rand_data;+------+| id |+------+| 1 || 2 || 3 |+------+3 rows in set (0.00 sec)# 在如下示例中可观察到,rand()的函数产生的随机数随机改变,rand(n)的函数产生的随机数是确定值root@localhost : wing 05:08:58&gt; select id , rand() from rand_data;+------+---------------------+| id | rand() |+------+---------------------+| 1 | 0.07249362959758716 || 2 | 0.3962498189846518 || 3 | 0.7637682368641423 |+------+---------------------+3 rows in set (0.00 sec)root@localhost : wing 05:09:08&gt; select id , rand(1) from rand_data;+------+---------------------+| id | rand(1) |+------+---------------------+| 1 | 0.40540353712197724 || 2 | 0.8716141803857071 || 3 | 0.1418603212962489 |+------+---------------------+3 rows in set (0.00 sec)root@localhost : wing 05:09:14&gt; select id , rand() from rand_data;+------+--------------------+| id | rand() |+------+--------------------+| 1 | 0.6300917581004014 || 2 | 0.8591541106432249 || 3 | 0.4054953096485653 |+------+--------------------+3 rows in set (0.00 sec)root@localhost : wing 05:09:21&gt; select id , rand(1) from rand_data;+------+---------------------+| id | rand(1) |+------+---------------------+| 1 | 0.40540353712197724 || 2 | 0.8716141803857071 || 3 | 0.1418603212962489 |+------+---------------------+3 rows in set (0.00 sec) 对于想获得在指定范围内的随机数,可以通过不同的函数构造得到。 # 获得7到12之间的随机整数root@localhost : wing 05:43:44&gt; select floor(7+rand()*5);+-------------------+| floor(7+rand()*5) |+-------------------+| 7 |+-------------------+1 row in set (0.00 sec)root@localhost : wing 05:43:50&gt; select floor(7+rand()*5);+-------------------+| floor(7+rand()*5) |+-------------------+| 9 |+-------------------+1 row in set (0.00 sec) ROUND(X),ROUND(X,D)将参数X四舍五入到指定的对应的D精度的值,如果D没有指定,则默认为0。 root@localhost : wing 05:50:05&gt; select round(2.34);+-------------+| round(2.34) |+-------------+| 2 |+-------------+1 row in set (0.00 sec)root@localhost : wing 05:50:08&gt; select round(2.78);+-------------+| round(2.78) |+-------------+| 3 |+-------------+1 row in set (0.00 sec)root@localhost : wing 05:50:11&gt; select round(2.34,1);+---------------+| round(2.34,1) |+---------------+| 2.3 |+---------------+1 row in set (0.00 sec)root@localhost : wing 05:50:17&gt; select round(2.78,1);+---------------+| round(2.78,1) |+---------------+| 2.8 |+---------------+1 row in set (0.00 sec)root@localhost : wing 05:50:24&gt; select round(2.5,0);+--------------+| round(2.5,0) |+--------------+| 3 |+--------------+1 row in set (0.00 sec) SIGN()返回参数的符号,-1代表负数,0代表0,1代表正数。 root@localhost : wing 05:52:04&gt; select sign(-123);+------------+| sign(-123) |+------------+| -1 |+------------+1 row in set (0.00 sec)root@localhost : wing 05:57:51&gt; select sign(0);+---------+| sign(0) |+---------+| 0 |+---------+1 row in set (0.00 sec)root@localhost : wing 05:57:54&gt; select sign(123);+-----------+| sign(123) |+-----------+| 1 |+-----------+1 row in set (0.00 sec) SQRT()平方根函数 root@localhost : wing 09:18:43&gt; select sqrt(16);+----------+| sqrt(16) |+----------+| 4 |+----------+1 row in set (0.00 sec)root@localhost : wing 09:18:49&gt; select sqrt(-16);+-----------+| sqrt(-16) |+-----------+| NULL |+-----------+1 row in set (0.00 sec)root@localhost : wing 09:18:51&gt; select sqrt(null);+------------+| sqrt(null) |+------------+| NULL |+------------+1 row in set (0.00 sec) TRUNCATE(X,D)对参数X取D精度的值,其后的值将都被截取掉。 root@localhost : wing 09:20:48&gt; select truncate(1.7699,2);+--------------------+| truncate(1.7699,2) |+--------------------+| 1.76 |+--------------------+1 row in set (0.00 sec) 时间类型函数ADDDATE()在某一个日期之后增加一定的时间间隔并输出。ADDDATE(date,INTERVAL expr unit), ADDDATE(expr,days) root@localhost : wing 10:16:38&gt; select adddate(now(),interval 31 day);+--------------------------------+| adddate(now(),interval 31 day) |+--------------------------------+| 2015-09-27 10:16:44 |+--------------------------------+1 row in set (0.00 sec)root@localhost : wing 10:16:44&gt; select adddate(now(),31);+---------------------+| adddate(now(),31) |+---------------------+| 2015-09-27 10:16:50 |+---------------------+1 row in set (0.00 sec) ADDTIME(expr1,expr2)在expr1的基础上加上expr2,形成新的时间值。 root@localhost : wing 10:40:21&gt; select now(),addtime(now(),'1 1:1:1');+---------------------+--------------------------+| now() | addtime(now(),'1 1:1:1') |+---------------------+--------------------------+| 2015-08-27 10:40:33 | 2015-08-28 11:41:34 |+---------------------+--------------------------+1 row in set (0.00 sec) CONVERT_TZ(dt,from_tz,to_tz)将dt从from_tz的时区转换到to_tz的时区,在该函数中,dt会被认为from_tz时区的时间值。 root@localhost : wing 10:47:37&gt; select now(),convert_tz(now(),'+08:00','+00:00'),convert_tz(now(),'+02:00','+00:00'),convert_tz(now(),'+02:00','-02:00');+---------------------+-------------------------------------+-------------------------------------+-------------------------------------+| now() | convert_tz(now(),'+08:00','+00:00') | convert_tz(now(),'+02:00','+00:00') | convert_tz(now(),'+02:00','-02:00') |+---------------------+-------------------------------------+-------------------------------------+-------------------------------------+| 2015-08-27 10:49:44 | 2015-08-27 02:49:44 | 2015-08-27 08:49:44 | 2015-08-27 06:49:44 |+---------------------+-------------------------------------+-------------------------------------+-------------------------------------+ CURDATE() &amp; CURRENT_DATE &amp; CURRENT_DATE()返回当前日期 root@localhost : wing 10:56:47&gt; select curdate();+------------+| curdate() |+------------+| 2015-08-27 |+------------+1 row in set (0.00 sec) CURRENT_TIME([fsp]) &amp; CURTIMR()返回当前时间 root@localhost : wing 10:56:51&gt; select current_time(6);+-----------------+| current_time(6) |+-----------------+| 10:57:23.324593 |+-----------------+1 row in set (0.00 sec) CURRENT_TIMESTAMP([fsp]) &amp; CURRENT_TIMESTAMP &amp; NOW()返回当前日期+时间 root@localhost : wing 10:57:23&gt; select current_timestamp(6);+----------------------------+| current_timestamp(6) |+----------------------------+| 2015-08-27 10:57:43.637662 |+----------------------------+1 row in set (0.00 sec) DATE(expr)返回expr的日期部分。 root@localhost : wing 10:59:53&gt; select date(now());+-------------+| date(now()) |+-------------+| 2015-08-27 |+-------------+1 row in set (0.00 sec) DATEDIFF(expr1,expr2)返回expr1-expr2得到的天数 root@localhost : wing 10:59:58&gt; select datediff(now(),'2015-01-25');+------------------------------+| datediff(now(),'2015-01-25') |+------------------------------+| 214 |+------------------------------+1 row in set (0.00 sec) DATE_ADD() &amp; DATE_SUB()在date后增加时间间隔：DATE_ADD(date,INTERVAL expr unit)在date后减少时间间隔：DATE_SUB(date,INTERVAL expr unit) root@localhost : wing 11:08:07&gt; select date_add('2015-01-25 00:00:00', interval 5 day),date_sub('2015-01-25 00:00:00',interval 5 day);+-------------------------------------------------+------------------------------------------------+| date_add('2015-01-25 00:00:00', interval 5 day) | date_sub('2015-01-25 00:00:00',interval 5 day) |+-------------------------------------------------+------------------------------------------------+| 2015-01-30 00:00:00 | 2015-01-20 00:00:00 |+-------------------------------------------------+------------------------------------------------+1 row in set (0.00 sec)root@localhost : wing 11:11:32&gt; select date_add('2015-01-25 00:00:00', interval '5 2' day_hour),date_sub('2015-01-25 00:00:00',interval '5 2' day_hour);+----------------------------------------------------------+---------------------------------------------------------+| date_add('2015-01-25 00:00:00', interval '5 2' day_hour) | date_sub('2015-01-25 00:00:00',interval '5 2' day_hour) |+----------------------------------------------------------+---------------------------------------------------------+| 2015-01-30 02:00:00 | 2015-01-19 22:00:00 |+----------------------------------------------------------+---------------------------------------------------------+1 row in set (0.00 sec)root@localhost : wing 11:11:40&gt; select date_add('2015-01-25 00:00:00', interval '5 2:2:2' day_second),date_sub('2015-01-25 00:00:00',interval '5 2:2:2' day_second);+----------------------------------------------------------------+---------------------------------------------------------------+| date_add('2015-01-25 00:00:00', interval '5 2:2:2' day_second) | date_sub('2015-01-25 00:00:00',interval '5 2:2:2' day_second) |+----------------------------------------------------------------+---------------------------------------------------------------+| 2015-01-30 02:02:02 | 2015-01-19 21:57:58 |+----------------------------------------------------------------+---------------------------------------------------------------+1 row in set (0.00 sec) DATE_DORMAT(date,format)将日期以指定的format返回。关于format的形式,详情见：http://dev.mysql.com/doc/refman/5.6/en/date-and-time-functions.html#function_date-format root@localhost : wing 11:22:35&gt; select date_format('2015-01-25','%W %M %Y');+--------------------------------------+| date_format('2015-01-25','%W %M %Y') |+--------------------------------------+| Sunday January 2015 |+--------------------------------------+1 row in set (0.00 sec) DAYNAME(date)返回星期几。 root@localhost : wing 11:26:16&gt; select dayname('2015-01-25');+-----------------------+| dayname('2015-01-25') |+-----------------------+| Sunday |+-----------------------+1 row in set (0.00 sec) DAYOFMONTH() &amp; DAYOFWEEK &amp; DAYOFYEAR()DAYOFMONTH()：返回日期在这个月的第多少天DAYOFWEEK()：返回日期在这个星期的第多少天,星期天为第一天DAYOFYEAR()：返回日期在这一年的第多少天 root@localhost : wing 11:33:22&gt; select dayofmonth('2015-02-14');+--------------------------+| dayofmonth('2015-02-14') |+--------------------------+| 14 |+--------------------------+1 row in set (0.00 sec)root@localhost : wing 11:34:30&gt; select dayofweek('2015-01-25');+-------------------------+| dayofweek('2015-01-25') |+-------------------------+| 1 |+-------------------------+1 row in set (0.00 sec)root@localhost : wing 11:35:41&gt; select dayofyear('2015-02-14');+-------------------------+| dayofyear('2015-02-14') |+-------------------------+| 45 |+-------------------------+1 row in set (0.00 sec) from_days(N)根据数值N换算出日期,开始日期为：0001-01-01 root@localhost : wing 12:01:34&gt; SELECT FROM_DAYS(365);+----------------+| FROM_DAYS(365) |+----------------+| 0000-00-00 |+----------------+1 row in set (0.00 sec)# from_days从这个日期开始记起,从N=366开始有效root@localhost : wing 12:01:36&gt; SELECT FROM_DAYS(366);+----------------+| FROM_DAYS(366) |+----------------+| 0001-01-01 |+----------------+1 row in set (0.00 sec)root@localhost : wing 12:01:39&gt; SELECT FROM_DAYS(367);+----------------+| FROM_DAYS(367) |+----------------+| 0001-01-02 |+----------------+1 row in set (0.00 sec)root@localhost : wing 12:02:35&gt; SELECT FROM_DAYS(3670);+-----------------+| FROM_DAYS(3670) |+-----------------+| 0010-01-18 |+-----------------+1 row in set (0.00 sec) from_unixtime()FROM_UNIXTIME(unix_timestamp)FROM_UNIXTIME(unix_timestamp,format)将数值转换为时间 root@localhost : wing 12:05:32&gt; select from_unixtime(0);+---------------------+| from_unixtime(0) |+---------------------+| 1970-01-01 08:00:00 |+---------------------+1 row in set (0.00 sec)root@localhost : wing 12:05:37&gt; select from_unixtime(125);+---------------------+| from_unixtime(125) |+---------------------+| 1970-01-01 08:02:05 |+---------------------+1 row in set (0.00 sec)root@localhost : wing 12:05:48&gt; select from_unixtime(125,'%W');+-------------------------+| from_unixtime(125,'%W') |+-------------------------+| Thursday |+-------------------------+1 row in set (0.00 sec) get_format()以指定的格式输出时间关于格式请见：http://dev.mysql.com/doc/refman/5.6/en/date-and-time-functions.html#function_get-format root@localhost : wing 01:17:58&gt; SELECT DATE_FORMAT('2015-01-25',GET_FORMAT(DATE,'EUR'));+--------------------------------------------------+| DATE_FORMAT('2015-01-25',GET_FORMAT(DATE,'EUR')) |+--------------------------------------------------+| 25.01.2015 |+--------------------------------------------------+1 row in set (0.00 sec)# get_format()函数不可单独使用root@localhost : wing 01:18:20&gt; select GET_format('2015-01-25','EUR');ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ''2015-01-25','EUR')' at line 1root@localhost : wing 01:19:41&gt; SELECT DATE_FORMAT('2015-01-25 00:00:00',GET_FORMAT(DATE,'EUR'));+-----------------------------------------------------------+| DATE_FORMAT('2015-01-25 00:00:00',GET_FORMAT(DATE,'EUR')) |+-----------------------------------------------------------+| 25.01.2015 |+-----------------------------------------------------------+1 row in set (0.00 sec) HOUR(time)返回时间的小时数,它可以超过23。 root@localhost : wing 01:23:51&gt; select now();+---------------------+| now() |+---------------------+| 2015-08-27 13:24:00 |+---------------------+1 row in set (0.00 sec)root@localhost : wing 01:24:00&gt; select hour(now());+-------------+| hour(now()) |+-------------+| 13 |+-------------+1 row in set (0.00 sec)root@localhost : wing 01:24:27&gt; select hour('356:24:24');+-------------------+| hour('356:24:24') |+-------------------+| 356 |+-------------------+1 row in set (0.00 sec) LAST_DAY(date)返回date所在月份的最后一天日期。 root@localhost : wing 01:48:06&gt; select last_day('2015-01-25');+------------------------+| last_day('2015-01-25') |+------------------------+| 2015-01-31 |+------------------------+1 row in set (0.00 sec)root@localhost : wing 01:48:23&gt; select last_day('2015-02-02');+------------------------+| last_day('2015-02-02') |+------------------------+| 2015-02-28 |+------------------------+1 row in set (0.00 sec) MAKEDATE() &amp; MAKETIME()MAKEDATE(year,dayofyear)：year为年份,dayofyear为一年天数MAKETIME(hour,minute,second) root@localhost : wing 01:48:29&gt; select makedate(2015,35);+-------------------+| makedate(2015,35) |+-------------------+| 2015-02-04 |+-------------------+1 row in set (0.00 sec)root@localhost : wing 01:52:37&gt; select maketime(13,25,25);+--------------------+| maketime(13,25,25) |+--------------------+| 13:25:25 |+--------------------+1 row in set (0.00 sec) MICROSECOND() &amp; MINUTE() &amp; MONTH() &amp; SECOND() &amp; TIME()分别为返回时间的微秒,分钟,月份,秒数,时间。 MONTHNAME(date)返回date的月份值 NOW()返回语句执行时的日期+时间。now()在同一个语句中时间是一致的。 root@localhost : wing 01:56:49&gt; select now(),sleep(2),now();+---------------------+----------+---------------------+| now() | sleep(2) | now() |+---------------------+----------+---------------------+| 2015-08-27 13:57:00 | 0 | 2015-08-27 13:57:00 |+---------------------+----------+---------------------+1 row in set (2.00 sec) SEC_TO_TIME(seconds)将seconds计算为时分秒返回。 root@localhost : wing 01:57:02&gt; select sec_to_time(125);+------------------+| sec_to_time(125) |+------------------+| 00:02:05 |+------------------+1 row in set (0.00 sec) STR_TO_DATE(str,format)将str以format的形式返回一个时间值。 root@localhost : wing 02:01:40&gt; select str_to_date('25,01,2015','%Y,%m,%d');+--------------------------------------+| str_to_date('25,01,2015','%Y,%m,%d') |+--------------------------------------+| 2025-01-20 |+--------------------------------------+1 row in set, 1 warning (0.00 sec)Warning (Code 1292): Truncated incorrect date value: '25,01,2015'root@localhost : wing 02:01:49&gt; select str_to_date('2015,01,25','%Y,%m,%d');+--------------------------------------+| str_to_date('2015,01,25','%Y,%m,%d') |+--------------------------------------+| 2015-01-25 |+--------------------------------------+1 row in set (0.00 sec)root@localhost : wing 02:02:04&gt; select str_to_date('2015,1,2.5','%Y,%m,%d');+--------------------------------------+| str_to_date('2015,1,2.5','%Y,%m,%d') |+--------------------------------------+| 2015-01-02 |+--------------------------------------+1 row in set, 1 warning (0.00 sec) SUBDATE() &amp; SUBTIME()SUBDATE()：将日期减少时间间隔后,返回时间值SUBDATE(date,INTERVAL expr unit)SUBDATE(expr,days)SUBTIME()：将时间减少时间间隔后,返回时间值SUBTIME(expr1,expr2) root@localhost : wing 02:08:20&gt; select subdate(now(),5),subdate(now(),interval 5 hour),subtime(now(),'2015-01-25 00:00:00');+---------------------+--------------------------------+--------------------------------------+| subdate(now(),5) | subdate(now(),interval 5 hour) | subtime(now(),'2015-01-25 00:00:00') |+---------------------+--------------------------------+--------------------------------------+| 2015-08-22 14:08:33 | 2015-08-27 09:08:33 | NULL |+---------------------+--------------------------------+--------------------------------------+1 row in set (0.00 sec) SYSDATE([fsp])返回SYSDATE()执行时的时间值。当使用–sysdate-is-now时,sysdate()与now()一样,否则存在不同,不同之处见举例。 # 以下均在未使用--sysdate-is-now时root@localhost : wing 02:13:49&gt; select now(),sysdate(),sleep(2),now(),sysdate();+---------------------+---------------------+----------+---------------------+---------------------+| now() | sysdate() | sleep(2) | now() | sysdate() |+---------------------+---------------------+----------+---------------------+---------------------+| 2015-08-27 14:14:44 | 2015-08-27 14:14:44 | 0 | 2015-08-27 14:14:44 | 2015-08-27 14:14:46 |+---------------------+---------------------+----------+---------------------+---------------------+1 row in set (2.00 sec)# 如上可见,now()值是相同的,因为now()是指语句开始执行时的时间值,sysdate()是不同的,因为sysdate()指的是执行sysdate时的时间值。 TIMEDIFF(expr1,expr2)返回expr1-expr2之间的差值时间。 root@localhost : wing 02:20:02&gt; select timediff(time(now()),'08:55:00');+----------------------------------+| timediff(time(now()),'08:55:00') |+----------------------------------+| 05:25:18 |+----------------------------------+1 row in set (0.00 sec) TIMESTAMP() &amp; TIMESTAMPADD() &amp; TIMESTAMPDIFF()TO_DAYS() &amp; TO_SECONDS()TO_DAYS()：返回日期的天数,开始日期为0000-00-00TO_SECONDS()：返回日期的秒数,开始日期为0000-00-00 root@localhost : wing 02:25:47&gt; select to_days('0000-01-01');+-----------------------+| to_days('0000-01-01') |+-----------------------+| 1 |+-----------------------+1 row in set (0.00 sec)root@localhost : wing 02:25:53&gt; select to_days('2015-01-25');+-----------------------+| to_days('2015-01-25') |+-----------------------+| 735988 |+-----------------------+1 row in set (0.00 sec)root@localhost : wing 02:26:02&gt; select to_seconds('2015-01-25');+--------------------------+| to_seconds('2015-01-25') |+--------------------------+| 63589363200 |+--------------------------+1 row in set (0.00 sec) UNIX_TIMESTAMP([date])返回unix timestamp值，从’1970-01-01 00:00:00’UTC时区的时间值开始。 root@localhost : wing 02:28:22&gt; select unix_timestamp('2015-01-25 00:00:00');+---------------------------------------+| unix_timestamp('2015-01-25 00:00:00') |+---------------------------------------+| 1422115200 |+---------------------------------------+1 row in set (0.00 sec) UTC_DATE &amp; UTC_DATE() &amp; UTC_TIME &amp; UTC_TIME() &amp; UTC_TIMESTAMP &amp; UTC_TIMESTAMP()将日期或时间以UTC时区返回。 root@localhost : wing 02:31:31&gt; select utc_date();+------------+| utc_date() |+------------+| 2015-08-27 |+------------+1 row in set (0.00 sec)root@localhost : wing 02:32:51&gt; select utc_timestamp();+---------------------+| utc_timestamp() |+---------------------+| 2015-08-27 06:32:57 |+---------------------+1 row in set (0.00 sec)root@localhost : wing 02:32:57&gt; select now(),utc_timestamp();+---------------------+---------------------+| now() | utc_timestamp() |+---------------------+---------------------+| 2015-08-27 14:33:10 | 2015-08-27 06:33:10 |+---------------------+---------------------+1 row in set (0.00 sec)root@localhost : wing 02:33:10&gt; select utc_timestamp;+---------------------+| utc_timestamp |+---------------------+| 2015-08-27 06:34:10 |+---------------------+1 row in set (0.00 sec)root@localhost : wing 02:34:10&gt; select time(now()),utc_time;+-------------+----------+| time(now()) | utc_time |+-------------+----------+| 14:35:58 | 06:35:58 |+-------------+----------+1 row in set (0.00 sec) WEEK(date[,mode])返回date在这一年的第几周。关于mode,详见：http://dev.mysql.com/doc/refman/5.6/en/date-and-time-functions.html#function_week root@localhost : wing 02:35:58&gt; select week(date(now()));+-------------------+| week(date(now())) |+-------------------+| 34 |+-------------------+1 row in set (0.00 sec) WEEKDAY(date)查看date是星期几。(0表示星期一，1表示星期二….) root@localhost : wing 02:38:15&gt; select weekday(date(now()));+----------------------+| weekday(date(now())) |+----------------------+| 3 |+----------------------+1 row in set (0.00 sec)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[mysqld_multi多实例启动工具]]></title>
      <url>%2F2015%2F09%2F29%2Fmysqld-multi%E5%A4%9A%E5%AE%9E%E4%BE%8B%E5%90%AF%E5%8A%A8%E5%B7%A5%E5%85%B7%2F</url>
      <content type="text"><![CDATA[我们往往喜欢在一台服务器上安装多个实例，一般使用不同的端口，如3306，3307等，那么怎么才能启动这些实例呢？怎么才能一起启动呢？又怎么才能一个一个启动呢？MySQL很人性化的提供了一款自带的工具：mysqld_multi，可以毫无压力地满足我们对多实例启动的方式。 mysqld_multi准备知识 mysqld_multi启动会查找my.cnf文件中的[mysqldN]组，N为mysqld_multi后携带的整数值。 mysqld_multi的固定选项可在配置文件my.cnf中进行配置，在[mysqld_multi]组下配置（如果没有该组，可自行建立）。 mysqld_multi使用方式如下： mysqld_multi [options] &#123;start|stop|reload|report&#125; [GNR[,GNR] ...] mysqld_multi选项start 开启MySQL实例stop 关闭MySQL实例reload 重新加载MySQL实例report 返回MySQL当前状态 # report返回当前MySQL状态[root@mysql log]# mysqld_multi report 3307Reporting MySQL serversMySQL server from group: mysqld3307 is not running# start开启MySQL实例[root@mysql log]# mysqld_multi start 3307[root@mysql log]# mysqld_multi report 3307Reporting MySQL serversMySQL server from group: mysqld3307 is running# reload重新加载MySQL实例[root@mysql log]# mysqld_multi reload 3307[root@mysql log]# mysqld_multi report 3307Reporting MySQL serversMySQL server from group: mysqld3307 is running# stop关闭MySQL实例，注意此处是需要一个具有shutdown权限的用户，且密码并被是加密的，也不可以交互式输入密码，Linux又具有history功能，所以为了数据库的安全，还是不要用mysqld_multi stop的方式关闭数据库了吧[root@mysql log]# mysqld_multi stop 3307 --user=root --password=******[root@mysql log]# mysqld_multi report 3307Reporting MySQL serversMySQL server from group: mysqld3307 is not running –example 输出一个mysqld_multi配置文件中的配置示例 mysqld_multi --example# This is an example of a my.cnf file for mysqld_multi.# Usually this file is located in home dir ~/.my.cnf or /etc/my.cnf## SOME IMPORTANT NOTES FOLLOW:## 1.COMMON USER## Make sure that the MySQL user, who is stopping the mysqld services, has# the same password to all MySQL servers being accessed by mysqld_multi.# This user needs to have the &apos;Shutdown_priv&apos; -privilege, but for security# reasons should have no other privileges. It is advised that you create a# common &apos;multi_admin&apos; user for all MySQL servers being controlled by# mysqld_multi. Here is an example how to do it:## GRANT SHUTDOWN ON *.* TO multi_admin@localhost IDENTIFIED BY &apos;password&apos;## You will need to apply the above to all MySQL servers that are being# controlled by mysqld_multi. &apos;multi_admin&apos; will shutdown the servers# using &apos;mysqladmin&apos; -binary, when &apos;mysqld_multi stop&apos; is being called.## 2.PID-FILE## If you are using mysqld_safe to start mysqld, make sure that every# MySQL server has a separate pid-file. In order to use mysqld_safe# via mysqld_multi, you need to use two options:## mysqld=/path/to/mysqld_safe# ledir=/path/to/mysqld-binary/## ledir (library executable directory), is an option that only mysqld_safe# accepts, so you will get an error if you try to pass it to mysqld directly.# For this reason you might want to use the above options within [mysqld#]# group directly.## 3.DATA DIRECTORY## It is NOT advised to run many MySQL servers within the same data directory.# You can do so, but please make sure to understand and deal with the# underlying caveats. In short they are:# - Speed penalty# - Risk of table/data corruption# - Data synchronising problems between the running servers# - Heavily media (disk) bound# - Relies on the system (external) file locking# - Is not applicable with all table types. (Such as InnoDB)# Trying so will end up with undesirable results.## 4.TCP/IP Port## Every server requires one and it must be unique.## 5.[mysqld#] Groups## In the example below the first and the fifth mysqld group was# intentionally left out. You may have &apos;gaps&apos; in the config file. This# gives you more flexibility.## 6.MySQL Server User## You can pass the user=... option inside [mysqld#] groups. This# can be very handy in some cases, but then you need to run mysqld_multi# as UNIX root.## 7.A Start-up Manage Script for mysqld_multi## In the recent MySQL distributions you can find a file called# mysqld_multi.server.sh. It is a wrapper for mysqld_multi. This can# be used to start and stop multiple servers during boot and shutdown.## You can place the file in /etc/init.d/mysqld_multi.server.sh and# make the needed symbolic links to it from various run levels# (as per Linux/Unix standard). You may even replace the# /etc/init.d/mysql.server script with it.## Before using, you must create a my.cnf file either in /usr/my.cnf# or /root/.my.cnf and add the [mysqld_multi] and [mysqld#] groups.## The script can be found from support-files/mysqld_multi.server.sh# in MySQL distribution. (Verify the script before using)#[mysqld_multi]mysqld = /usr/bin/mysqld_safemysqladmin = /usr/bin/mysqladminuser = multi_adminpassword = my_password[mysqld2]socket = /tmp/mysql.sock2port = 3307pid-file = /var/lib/mysql2/hostname.pid2datadir = /var/lib/mysql2language = /usr/share/mysql/mysql/englishuser = unix_user1[mysqld3]mysqld = /path/to/mysqld_safeledir = /path/to/mysqld-binary/mysqladmin = /path/to/mysqladminsocket = /tmp/mysql.sock3port = 3308pid-file = /var/lib/mysql3/hostname.pid3datadir = /var/lib/mysql3language = /usr/share/mysql/mysql/swedishuser = unix_user2[mysqld4]socket = /tmp/mysql.sock4port = 3309pid-file = /var/lib/mysql4/hostname.pid4datadir = /var/lib/mysql4language = /usr/share/mysql/mysql/estoniauser = unix_user3 [mysqld6]socket = /tmp/mysql.sock6port = 3311pid-file = /var/lib/mysql6/hostname.pid6datadir = /var/lib/mysql6language = /usr/share/mysql/mysql/japaneseuser = unix_user4 –log=file_name 指定一个日志输出文件，如果文件存在则在文件末尾处添加日志信息 –mysqladmin=pro_name 用于指定一个程序来实现mysqladmin的功能 –mysqld=pro_name 用于指定一个程序来实现mysqld的功能，如mysqld_safe –no-log 将日志信息输出到屏幕上，而不是输入日志文件中 –password= mysqladmin用户的密码 –silent 简要信息 –user= mysqladmin用户，默认为mysql –tcp-ip 连接到每个服务器的tcp/ip端口，有时候sock文件丢失，但仍然可以通过tcp/ip端口连接服务器]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux下替换Python新版本]]></title>
      <url>%2F2015%2F09%2F29%2FLinux%E4%B8%8B%E6%9B%BF%E6%8D%A2Python%E6%96%B0%E7%89%88%E6%9C%AC%2F</url>
      <content type="text"><![CDATA[目前Centos6.5的Python版本为2.6.6，可是python3和python2的区别真的是很大呢，现在想入手python3，当然要将python2.6.6替换成python3啦。 1.安装相关包：yum install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gcc make 2.下载python源码包，在官网上。 3.安装Python3.4：tar -zxvf Python-3.4.0.tgz -C /usr/local/src/cd /usr/local/src/Python-3.4.0/./configure –prefix=/usr/local/python3.4make -j4 &amp;&amp; make install 4.添加python3.4命令到环境变量中：vi ~/.bash_profilePATH=$PATH:$HOME/bin改为： PATH=$PATH:$HOME/bin:/usr/local/python3.4/bin使环境变量生效：. ~/.bash_profile 5.此时可以通过python3.4命令调用python，但很多时候我只希望用到python就可以调用python3.4，但centos6.5默认的python版本是2.6.6。所以我们可以进行接下来的步骤。 6.先对系统默认版本进行操作：mv /usr/bin/python /usr/bin/python2.6ln -s /usr/local/python3.4/bin/python3.4 /usr/bin/python 7.此时在使用Python命令调用的就是Python3.4的命令。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[mysql_config_editor工具]]></title>
      <url>%2F2015%2F09%2F28%2Fmysql-config-editor%E5%B7%A5%E5%85%B7%2F</url>
      <content type="text"><![CDATA[mysql_config_editor是MySQL自带的一款用于安全加密登录的工具，对于多实例的MySQL数据库来说，每次登陆需要指定host,port,password真的有点烦躁呢，所以可以使用mysql_config_editor工具轻松搞定。 实战演练首先对该工具进行一个快速上手吧。 # 首先根据你的MySQL客户端登陆需求，设置登陆信息，不用害怕，这些信息不会泄露，都会被MySQL加密为别人看不懂的格式[root@mysql ~]# mysql_config_editor set --login-path=wing3306 --user=wing --host=127.0.0.1 --port=3306 --passwordEnter password: # 为了相信真的是别人看不懂，还是看一下保存信息吧，信息保存为~/.mylogin.cnf，其保存信息为二进制文件，反正这结果我是看不懂哒[root@mysql ~]# hexdump ~/.mylogin.cnf 0000000 0000 0000 1912 0410 0d08 0a0d 0307 12160000010 0b13 0700 1800 021e 0010 0000 00d2 4a120000020 229e 43c1 0aba ba4e 2899 314a 0010 00000000030 f41c 5f8d 0b8b 1efc bf69 c6b2 95de 865c0000040 0020 0000 3913 f6af 32bf 3a02 f6d0 20070000050 9e60 9ff7 ae3b 6f04 e9ad 59c8 38b2 688a0000060 d143 ffb7 0020 0000 a2b5 c420 3de7 99e30000070 2909 085f f4c7 2740 769f 86e9 8208 d7d60000080 3ad8 93cc aafd 4389 0010 0000 6bd2 327a0000090 94dc fbd5 86af d82b a87b 27aa 000009c# 此时登陆MySQL,我们可以很轻松的看到MySQL登陆成功的界面啦[root@mysql ~]# mysql --login-path=wing3306Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 24Server version: 5.6.26-log MySQL Community Server (GPL)Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.wing@127.0.0.1 : (none) 03:22:19&gt; 相关选项mysql_config_editor set 对login path进行登陆信息设置 [root@mysql ~]# mysql_config_editor set --login-path=wing3306 --user=wing --host=127.0.0.1 --port=3306 --passwordEnter password: print 显示指定的login path所有信息 # 再次验证没有泄露password哦[root@mysql ~]# mysql_config_editor print --login-path=wing3306[wing3306]user = wingpassword = *****host = 127.0.0.1port = 3306 remove 从登陆文件中删除所有的login path # 第一次可以登录[root@mysql ~]# mysql --login-path=wing3306Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 26Server version: 5.6.26-log MySQL Community Server (GPL)Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.wing@127.0.0.1 : (none) 04:04:04&gt; \qBye# 将指定的login path删除，同样的方式将无法登陆[root@mysql ~]# mysql_config_editor remove --login-path=wing3306[root@mysql ~]# mysql --login-path=wing3306ERROR 2002 (HY000): Can&apos;t connect to local MySQL server through socket &apos;/var/lib/mysql/mysql.sock&apos; (111)[root@mysql ~]# mysql_config_editor print --login-path=wing3306 reset 删除登陆日志的所有内容 [root@mysql ~]# mysql_config_editor reset mysql_config_editor set -h,–host=name 添加host到登陆文件中 -G，–login-path=name 在登录文件中为login path添加名字（默认为client） -p,–password 在登陆文件中添加密码（该密码会被mysql_config_editor自动加密） -u，–user 添加用户名到登陆文件中 -S,–socket=name 添加sock文件路径到登陆文件中 -P，–port=name 添加登陆端口到登陆文件中 mysql_config_editor print–all 输出所有的login path的登陆信息–login-path 指定login path，输出指定的login path登陆信息 [root@mysql ~]# mysql_config_editor print --all[wing3306]user = wingpassword = *****host = 127.0.0.1port = 3306[wing3307]user = wingpassword = *****host = 127.0.0.1port = 3307[root@mysql ~]# mysql_config_editor print --login-path=wing3306[wing3306]user = wingpassword = *****host = 127.0.0.1port = 3306 mysql_config_editor remove -h,–host 删除login path中的host信息 -G，–login-path 指定删除的login path(默认为client) -p,–password 删除login path中的password信息 -u，–user 删除login path中的用户名信息 -S，–socket 删除login path中的sock文件信息 -P，–port 删除login path中的port信息 mysql_config_editor reset无选项。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[TPCC-MySQL工具]]></title>
      <url>%2F2015%2F09%2F28%2FTPCC-MySQL%E5%B7%A5%E5%85%B7%2F</url>
      <content type="text"><![CDATA[TPCC-MySQL是一款MySQL基准测试工具，能测试出当前MySQL的每分钟事务数（TPmC）。本文介绍TPCC-MySQL工具的安装以及如何使用。 TPCC-MySQL安装 安装bazaar客户端yum install -y bzr 至官网注册帐号https://launchpad.net/bzr 在Linux下生成SSH密钥ssh-keygen -t rsa 将SSH密钥提交至bazaarless /root/.ssh/id_rsa.pub将内容复制到 https://launchpad.net/~username/+editsshkeys注：username为用户名 创建tpcc-mysql目录mkdir /tpcc-mysql 使用bazaar下载tpcc-mysql源码bzr launchpad-login usernamebzr branch lp:~percona-dev/perconatools/tpcc-mysql 编译安装cd /tpcc-mysql/tpcc-mysql/srcmake 安装成功的标志为在/tpcc-mysql/tpcc-mysql目录下生成tpcc命令行工具tpcc_load、tpcc_start 注意事项 安装过程遇到如下错误：/usr/bin/ld: cannot find -lmysqlclient_rcollect2: ld 返回 1make: * [../tpcc_load] 错误 1解决方法：export PATH=/usr/local/mysql/bin:$PATHexport LD_LIBRARY_PATH=/usr/local/mysql/lib 启用tpcc_load时,遇到如下错误:./tpcc_load: error while loading shared libraries: libmysqlclient.so.18: cannot open shared object file: No such file or directory解决方法：ln -s /usr/local/mysql-5.6.24-linux-glibc2.5-x86_64/lib/libmysqlclient.so.18 /usr/lib64/libmysqlclient.so.18 TPCC-MySQL相关数据表及用途customer:客户表district:地区表history:历史订单表item:商品表new_orders:新订单表orders:订单表stock:库存表warehouse:仓库表 TPCC-MySQL测试 创建测试数据库mysqladmin -uroot –socket=/data/mysqldata3306/sock/mysql.sock create tpcc 创建相关数表结构1). 创建表source /tpcc-mysql/tpcc-mysql/create_table.sql2). 同种方法创建其他条件,TPCC-MySQL提供如下脚本add_fkey_idx.sqlcount.sqlcreate_table.sqldrop_cons.sql 使用tpcc_load加载测试数据语法./tpcc_load –helptpcc_load [server] [DB] [user] [pass] [warehouse]tpcc_load [server] [DB] [user] [pass] [warehouse] [part] [min_wh] [max_wh][part]: 1=ITEMS 2=WAREHOUSE 3=CUSTOMER 4=ORDERS此处连接的socket为/var/lib/mysql/mysql.sock,如果需要连接指定的socket,将[server]改写成host:port即可;[part]为只创建数据到[part]对应的表中;[min_wh] [max_wh]为min_wid max_wid。测试用例./tpcc_load 127.0.0.1:3306 tpcc root ‘’ 1该测试用例为创建一个仓库,表数据如下： root@localhost : tpcc 08:51:20&gt; select count(*) from customer;+----------+| count(*) |+----------+| 30000 |+----------+1 row in set (0.14 sec)root@localhost : tpcc 08:51:29&gt; select count(*) from district;+----------+| count(*) |+----------+| 10 |+----------+1 row in set (0.00 sec)root@localhost : tpcc 08:51:39&gt; select count(*) from history;+----------+| count(*) |+----------+| 30000 |+----------+1 row in set (0.04 sec)root@localhost : tpcc 08:51:47&gt; select count(*) from item ;+----------+| count(*) |+----------+| 100000 |+----------+1 row in set (0.04 sec)root@localhost : tpcc 08:51:53&gt; select count(*) from new_orders;+----------+| count(*) |+----------+| 9000 |+----------+1 row in set (0.01 sec)root@localhost : tpcc 08:52:00&gt; select count(*) from order_line;+----------+| count(*) |+----------+| 299616 |+----------+1 row in set (0.10 sec)root@localhost : tpcc 08:52:08&gt; select count(*) from orders;+----------+| count(*) |+----------+| 30000 |+----------+1 row in set (0.02 sec)root@localhost : tpcc 08:52:14&gt; select count(*) from stock;+----------+| count(*) |+----------+| 100000 |+----------+1 row in set (0.06 sec)root@localhost : tpcc 08:52:20&gt; select count(*) from warehouse;+----------+| count(*) |+----------+| 1 |+----------+1 row in set (0.00 sec) ./tpcc_load 127.0.0.1:3306 tpcc root ‘’ 1 3 5 9 该测试用例为创建5&lt;WID&lt;9的数据到customer表中。 使用tpcc_start进行基准测试语法./tpcc_start –helptpcc_start -h server_host -P port -d database_name -u mysql_user -p mysql_password -w warehouses -c connections -r warmup_time -l running_time -i report_interval -f report_file -t trx_file参数含义-w 指定仓库数量-c 指定并发连接数-r 指定开始测试前的warmup时间,预热后测试效果会更好-l 指定测试持续时间-i 指定生成报告间隔时长,默认为10s-f 指定生成的报告文件名测试用例./tpcc_start -h 127.0.0.1 -P3306 -d tpcc -u root -w 1 -c 5 -r 10 -l 60模拟对一个仓库(-w 1),并发5个线程(-c 5),预热10s(-r 10),持续压测60s(-l 60)测试结果 option h with value '127.0.0.1'option P with value '3306'option d with value 'tpcc'option u with value 'root'option w with value '1'option c with value '5'option r with value '10'option l with value '60'&lt;Parameters&gt; [server]: 127.0.0.1 [port]: 3306 [DBname]: tpcc [user]: root [pass]: [warehouse]: 1 [connection]: 5 [rampup]: 10 (sec.) [measure]: 60 (sec.)RAMP-UP TIME.(10 sec.)MEASURING START. 10, 66(47):19.999|66.298, 64(2):12.187|14.665, 7(0):2.450|8.299, 7(0):19.999|55.631, 8(7):19.999|144.925 20, 398(106):19.999|54.874, 403(12):9.354|91.049, 39(1):1.691|14.304, 39(0):17.983|19.232, 40(8):19.999|73.468 30, 371(85):19.999|58.431, 369(12):19.999|33.536, 38(1):4.278|16.276, 36(0):19.999|37.133, 38(7):19.999|49.256 40, 555(94):19.999|60.288, 551(5):3.294|6.692, 55(0):1.188|1.403, 57(0):12.233|45.886, 54(1):18.236|58.101 50, 394(81):19.999|56.410, 395(9):8.006|37.376, 39(0):1.008|1.441, 39(0):19.999|58.982, 41(8):19.999|39.037 60, 518(65):19.478|32.064, 517(5):4.041|18.009, 52(2):5.731|11.318, 52(0):19.999|25.891, 51(3):19.999|24.224# 每10s输出一次压测数据,以逗号分隔,总共6列# 第一列：第N次10s# 第二列：总成功执行压测的次数(总推迟执行压测的次数):90%事务的响应时间|本轮测试最大响应时间# 第三列：创建订单业务执行成功次数(推迟执行次数):90%事务的响应时间|本轮测试最大响应时间# 第四列：支付订单业务执行成功次数(推迟执行次数):90%事务的响应时间|本轮测试最大响应时间# 第五列：发货业务执行成功次数(推迟执行次数):90%事务的响应时间|本轮测试最大响应时间# 第六列：查询库存业务执行成功次数(推迟执行次数):90%事务的响应时间|本轮测试最大响应时间STOPPING THREADS.....&lt;Raw Results&gt; [0] sc:1825 lt:478 rt:0 fl:0 [1] sc:2254 lt:45 rt:0 fl:0 [2] sc:226 lt:4 rt:0 fl:0 [3] sc:230 lt:0 rt:0 fl:0 [4] sc:198 lt:34 rt:0 fl:0 in 60 sec.# 第一次结果统计# [0]:创建订单业务统计成功次数(success),延迟次数(late),重试次数(retry),失败次数(failure)# [1]:支付订单业务统计成功次数(success),延迟次数(late),重试次数(retry),失败次数(failure)# [2]:查询订单状态业务统计成功次数(success),延迟次数(late),重试次数(retry),失败次数(failure)# [3]:发货业务统计成功次数(success),延迟次数(late),重试次数(retry),失败次数(failure)# [4]:查询库存业务统计成功次数(success),延迟次数(late),重试次数(retry),失败次数(failure)&lt;Raw Results2(sum ver.)&gt; [0] sc:1826 lt:478 rt:0 fl:0 [1] sc:2254 lt:45 rt:0 fl:0 [2] sc:226 lt:4 rt:0 fl:0 [3] sc:230 lt:0 rt:0 fl:0 [4] sc:198 lt:34 rt:0 fl:0 # 第二次结果统计,与第一次结果相同&lt;Constraint Check&gt; (all must be [OK]) -- 所有的都必须为OK才能通过 [transaction percentage] Payment: 43.43% (&gt;=43.0%) [OK] -- 支付成功次数(上述结果中sc+lt&gt;43%,才为OK,否则为NG) Order-Status: 4.34% (&gt;= 4.0%) [OK] -- 订单状态(上述结果中sc+lt&gt;4%,才为OK,否则为NG) Delivery: 4.34% (&gt;= 4.0%) [OK] -- 发货业务(上述结果中sc+lt&gt;4%,才为OK,否则为NG) Stock-Level: 4.38% (&gt;= 4.0%) [OK] -- 库存业务(上述结果中sc+lt&gt;4%,才为OK,否则为NG) [response time (at least 90% passed)] -- 响应耗时指标必须通过90%才为OK,否则NG New-Order: 79.24% [NG] * -- 新订单 Payment: 98.04% [OK] -- 支付 Order-Status: 98.26% [OK] -- 订单状态 Delivery: 100.00% [OK] -- 发货 Stock-Level: 85.34% [NG] * -- 库存&lt;TpmC&gt; -- 最终的测试结果 2303.000 TpmC -- TpmC结果值,即每分钟的事务数 参考链接http://imysql.cn/2012/08/04/tpcc-for-mysql-manual.htmlhttp://mp.weixin.qq.com/s?__biz=MjM5NzAzMTY4NQ==&amp;mid=200754078&amp;idx=1&amp;sn=91fd3b9ea7d4a20ea90c4eafe4f9fa3c&amp;3rd=MzA3MDU4NTYzMw==&amp;scene=6#rd叶金荣修改过的TPCC-MySQL版本 https://github.com/yejr/tpcc-mysql]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux的各种环境变量修改方式大PK]]></title>
      <url>%2F2015%2F09%2F28%2FLinux%E7%9A%84%E5%90%84%E7%A7%8D%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E4%BF%AE%E6%94%B9%E6%96%B9%E5%BC%8F%E5%A4%A7PK%2F</url>
      <content type="text"><![CDATA[Linux下修改环境变量有好几种方式，到底应该用哪种方式修改呢？每种方式的区别又是什么呢？ 关于什么是环境变量，请自行google参考连接：http://www.powerxing.com/linux-environment-variable/ 查看环境变量 显示当前某个环境变量的指令为：echo $PATH使修改PATH文件立即在当前窗口生效，可以使用source path_file。 显示当前所有的环境变量：env [root@localhost ~]# envXDG_SESSION_ID=8HOSTNAME=localhost.localdomainSELINUX_ROLE_REQUESTED=TERM=vt100SHELL=/bin/bashHISTSIZE=1000SSH_CLIENT=192.168.137.1 5538 22SELINUX_USE_CURRENT_RANGE=SSH_TTY=/dev/pts/0USER=rootLS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;05;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.axv=01;35:*.anx=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=01;36:*.au=01;36:*.flac=01;36:*.mid=01;36:*.midi=01;36:*.mka=01;36:*.mp3=01;36:*.mpc=01;36:*.ogg=01;36:*.ra=01;36:*.wav=01;36:*.axa=01;36:*.oga=01;36:*.spx=01;36:*.xspf=01;36:MAIL=/var/spool/mail/rootPATH=/usr/java/jdk1.7.0_80//bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin:/python:/root/binPWD=/rootJAVA_HOME=/usr/java/jdk1.7.0_80/LANG=zh_CN.UTF-8SELINUX_LEVEL_REQUESTED=HISTCONTROL=ignoredupsSHLVL=1HOME=/rootLOGNAME=rootSSH_CONNECTION=192.168.137.1 5538 192.168.137.129 22LESSOPEN=||/usr/bin/lesspipe.sh %sXDG_RUNTIME_DIR=/run/user/0_=/usr/bin/env 全局环境变量全局环境变量，即对每个用户都生效。 /etc/profile文件，当用户登录时，该文件被执行，即生效； [root@localhost ~]# echo $PATH/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin[root@localhost ~]# vi /etc/profile# 添加export PATH=$PATH:/python[root@localhost ~]# source /etc/profile[root@localhost ~]# echo $PATH/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin:/python#此时使用其他用户使用echo $PATH命令，可发现PATH变量中有新添加的/python路径。 /etc/bashrc文件，当新打开一个终端时，该文件被执行，即生效。 [root@localhost ~]# echo $PATHusr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin[root@localhost ~]# vi /etc/bashrc# 添加export PATH=$PATH:/python[root@localhost ~]# source /etc/profile[root@localhost ~]# echo $PATHusr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin:/python#此时使用其他用户使用echo $PATH命令，可发现PATH变量中有新添加的/python路径 etc/environment文件不做解释。 局部环境变量局部环境变量，即对单个用户生效。 ~/.bash_profile文件，当用户登录时，该文件仅执行一次，即生效一次； [root@localhost ~]# echo $PATH/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin[root@localhost ~]# vi ~/.bash_profilePATH=$PATH:$HOME/bin: &gt;&gt; PATH=$PATH:$HOME/bin:/python[root@localhost ~]# source ~/.bash_profile [root@localhost ~]# echo $PATH/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin:/root/bin:/python#此时使用其他用户使用echo $PATH命令，并未发现PATH变量中有新添加的/python路径。 ~/.bashrc文件，当用户登录以及每次新打开终端时，该文件被执行，即生效。 [root@localhost ~]# echo $PATH/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin[root@localhost ~]# vi ~/.bashrc# 添加如下两行PATH=$PATH:/pythonexport PATH[root@localhost ~]# source ~/.bashrc [root@localhost ~]# echo $PATH/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin:/python#此时使用其他用户使用echo $PATH命令，并未发现PATH变量中有新添加的/python路径。 使用export命令设置临时变量直接使用export命令设置临时变量，仅对当前终端有效。 # 终端1[root@localhost ~]# echo $PATHusr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin:/python[root@localhost ~]# export PATH=usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin[root@localhost ~]# echo $PATHusr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin#终端2Last login: Thu Sep 24 05:50:31 2015 from 192.168.137.1[root@localhost ~]# echo $PATH/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/python:/root/bin]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[binlog_format和transaction_isolation组合]]></title>
      <url>%2F2015%2F09%2F27%2Fbinlog-format%E5%92%8Ctransaction-isolation%E7%BB%84%E5%90%88%2F</url>
      <content type="text"><![CDATA[你知道吗？MySQL中并不是所有的binlog_format和所有的transaction_isolation都可以在一起愉快的合作哦。 测试环境：MySQL5.6.11 组合产生的效果 YES 代表支持binlog_format+transaction_isolation组合NO 代表不支持binlog_format+transaction_isolation组合 WHY NO（以测试的INSERT、UPDATE、DELETE语句为例） STATEMENT + READ UNCOMMITTEDINSERT、UPDATE、DELETE语句无法写入二进制日志中，进而导致主库上无法执行该类型语句。 STATEMENT + READ COMMITEDINSERT、UPDATE、DELETE语句无法写入二进制日志中，进而导致主库上无法执行该类型语句。 MySQL5.6官方文档有如下解释：If you are using InnoDB tables and the transaction isolation level is READ COMMITTED or READ UNCOMMITTED, only row-based logging can be used. It is possible to change the logging format to STATEMENT, but doing so at runtime leads very rapidly to errors because InnoDB can no longer perform inserts. 解释原因：在READ COMMITTED、READ UNCOMMITED事务隔离级别下，基于binlog_format=’STATEMENT’模式下，SQL语句记录的顺序会导致主从数据不一致。 测试以READ COMMITED为例，假设STATEMENT模式下能写入二进制日志中。此处需要提醒一个很重要的概念：RC级别是会话提交后，其他的会话就可以读到提交过后的数据（即有幻读）；RR级别由于MVCC多版本控制，会话提交后，其他会话未提交的状态下，仍旧读到其他会话中第一次读取的版本（即无幻读） 分析：如上，在主库上执行一次如上操作，二进制日志记录SQL语句的方式为commit之后写入二进制日志中。故记录二进制日志的顺序为SESSION 2 , SESSION 1此时如果有从库，那么从库执行的结果为： start transaction; delete from t1 where c1 = 2; select * from t1; +------+------+ | c1 | c2 | +------+------+ | 1 | 1 | | 1 | 1 | +------+------+ commit; start transaction; update t2 set c2 = 3 where c1 in (select c1 from t1); select * from t2; +------+------+ | c1 | c2 | +------+------+ | 1 | 3 | | 2 | 2 | | 1 | 3 | | 2 | 2 | +------+------+ update t2 set c2 = 4 where c1 in (select c1 from t1); select * from t2; +------+------+ | c1 | c2 | +------+------+ | 1 | 4 | | 2 | 2 | | 1 | 4 | | 2 | 2 | +------+------+ commit; 此时对比主库与从库的t2表： 由此可见数据不一致现象，read uncommitted+statement同理可得。 解决方法 使用binlog_foemat=row/mixed 使用repeatable read 或 serializable事务隔离级别 关闭将语句记录到二进制日志的功能，set session sql_log_bin=0，但是注意使用该参数之后，执行的语句将不被记录到二进制日志中。主从会造成数据不一致。 注：REPEATABLE READ + MIXED SERIALIZABLE + MIXEDINSERT、UPDATE、DELETE是以STATEMENT写入二进制日志中 READ UNCOMMITTED + MIXED READ COMMITTED + MIXEDINSERT、UPDATE、DELETE是以ROW写入二进制日志中]]></content>
    </entry>

    
  
  
</search>
